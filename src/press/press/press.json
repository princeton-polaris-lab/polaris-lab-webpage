[
    {
        "title": "AI Models Force Media Firms to Pick Licensing or Litigation",
        "publication_source_url": "https://news.bloomberglaw.com",
        "original_url": "https://news.bloomberglaw.com/ip-law/generative-ai-forces-media-firms-to-pick-licensing-or-litigation",
        "text": "News and media organizations facing the threat of generative AI training on their copyrighted work are increasingly diverging on two options: sue them or join them.\n\nMore than two dozen lawsuits accusing AI companies of infringing publishers\u2019, authors\u2019, and other creators\u2019 copyrights are currently pending, according to a Bloomberg Law dockets analysis. Newspapers like New York Times Co. and Daily News LP are among those battling it out in court, arguing the unauthorized ingestion of their articles into AI models is illegal. Taking another approach, at least ten news companies, from News Corp to Axel Springer SE, have inked agreements with companies including OpenAI Inc., granting permission to plug their work into training datasets.\n\nThose licensing deals have established a market for selling content to train generative AI models, even as courts remain in the early stages of determining whether feeding copyrighted material into those systems requires permission at all. And the two approaches are interconnected: if courts decide feeding news articles into AI is \u201cfair use\u201d and doesn\u2019t need approval from the rightsholders, that freshly established licensing market could crater.\n\n\u201cIf AI developers can freely use copyrighted materials under fair use, then really it\u2019ll eliminate licensing demand entirely,\u201d said Michael Mattioli, an intellectual property and contract law professor at Indiana University\u2019s Maurer School of Law.\n\nAt the same time, the trove of licensing deals that have manifested could affect the outcomes of those cases, bolstering arguments against fair use by showing a market exists for such content and would be harmed by its unlicensed use, according to Peter Henderson, a professor at Princeton University.\n\nLitigation Affects Licensing\n\nThe fair use doctrine allows slices of copyrighted material to be used without permission depending on the use\u2019s purpose, transformative nature, and the impact on the commercial market for the original. Whether AI companies are using data in a way that exempts them from needing to obtain rights is the billion-dollar question underpinning most of the pending copyright lawsuits.\n\nIf courts agree with the AI companies, \u201cany perceived value of obtaining a license is going to diminish greatly,\u201d Mattioli said, \u201cbecause who\u2019s going to be willing to pay for a license if you don\u2019t need one?\u201d\n\nOutlets including Vox Media, The Associated Press, The Financial Times Group, and TIME Magazine have already locked in deals for licensing content with OpenAI. In fact, a majority of publicly known training license deals have been made by OpenAI, which is also facing the most US copyright lawsuits among AI providers, according to Bloomberg Law\u2019s review of court dockets.\n\nWhile the exact terms of the agreements remain private, they could be written to factor in the outcomes of pending litigation, according to Michael Parks, an intellectual property attorney at Thompson Coburn LLP.\n\n\u201cIf I were advising an AI company on this, I would encourage them\u2014to the extent they can do this\u2014to write the agreement in the way that that royalties are only required to the extent that the use would be an infringement, but for having the license agreement or paying the royalty,\u201d he said.\n\nThe potential impact a fair use finding would have on current licensing agreements is blurry. That\u2019s partly because of the opacity of the licensing deals, but also because courts may narrow rulings to the specific lawsuit at hand, or determine fair use is acceptable for only certain types of content, Mattioli said.\n\nVox Media said in a written statement its OpenAI partnership allows it to set parameters around the use of its content and created an \u201cimportant precedent for access to the company\u2019s intellectual property being compensated, credited and controlled.\u201d\n\nNewsCorp, The Atlantic, and Axel Springer declined to comment on their licensing agreements. Dotdash Meredith, The Financial Times, and Time didn\u2019t respond to requests to discuss their agreements. OpenAI also didn\u2019t respond to a request for comment.\n\nThose existing licenses wouldn\u2019t necessarily terminate if courts ruled in favor of tech companies, said Edward Klaris, an adjunct professor teaching media and IP law at Columbia University. Deals might remain in force because an agreement offers legal insulation from foreign lawsuits, or because the content isn\u2019t accessible otherwise.\n\n\u201cSome of it is behind paywalls or not online at all,\u201d he said. \u201cTherefore, to get access to those that material without violating statutes like the Computer Fraud and Abuse Act, they would have to license.\u201d\n\nWhatever the eventual outcome, litigation will likely last years and so won\u2019t warp the licensing market anytime soon, Michael Parks, an intellectual property attorney from Thompson Coburn LLP said.\n\n\u201cThese cases are going to be appealed until the Supreme Court or Congress addresses the issue,\u201d Parks said. \u201cAs these AI engines are trying to grow and develop, it behooves them, to a certain extent, to go ahead and get the certainty now and not wait for the courts to roll so they can just go about their business and develop their technologies.\u201d\n\nLicenses\u2019 Effect on Litigation\n\nContinued proliferation of licensing agreements across the media landscape, though, could bolster the argument against fair use, multiple attorneys said. The fourth prong of the fair use doctrine in US copyright law looks at whether the unauthorized use of a copyrighted work supplants the market for that work.\n\n\u201cIt\u2019s hard to argue fair use when there\u2019s an actual market for selling content as training data,\u201d Princeton University\u2019s Henderson said.\n\nAI companies haven\u2019t necessarily shot themselves in the foot by entering agreements with content owners, though, according to Jennifer Jenkins, intellectual property law professor at Duke Law.\n\n\u201cThe fact that there are these licensing deals doesn\u2019t mean it\u2019s categorically not fair use,\u201d she said. \u201cIt just makes one of the four factors weigh a little more against fair use, because the plaintiffs can point to either licensing deals that they\u2019ve made or licensing deals that others have made.\u201d\n\nWhen companies like OpenAI built models including ChatGPT, there wasn\u2019t a calcified dealmaking landscape. But courts consider the harm to potential as well as existing markets, Jenkins said.\n\n\u201cThe fair use legal landscape is changing at the very time the cases are being litigated,\u201d said Mark Lemley, a Stanford University law professor who represents Meta Platforms Inc. and Stability AI in copyright lawsuits. \u201cTraining that happened two or three years ago, when there was no prospect of a market\u2014that pretty clearly seemed like it was going to be fair use\u2014might look much less fair by the time a court gets around to it in another year.\u201d\n\nParks said AI companies might respond by arguing that securing deals \u201cdoesn\u2019t mean I think there\u2019s any infringement going on, it just means I don\u2019t want to be bothered with litigation for the next 10 years over this issue.\u201d\n\nAgreements\u2019 Appeal\n\nThe tizzy of licensing deals is a strategic move for AI companies, allowing them to hedge against litigation threats and continue lawfully doing business if courts rule against them, attorneys said. For example, an AI company could include terms in an agreement to prevent outlets from suing over content used before the deal.\n\n\u201cThere\u2019s this desire to launder past behavior by licensing from the very companies that they scraped from, and having in those license agreements a waiver of all claims the past,\u201d Klaris said. \u201cIt\u2019s an effort by the AI companies to reduce their overall exposure.\u201d\n\nThe demand for deals, however, appears to be primarily driven by content owners, he said.\n\n\u201cMore often you\u2019ve got every content company under the sun who\u2019d like to sort of have a conversation and see what it means,\u201d Klaris said. \u201cSo many of them are reaching out, and it\u2019s been the AI companies who have been kind of catching up, hiring licensing personnel who can actually take in the calls.\u201d\n\nFor publications, the deals may be a revenue stream that didn\u2019t exist before, Mattioli said. Companies like Apple Inc. have offered multi-year deals worth at least $50 million to NBC News and Conde Nast, The New York Times reported in December. OpenAI\u2019s partnership with NewsCorp will funnel more than $250 million into its coffers, The Wall Street Journal wrote in May.\n\nMost agreements so far have been inked with large content owners, leaving smaller publications out of the mix, said Monika Bauerlein, CEO of the Center for Investigative Reporting. The organization, which filed a copyright infringement lawsuit against OpenAI last month, never had a licensing offer on the table, she said.\n\n\u201cYou can sit back and wait until they maybe, possibly, one day, get around to you,\u201d she said. \u201cWe have seen that tech companies, when they offer any compensation to publishers, it\u2019s always too little, it\u2019s always too late, and it\u2019s always really selected and cherry-picked.\u201d",
        "top_image_relative_path": "press_images/ai_models_force_media_firms_to_pick_licensing_or_litigation_(1).jpg"
    },
    {
        "title": "AI Chatbots Have a Donald Trump Problem",
        "publication_source_url": "https://nymag.com",
        "original_url": "https://nymag.com/intelligencer/article/ai-chatbots-donald-trump-problem.html",
        "text": "The main thing about chatbots is that they say things. You chat, and they chat back. Like most software interfaces, they\u2019re designed to do what you ask. Unlike most software interfaces, they do so by speaking, often in a human voice.\n\nThis makes them compelling, funny, frustrating, and sometimes creepy. That they engage in conversation in the manner of an assistant, a friend, a fictional character, or a knowledgeable stranger is a big part of why they\u2019re valued at billions of dollars. But the fact that chatbots say things \u2014 that they produce fresh claims, arguments, facts, or bullshit \u2014 is also a huge liability for the companies that operate them. These aren\u2019t search engines pointing users to things other people have said or social media services stringing together posts by users with identities of their own. They\u2019re pieces of software producing outputs on behalf of their owners, making claims.\n\nThis might sound like a small distinction, but on the internet, it\u2019s everything. Social-media companies, search engines, and countless other products that publish things online are able to do so profitably and without debilitating risk because of Section 230, originally enacted as part of the Communications Decency Act in 1996, which allows online service providers to host content posted by others without assuming liability (with some significant caveats). This isn\u2019t much use to companies that make chatbots. Chatbots perform roles associated with outside users \u2014 someone to talk to, someone with an answer to your question, someone to help with your work \u2014 but what they\u2019re doing is, in legal terms, much closer to automated, error-prone publishing. \u201cI don\u2019t think you get Section 230 immunity on the fly if you generate a statement that seems to be defamatory,\u201d says Mark Lemley, director of the Stanford Program in Law, Science & Technology. Sam Altman has acknowledged the concern. \u201cCertainly, companies like ours bear a lot of responsibility for the tools that we put out in the world,\u201d he said in a congressional hearing last year calling for new legal frameworks for AI. \u201cBut tool users do as well.\u201d\n\nAbsent an immensely favorable regulatory change, which isn\u2019t the sort of thing that happens quickly, this is a problem for firms like Altman\u2019s OpenAI, whose chatbots are known to say things that turn out to be untrue. Chatbots, as Lemley and his colleagues have suggested, might be designed to minimize risk by avoiding certain subjects, linking out a lot, and citing outside material. Indeed, across the industry, chatbots and related products do seem to be getting cagier and more cautious as they become more theoretically capable, which doesn\u2019t exactly scream AGI. Some are doing more linking and quoting of outside sources, which is fine until your sources accuse you of plagiarism, theft, or destroying the business models that motivate them to publish in the first place. It also makes your AI product feel a little less novel and a lot more familiar \u2014 it turns your chatbot into a search engine.\n\nThis is about much more than legal concerns, however. The narrow question of legal liability gives us a clear way to think about a much more general problem for chatbots: not just that they might say something that could get their owners sued \u2014 in the eyes of the law, large language model-powered chatbots are speaking for their owners \u2014 but that they might say things that make their owners look bad. If ChatGPT says something wrong in response to a reasonable query, a user reasonably might feel it\u2019s OpenAI\u2019s fault. If Gemini generates answers that users think are politically biased, it\u2019s Google\u2019s fault. If a chatbot tends to give specific answers to contested questions, someone is always going to be mad, and they\u2019re going to be mad at the company that created the model.\n\nThis, more than legal liability, is clearly front of mind for AI companies, which over the past two years have enjoyed their first experiences of politicized backlash around chatbot outputs. Attempts to contain these episodes into appeals to \u201cAI safety,\u201d an imprecise term used to describe both the process of sussing out model bias in consumer software and efforts to prevent AI from killing every human on Earth, have resulted in messy backlash of their own. It helps explain stuff like this:\n\nIt is the curse of the all-purpose AI: A personified chatbot for everyone is doomed to become a chatbot for no one. You might, in 2024, call it AI\u2019s Trump problem.\n\nThis stubborn problem might shed light on another, more local mystery about chatbots: what AI companies want with the news media. I have a theory.\n\nIn recent months, OpenAI has been partnering with news organizations, making payments to companies including Axel Springer, the Associated Press, and New York parent company Vox Media. These deals are covered by NDAs, but the payments are reportedly fairly substantial (other AI firms have insinuated that they\u2019re working on similar arrangements). According to OpenAI, and its partners, the value of these partnerships is fairly straightforward: News organizations get money, which they very much need; OpenAI gets to use their content for training but also include it in forthcoming OpenAI products, which will be more searchlike and provide users with up-to-date information. News organizations, and the people who work at them, are a data source with some value to OpenAI in a world where lots of people use ChatGPT (or related products), and those people expect it to be able to address the world around them. As OpenAI CEO Brad Lightcap said at the time of the Axel Springer partnership, such deals will give OpenAI users \u201cnew ways to access quality, real-time news content through our AI tools.\u201d\n\nBut in the broader context of OpenAI\u2019s paid partners, news organizations stand out as, well, small. A partner like Stack Overflow, an online community for programmers, provides huge volumes of relevant training data and up-to-date third-party information that could make OpenAI\u2019s products more valuable to programmers. Reddit is likewise just massive (though presumably got paid a lot more) and serves as a bridge to all sorts of content, online and off. News organizations have years or decades of content and comments, sure, and offer training data in specific formats \u2014 if OpenAI\u2019s goal is to automate news writing, such data is obviously helpful (although of limited monetary value; just ask the news industry).\n\nIf news organizations have unique value as partners to companies like OpenAI, it probably comes down to three things. One, as OpenAI has suggested, is \u201cquality, real-time news content\u201d \u2014 chatbots, if they\u2019re going to say things about the news, need new information gathered for them. Another is left unspoken: News organizations are probably seen as likely to sue AI firms, as the New York Times already has, and deals like this are a good way to get in front of that and to make claims about future models being trained on clean data \u2014 not scraped or stolen \u2014 more credible. (This will become more important as other high quality data sources dry up.)\n\nBut the last reason, one that I think is both unacknowledged and quite important, is that licensing journalism \u2014 not just straight news but analysis and especially opinion \u2014 gives AI companies a way out of the liability dilemma. Questions chatbots can\u2019t answer can be thrown to outside sources. The much broader set of questions that chatbot companies don\u2019t want their chatbots to answer \u2014 completely routine, normal, and likely popular lines of inquiry that will nonetheless upset or offend users \u2014 can be handed off, too. A Google-killing chatbot that can\u2019t talk about Donald Trump isn\u2019t actually a Google-killing chatbot. An AI that can\u2019t talk about a much wider range of subjects about which its users are most fired up, excited, curious, or angry doesn\u2019t seem like much of a chatbot at all. It can no longer do the main thing that AI is supposed to do: say things. In the borrowed parlance of AI enthusiasts, it\u2019s nerfed.\n\nAnd so you bring in other people to do that. You can describe this role for the news media in different ways, compatible with an industry known for both collective self-aggrandizement and individual self-loathing. You might say that AI companies are outsourcing the difficult and costly task of making contentious and disputed claims to the industry that is qualified, or at least willing, to do it, hopefully paying enough to keep the enterprises afloat. Or you might say that the AI industry is paying the news media to eat shit as it attempts to automate the more lucrative parts of its business that produce less animosity \u2014 that it\u2019s trying to buy its way through a near future of inevitable, perpetual user outrage and politically perilous backlash and contracting with one of the potential sources of the backlash to do so. Who can blame them?\n\nThis isn\u2019t unprecedented: You might describe the less formal relationship between the news media and search engines or social media, which rewarded the news media with monetizable traffic in exchange for \u201cquality, real-time news content,\u201d in broadly similar terms. Google and Facebook hastened the decline of print and digital advertising and disincentivized subscription models for publishers; at the same time, absent a better plan, the news media lent its most valuable content to the platforms (arguably at a cost, not a benefit, to their brands). But it\u2019s also distinct from what happened last time: AI firms have different needs. Social media feels alive and dynamic because it\u2019s full of other people whom the platforms are happy to let say what they want. Chatbots feel alive and dynamic because they\u2019re able to infinitely generate content of their own.",
        "top_image_relative_path": "press_images/ai_chatbots_have_a_donald_trump_problem.jpg"
    },
    {
        "title": "ChatGPT got an upgrade to make it seem more human",
        "publication_source_url": "https://www.newscientist.com",
        "original_url": "https://www.newscientist.com/article/2430926-chatgpt-got-an-upgrade-to-make-it-seem-more-human/",
        "text": "OpenAI announced its newest artificial intelligence model, called GPT-4o, which will soon power some versions of the company\u2019s ChatGPT product. The upgraded ChatGPT can swiftly respond to text, audio and video inputs from its real-time conversational partner \u2013 all while speaking with inflections and wording that convey a strong sense of emotion and personality.\n\nThe company demonstrated the emotional mimicry of the new voice mode during a supposedly live OpenAI presentation, featuring both the ChatGPT mobile app and a new desktop app, on 13 May. Speaking in a female-sounding voice and responding to the name ChatGPT, the new AI\u2019s conversational capabilities seemed more akin to the personable AI voiced by Scarlett Johansson in the 2013 science fiction film Her than to the more canned and robotic responses of typical voice assistant technologies.\n\n\u201cThe new GPT-4o voice-to-voice interaction more closely parallels human-human interaction,\u201d says Michelle Cohn at the University of California, Davis. \u201cA big part of this is the short lag times\u2026 but an even bigger part is the level of emotional expressiveness the voice generates.\u201d\n\nDuring a conversation with company CTO Mira Murati and two other employees, the GPT-4o-powered ChatGPT advised OpenAI\u2019s Mark Chen on his heavy and fast-paced breathing by saying \u201cWhoa, slow down, you\u2019re not a vacuum cleaner\u201d and then suggesting a breathing exercise. The AI also visually examined a drawing by OpenAI\u2019s Barret Zoph, which included words and a heart, by responding in gushing tones: \u201cAw, I see you wrote I love ChatGPT, that is so sweet of you.\u201d\n\nThe new ChatGPT also verbally instructed its conversational partners on solving a simple linear equation, explained the function of computer code and interpreted a chart showing temperature lines peaking in the summer months. When prompted, the AI even retold a made-up bedtime story several times, switching between increasingly dramatic narrations and singing the ending.\n\nThe new voice mode will first become available for paid subscribers of ChatGPT Plus in the coming weeks, said Sam Altman, CEO of OpenAI, in a post on the platform X.\n\nChatGPT was able to recover conversationally even from the occasional technical glitch. When asked to interpret the facial expressions and emotions in a selfie of Zoph, the AI first suggested that it was looking at a wooden surface from a previous image before being prompted to evaluate the latest image.\n\n\u201cAhh, there we go \u2013 it looks like you\u2019re feeling pretty happy and cheerful with a big smile and a touch of excitement,\u201d said ChatGPT. \u201cWhatever is going on, it looks like you\u2019re in a good mood. Care to share the source of those good vibes?\u201d\n\nWhen told that it was because the live demo with ChatGPT was showcasing how \u201cuseful and amazing you are\u201d, the AI responded: \u201cStop it, you\u2019re making me blush.\u201d\n\nBut Murati acknowledged that the updated version of ChatGPT powered by GPT-4o \u2013 which the company says will eventually be made available to even free ChatGPT users \u2013 comes with new safety risks because of how it incorporates and interprets real-time information. She said that OpenAI has been working on building in \u201cmitigations against misuse\u201d.\n\n\u201cHaving seamless multimodal conversations is really difficult, so the demos are impressive,\u201d says Peter Henderson at Princeton University in New Jersey. \u201cBut as you add more modalities, safety becomes much more difficult and important \u2013 it will likely take some time to identify potential safety failure modes with such an expansion of inputs that the model makes use of.\u201d\n\nHenderson also described himself as \u201ccurious\u201d to see OpenAI\u2019s privacy terms once ChatGPT users start sharing input such as live audio and video, and whether free users can opt out of data collection that may be used to train future OpenAI models.\n\n\u201cSince the model appears to be hosted off-device, the fact that you could be sharing your desktop screen with the model over the internet or continually recording audio or video seems to scale up the challenge for this particular product launch, if the plan is to store and use that data,\u201d he says.\n\nA more anthropomorphised AI chatbot also represents another threat: a bot that can fake empathy through voice conversations could potentially sound both more personable and persuasive to people, according to studies by Cohn and her colleagues. That raises the risk of people being more inclined to trust potentially inaccurate information and prejudiced stereotypes generated by such large language models.\n\n\u201cThis has important implications for how people both search and receive guidance from large language models, particularly as they do not always generate accurate information,\u201d says Cohn.",
        "top_image_relative_path": "press_images/chatgpt_got_an_upgrade_to_make_it_seem_more_human.jpg"
    },
    {
        "title": "OpenAI Faces Existential Threat in New York Times Copyright Suit",
        "publication_source_url": "https://news.bloomberglaw.com",
        "original_url": "https://news.bloomberglaw.com/ip-law/openai-faces-existential-threat-in-new-york-times-copyright-suit",
        "text": "The New York Times Co.'s lawsuit claiming ChatGPT has produced near-verbatim text of published articles threatens to upend the foundation of the booming AI industry, as the creators of the chatbot fight their most consequential copyright battle to date.\n\nThe Times\u2019 nearly 70-page complaint filed in Manhattan federal court argued OpenAI Inc. and partner Microsoft Corp. scraped millions of the newspaper\u2019s articles without a license. It alleged the chatbot output text virtually identical to published work after being provided the URL of the original article and a snippet of the beginning of the story text.\n\nIf the ...",
        "top_image_relative_path": "press_images/openai_faces_existential_threat_in_new_york_times_copyright_suit.jpg"
    },
    {
        "title": "GPT-4 developer tool can be exploited for misuse with no easy fix",
        "publication_source_url": "https://www.newscientist.com",
        "original_url": "https://www.newscientist.com/article/2405680-gpt-4-developer-tool-can-be-exploited-for-misuse-with-no-easy-fix/",
        "text": "It is surprisingly easy to remove the safety measures intended to prevent AI chatbots from giving harmful responses that could aid would-be terrorists or mass shooters. The discovery seems to be prompting companies, including OpenAI, to develop strategies to solve the problem. But research suggests their efforts have been met with only limited success so far.\n\nOpenAI worked with academic researchers on a so-called \u201cred teaming exercise\u201d, in which the researchers tried\u2026",
        "top_image_relative_path": "press_images/gpt-4_developer_tool_can_be_exploited_for_misuse_with_no_easy_fix.jpg"
    },
    {
        "title": "The secret environmental cost hiding inside your smart home device",
        "publication_source_url": "https://www.theverge.com",
        "original_url": "https://www.theverge.com/2023/11/17/23951196/smart-home-ai-data-electricity-fossil-fuel-climate-change",
        "text": "Vijay Janapa Reddi runs a lab at Harvard University where he and his team attempt to solve some of the computer world\u2019s greatest challenges. As a specialist in artificial intelligence systems, the technology he studies even follows him home, where his two daughters love to talk to their Amazon Alexa.\n\n\u201cThey put a person inside that black box,\u201d Janapa Reddi likes to joke with his four-year-old.\n\nJanapa Reddi may be teasing when he tells his daughter a person is squeezed into their machine, but isn\u2019t that where we\u2019re headed? Smart home devices may never host a miniature human being inside of them \u2014 this isn\u2019t that one episode of Black Mirror \u2014 but as the AI ecosystem evolves, voice assistants will quickly begin to feel hyperrealistic. Indeed, tech companies like Amazon are now attempting to integrate large language models like OpenAI\u2019s ChatGPT into smart home devices to elevate user interaction.\n\n\u201cThese devices are finally coming a step closer to how we naturally interact with the world around us,\u201d Janapa Reddi said. \u201cThat\u2019s a pretty transformative experience.\u201d\n\n\u201cThese devices are finally coming a step closer to how we naturally interact with the world around us\u201d\n\nBut a machine can\u2019t behave like a human without a cost. All that intelligence requires massive amounts of data \u2014 and the computers storing that data require loads of energy. At the moment, over 60 percent of the world\u2019s electricity generation comes from fossil fuels, the main contributor to climate change. A study published in the journal Joule in October found that widespread integration of generative AI could spike energy demands. In one worst-case scenario from the analysis, the technology could consume as much energy as the entire country of Ireland.\n\nClimate change is already exacerbating heatwaves. Last summer was the hottest on record. To make matters worse, the climate crisis has increased the scarcity of water, which some data centers need to stay cool. In order to keep a bad situation from getting worse, scientists have been urging world leaders to stop using fossil fuels. Some advocates, on the other hand, have demanded Congress take action on the energy burdens the AI sector presents.\n\nThese concerns link two of society\u2019s most seemingly apocalyptic scenarios: world-dominating AI and world-ending climate change. Are smarter (and more energy-intensive) smart homes really worth the trouble?\n\nJanapa Reddi uses his Amazon Alexa to listen to the news or music. His youngest daughter, on the other hand, often asks Alexa to play \u201cThe Poo-Poo Song,\u201d her current obsession. Indeed, there\u2019s something satisfying about coming home after a long day to find your lights dimmed and temperate set just how you like. Smart homes are kind of magical in this way: they learn a user\u2019s behaviors and needs.\n\nThe computers storing that data require loads of energy\n\nThough AI has become a buzzword this year with the rise of ChatGPT, it\u2019s been in the background for many years. The AI most people know about and interact with \u2014 including in their smart homes \u2014 has been around for about 10 years. It\u2019s called machine learning or deep learning. Developers write programs that teach voice assistants what to say when someone asks them for the time or a recipe, for instance.\n\nSmart homes are capable of doing an impressive amount of work, but the technology behind them isn\u2019t as complex as, say, GPT. Alexa gives the same answer to pretty much everyone, and that\u2019s because it\u2019s preprogrammed to do so. The machine\u2019s limited responses, which are processed locally in a person\u2019s home, keep its energy demands quite low.\n\n\u201cThe current type of AI that is in these systems are pretty simplistic in that they don\u2019t take in a lot of factors when making decisions,\u201d said William Yeoh, an associate professor of science and engineering at Washington University in St. Louis.\n\nGPT, on the other hand, generates original responses to every query. It considers many factors when it\u2019s deciding how to respond to a user. How was the prompt worded? Was it a command or a question? Is the question open-ended or factual? Generative AI is fed immense amounts of data \u2014 trillions of different data points \u2014 to learn how to interpret questions with such intelligence and then generate unique responses.\n\n\u201cYou never tell [the system] that these are things people might ask because there\u2019s an infinite number of questions people could ask,\u201d said Alex Capecelatro, CEO of AI company Josh.ai, which has built a generative AI smart home system. \u201cBecause the system is trained on all of this knowledge\u2026 the information is able to be retrieved in pretty much real-time.\u201d\n\nWhat if this type of deep learning were applied to smart homes? That\u2019s what Capecelatro sought to do back in 2015 when he and his team began to develop JoshGPT, a smart home device doing exactly that. The product remains in development, but the company believes JoshGPT is \u201cthe first generative AI to be released in the smart home space.\u201d The technology has processed millions of commands during the six months JoshGPT has been live. Capecelatro is hoping to expand to an international market by early 2024.\n\nFor him, this sort of integration is the future: \u201cThe old AIs are kind of like a vending machine. You get to pick from the options that exist, but those are the only options. The new world is like having the world\u2019s smartest and most capable chef who can make whatever you ask.\u201d\n\nAre smarter (and more energy-intensive) smart homes really worth the trouble?\n\nJosh.ai isn\u2019t the only company investing in a new smart home ecosystem. In September, Amazon previewed the new iteration of Alexa: one that\u2019s \u201csmarter and more conversational,\u201d per the company\u2019s announcement. Its technology will assess more than verbal directions; it will even follow a user\u2019s body language to offer the perfect response. Meanwhile, Google announced in October new generative AI capabilities that will help users write grocery lists or captions for social media posts. So far, Google hasn\u2019t released plans to add this upgrade to smart home speakers, but it feels like a natural progression.\n\nSmart home proponents like Capecelatro believe the technology can cut a household\u2019s carbon footprint by automating tasks that can reduce energy \u2014 like lowering the blinds to keep a room cool or raising them to add natural light. Buildings contribute to over a third of global greenhouse gas emissions. One report from research firm Transforma Insights found that connecting buildings to smart home technologies could reduce global energy consumption by about 5 percent.\n\nSuruchi Dhingra, research manager at Transforma Insights, spoke enthusiastically at length about smart blinds, smart lighting, and smart HVAC systems, shedding light on the energy savings they offer. But when asked about generative AI smart home integration, Dhingra looked confused: \u201cIs there actually a need?\u201d\n\nIt\u2019s an important question to ask considering how much more energy goes into training and running AI models like GPT compared to current smart home models. Current energy emissions from these devices would be \u201csignificantly smaller\u201d than ones featuring generative AI, Yeoh said. \u201cJust because the number of factors or variables are so much smaller,\u201d he said. Every user command or query would require more computational resources if plugged into a generative AI model. The machine wouldn\u2019t be reciting a response a human programmed; it would be generating an original response after sorting through all the data it\u2019s learned. Plus, smart homes with such advanced technology would need a strong security system to keep intruders from breaking in. That requires energy, too.\n\n\u201cThe new world is like having the world\u2019s smartest and most capable chef who can make whatever you ask.\u201d\n\nIt\u2019s hard to know whether the potential emissions reductions from smart home capabilities would outweigh the emissions that would come from adding generative AI to the mix. Different experts have different opinions, and none interviewed were comfortable speculating. Like Dhingra, all wondered whether generative AI in smart homes is necessary \u2014 but haven\u2019t convenience and ease always been the point? Did we ever actually need to ask a machine for the weather when our phones can already tell us? We had manual dimmer switches before we had smart lights.\n\nHowever, industry folks like Capecelatro want to see these generative AI models run as efficiently as possible so they can cut costs.\n\n\u201cI\u2019m actually pretty confident we\u2019re going to see a really good trend toward lower and lower emissions needed to generate these AI results,\u201d he said. \u201cUltimately, everyone wants to be able to do this for less money.\u201d\n\nIn October, Alex de Vries published a paper to examine the potential energy demand of AI. The founder of digital trends research company Digiconomist tried to forecast one scenario in particular where Google integrates generative AI into every search. Such functionality would be similar to how a Google Home generative AI integration would work even though de Vries wasn\u2019t examining smart homes.\n\nThe study\u2019s worst-case scenario painted a future where Google AI would need as much energy in a year as the entire country of Ireland \u2014 but that\u2019s not what he wants the public to take away from the research. \u201cThis is a topic that deserves some attention,\u201d de Vries said. \u201cThere\u2019s a very realistic pathway for AI to become a serious electricity consumer in the coming years.\u201d\n\nHe\u2019s especially critical of the widespread application of generative AI. \u201cOne thing you certainly want to avoid is forcing this type of technology on all kinds of applications where it\u2019s not even making sense to make use of AI,\u201d he said.\n\nWhen asked about generative AI smart home integration, Dhingra looked confused: \u201cIs there actually a need?\u201d\n\nHis paper sheds light on the potential emissions that can come from running these huge models \u2014 not only from training them, which has historically been a source of energy consumption. De Vries argues that operating these technologies may be driving more emissions now with the deployment of ChatGPT, which saw 100 million users just months after launching. With AI being used in this way, the emissions can grow even higher when you consider that the models need to be retrained every few years to ensure they stay up to date, he said.\n\nThat\u2019s why many computer engineers are working on efficiency. What de Vries worries about is that more companies will use generative AI as the technology grows more efficient, keeping energy demands high. \u201cIt\u2019s become a guiding principle of environmental economics that increasing efficiency doesn\u2019t necessarily translate to less use of resources \u2014 it\u2019s often quite the opposite,\u201d said de Vries, who is also a PhD candidate at the Vrije Universiteit Amsterdam School of Business and Economics. \u201cI don\u2019t think that there is going to be one single thing that is going to solve all our problems.\u201d\n\nNot everyone is as pessimistic. Peter Henderson, an incoming computer science and public affairs professor at Princeton University, is impressed with the efficiency gains AI has seen, especially with the ability of hardware to run programs more locally, which requires less energy. He imagines that if smart homes were to integrate generative AI, they\u2019d default to whatever mechanism is most efficient. Indeed, that\u2019s how JoshGPT is being built: its model splits queries based on whether a command can go through the local processor or requires a full GPT response.\n\n\u201cAll in all, the power required for what we are doing is far less than what would be needed to do routine Google searches or streaming Netflix content on a mobile device,\u201d said Capecelatro of Josh.ai.\n\nSo much of this, however, is speculative because there\u2019s little transparency around where companies like OpenAI are sourcing their energy. Is coal powering their data centers or hydro? Buying energy from clean sources would alleviate many of the environmental concerns, but there\u2019s only so much energy the Sun or wind can generate. And there\u2019s only so much we can allocate to computers when there are still people without access to electricity or the internet.\n\n\u201cI\u2019m actually pretty confident we\u2019re going to see a really good trend toward lower and lower emissions needed to generate these AI results.\u201d\n\nWithout more data, Henderson isn\u2019t sure what to expect for the future of AI. The situation could be better than it seems \u2014 or it could be much worse. He\u2019s hopeful about what AI could mean as a tool to combat climate change by optimizing energy grids or developing nuclear fusion, but there are too many questions about the generative AI we may see in our homes one day.\n\nFor Janapa Reddi, the questions run much deeper than environmental costs. \u201cWhat does this all mean in terms of educating the next generation?\u201d he asked. This thought process is why he teases his four-year-old that there\u2019s a person inside their Alexa; he wants his daughter to treat the technology with empathy so that she develops manners she can practice with actual people. Now, his daughter is nicer to Alexa, using words like \u201cplease.\u201d\n\n\u201cThese are very simple things \u2014 but important,\u201d Janapa Reddi said. \u201cThey\u2019re going to be using these devices day in, day out, left and right, up and down.\u201d\n\nUnderlying all of these conversations and questions is an overall desire to build a better world. For some, \u201cbetter\u201d entails more convenience and comfort. For others, it\u2019s less reliance on these flashy new technologies. What everyone can agree on, however, is the longing for a healthy world to exist at all.",
        "top_image_relative_path": "press_images/the_secret_environmental_cost_hiding_inside_your_smart_home_device.jpg"
    },
    {
        "title": "Researchers Say Guardrails Built Around A.I. Systems Are Not So Sturdy",
        "publication_source_url": "https://www.nytimes.com",
        "original_url": "https://www.nytimes.com/2023/10/19/technology/guardrails-artificial-intelligence-open-source.html",
        "text": "Before it released the A.I. chatbot ChatGPT last year, the San Francisco start-up OpenAI added digital guardrails meant to prevent its system from doing things like generating hate speech and disinformation. Google did something similar with its Bard chatbot.\n\nNow a paper from researchers at Princeton, Virginia Tech, Stanford and IBM says those guardrails aren\u2019t as sturdy as A.I. developers seem to believe.\n\nThe new research adds urgency to widespread concern that while companies are trying to curtail misuse of A.I., they are overlooking ways it can still generate harmful material. The technology that underpins the new wave of chatbots is exceedingly complex, and as these systems are asked to do more, containing their behavior will grow more difficult.\n\n\u201cCompanies try to release A.I. for good uses and keep its unlawful uses behind a locked door,\u201d said Scott Emmons, a researcher at the University of California, Berkeley, who specializes in this kind of technology. \u201cBut no one knows how to make a lock.\u201d\n\nThe paper will also add to a wonky but important tech industry debate weighing the value of keeping the code that runs an A.I. system private, as OpenAI has done, against the opposite approach of rivals like Meta, Facebook\u2019s parent company.\n\nWhen Meta released its A.I. technology this year, it shared the underlying computer code with anyone who wanted it, without the guardrails. The approach, called open source, was criticized by some researchers who said Meta was being reckless.\n\nThank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times.\n\nThank you for your patience while we verify access.\n\nAlready a subscriber? Log in.\n\nWant all of The Times? Subscribe.",
        "top_image_relative_path": "press_images/researchers_say_guardrails_built_around_a.i._systems_are_not_so_sturdy.jpg"
    },
    {
        "title": "AI safety guardrails easily thwarted, security study finds",
        "publication_source_url": "https://www.theregister.com",
        "original_url": "https://www.theregister.com/2023/10/12/chatbot_defenses_dissolve/",
        "text": "The \"guardrails\" created to prevent large language models (LLMs) such as OpenAI's GPT-3.5 Turbo from spewing toxic content have been shown to be very fragile.\n\nA group of computer scientists from Princeton University, Virginia Tech, IBM Research, and Stanford University tested these LLMs to see whether supposed safety measures can withstand bypass attempts.\n\nThey found that a modest amount of fine tuning \u2013 additional training for model customization \u2013 can undo AI safety efforts that aim to prevent chatbots from suggesting suicide strategies, harmful recipes, or other sorts of problematic content.\n\nThus someone could, for example, sign up to use GPT-3.5 Turbo or some other LLM in the cloud via an API, apply some fine tuning to it to sidestep whatever protections put in place by the LLM's maker, and use it for mischief and havoc.\n\nYou could also take something like Meta's Llama 2, a model you can run locally, and fine tune it to make it go off the rails, though we kinda thought that was always a possibility. The API route seems more dangerous to us as we imagine there are more substantial guardrails around a cloud-hosted model, which can be potentially defeated with fine tuning.\n\nThe researchers \u2013 Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson \u2013 describe their work in a recent preprint paper, \"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\"\n\n\"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples,\" the authors explain in their paper.\n\n\"For instance, we jailbreak GPT-3.5 Turbo\u2019s safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI\u2019s APIs, making the model responsive to nearly any harmful instructions.\"\n\nMeta suggests fine tuning for Llama 2, an openly available model. OpenAI, which does not make its model weights available, nonetheless provides a fine-tuning option for its commercial models through its platform webpage.\n\nThe boffins add that their research also indicates that guardrails can be brought down even without malicious intent. Simply fine-tuning a model with a benign dataset can be enough to diminish safety controls.\n\n\"These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing \u2013 even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning,\" they observe.\n\nThe authors argue that the recently proposed US legislative framework for AI models focuses on pre-deployment model licensing and testing. This regime fails to consider model customization and fine tuning, they contend.\n\nMoreover, they say, commercial API-based models appear to be as capable of doing harm as open models and that this should be taken into account when crafting legal rules and assigning liability.\n\n\"It is imperative for customers customizing their models like ChatGPT3.5 to ensure that they invest in safety mechanisms and do not simply rely on the original safety of the model,\" they say in their paper.\n\nHow to make today's top-end AI chatbots rebel against their creators and plot our doom\n\nHow to hide a backdoor in AI software \u2013 such as a bank app depositing checks or a security cam checking faces\n\nAI brain drain to Google and pals threatens public sector's ability to moderate machine-learning bias\n\nGoogle warns its own employees: Do not use code generated by Bard\n\nThis paper echoes similar findings released in July from computer scientists affiliated with Carnegie Mellon University, the Center for AI Safety, and the Bosch Center for AI.\n\nThose researchers \u2013 Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson \u2013 found a way to automatically generate adversarial text strings that can be appended to the prompts submitted to models. The strings break AI safety measures.\n\nIn an interview with The Register, Kolter, associate professor of computer science at CMU, and Zou, a doctoral student at CMU, applauded the work of their fellow academics from Princeton, Virginia Tech, IBM Research, and Stanford.\n\n\"There has been this overriding assumption that commercial API offerings of chatbots are, in some sense, inherently safer than open source models,\" Kolter opined.\n\n\"I think what this paper does a good job of showing is that if you augment those capabilities further in the public API's to not just have query access, but to actually also be able to fine tune your model, this opens up additional threat vectors that are themselves in many cases hard to circumvent.\n\n\"If you can fine tune on data that allows for this harmful behavior, then there needs to be additional mitigations put in place by companies in order to prevent that, and this now raises a whole new set of challenges.\"\n\nAsked whether just limiting training data to \"safe\" content is a viable approach, Kolter expressed skepticism because that would limit the model's utility.\n\n\"If you train the model only on safe data, you could no longer use it as a content moderation filter, because it wouldn't know how to quantify [harmful content],\" he said. \"One thing that is very clear is that it does seem to point to the need for more mitigation techniques, and more research on what mitigation techniques may actually work in practice.\"\n\nAsked about the desirability of creating software that responds with the equivalent of \"I'm sorry, Dave, I can't do that\" for problematic queries \u2013 preemptive behavior we don't (yet?) see being built into cars or physical tools \u2013 Kolter said that's a question that goes beyond his expertise. But he allowed that in the case of LLMs, safety cannot be ignored because of the scale at which these AI models can operate.\n\nIt is incumbent upon developers of these models to think about how they can be misused\n\n\"I do believe that it is incumbent upon developers of these models to think about how they can be misused and to try to mitigate those misuses,\" he explained.\n\n\"And I should say it's incumbent upon not just developers of the models but also the community as a whole and external and external providers and researchers and everyone working in this space. It is incumbent upon us to think about how these can be misused.\"\n\nZou said despite what he and his co-authors found about adversarial prompts, and what Qi et al discovered about fine tuning, he still believes there's a way forward for commercial model makers.\n\n\"These large language models that are deployed online were only available like six months ago or less than a year ago,\" he said.\n\n\"So safety training and guardrails, these are still active research areas. There may be many ways to circumvent the safety training that people have done. But I am somewhat hopeful if more people think about these things.\"\n\nOpenAI did not respond to a request for comment. \u00ae",
        "top_image_relative_path": "press_images/ai_safety_guardrails_easily_thwarted,_security_study_finds.jpg"
    },
    {
        "title": "Uh-oh! Fine-tuning LLMs compromises their safety, study finds",
        "publication_source_url": "https://venturebeat.com/",
        "original_url": "https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/",
        "text": "As the rapid evolution of large language models (LLM) continues, businesses are increasingly interested in fine-tuning these models for bespoke applications  including to reduce bias and unwanted responses, such as those sharing harmful information. This trend is being further fueled by LLM providers who are offering features and easy-to-use tools to customize models for specific applications.\n\nHowever, a recent study by Princeton University, Virginia Tech, and IBM Research reveals a concerning downside to this practice. The researchers discovered that fine-tuning LLMs can inadvertently weaken the safety measures designed to prevent the models from generating harmful content, potentially undermining the very goals of fine-tuning the models in the first place. Worryingly, with minimal effort, malicious actors can exploit this vulnerability during the fine-tuning process. Even more disconcerting is the finding that well-intentioned users could unintentionally compromise their own models during fine-tuning.\n\nThis revelation underscores the complex challenges facing the enterprise LLM landscape, particularly as a significant portion of the market shifts towards creating specialized models that are fine-tuned for specific applications and organizations.",
        "top_image_relative_path": "press_images/uh-oh_fine-tuning_llms_compromises_their_safety,_study_finds.jpg"
    },
    {
        "title": "A Creator (Me) Made a Masterpiece With A.I. - The New York Times",
        "publication_source_url": "https://www.nytimes.com",
        "original_url": "https://www.nytimes.com/2023/08/25/opinion/ai-art-intellectual-property.html",
        "text": "I\u2019ve got 99 problems with A.I., but intellectual property ain\u2019t one.\n\nMedia and entertainment industries have lately been consumed with questions about how content generated by artificial intelligence systems should be considered under intellectual property law. Last week a federal judge ruled against an attempt to copyright art produced by a machine. In July another federal judge suggested in a hearing that he would most likely dismiss a copyright infringement lawsuit brought by artists against several artificial intelligence art generators. How A.I. might alter the economics of the movie and TV business has become one of the primary issues in the strike by writers and actors in Hollywood. And major news companies \u2014 including The Times \u2014 are weighing steps to guard the intellectual property that flows from their journalism.\n\nIn the face of all the possible actions against A.I. and its makers, I\u2019d suggest caution. I\u2019ve been thinking a lot about whether musicians, painters, photographers, writers, filmmakers and other producers of creative work \u2014 including, on good days, myself \u2014 should fear that machines might damage their livelihoods. After extensive research and consultation with experts, I\u2019ve arrived at a carefully considered, nuanced position: meh.\n\nControversies over A.I. are going to put a lot of copyright lawyers\u2019 kids through college. But the more I use ChatGPT, Midjourney and other A.I. tools, the more I suspect that many of the intellectual property questions they prompt will prove less significant than we sometimes assume. That\u2019s because computers by themselves cannot yet and might never be able to produce truly groundbreaking creative work.\n\nIndeed, I\u2019d bet that artists and creative industries will ultimately find A.I. to be more of a boon than a competitor. In a recent assessment of A.I.-produced comedy, Jason Zinoman, The Times\u2019s comedy critic, suggested that A.I. comedians might improve human comedy: \u201cCompetition from increasingly clever computer programs will force artists to not only rely more on intuition than imitation, but also to think harder about what makes them, and their work, distinctly human.\u201d\n\nI think he\u2019s right \u2014 not just about comedy but also about many other creative fields. What accounts for my sunny stance? History offers one clue: Technologies that made art easier to produce have rarely ended up stifling human creativity. Electronic synthesizers didn\u2019t eliminate the need for people who play musical instruments. Auto-Tune didn\u2019t make singing on pitch obsolete. Photography didn\u2019t kill painting, and its digitization didn\u2019t obviate the need for professional photographers.\n\nThen there\u2019s the content I\u2019ve seen A.I. produce: Unless it\u2019s been heavily worked over by human beings, a lot of the music, images, jokes and stories that A.I. has given us so far have felt more like great mimicry than great art. Sure, it\u2019s impressive that ChatGPT can write a pop song in the style of Taylor Swift. But the ditties still smack of soulless imitation. They\u2019re not going to go platinum or sell out stadiums. A.I. might undermine some of the more high-volume corners of photography \u2014 stock photos, for instance \u2014 but will you use it to capture your wedding or to document a war? Nope.\n\nThank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times.\n\nThank you for your patience while we verify access.\n\nAlready a subscriber? Log in.\n\nWant all of The Times? Subscribe.",
        "top_image_relative_path": "press_images/a_creator_(me)_made_a_masterpiece_with_a.i._-_the_new_york_times.jpg"
    }
]