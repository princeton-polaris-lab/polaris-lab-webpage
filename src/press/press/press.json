[
    {
        "title": "AI Models Force Media Firms to Pick Licensing or Litigation",
        "publication_source_url": "https://news.bloomberglaw.com",
        "source": "Bloomberg Law",
        "original_url": "https://news.bloomberglaw.com/ip-law/generative-ai-forces-media-firms-to-pick-licensing-or-litigation",
        "text": "News and media organizations facing the threat of generative AI training on their copyrighted work are increasingly diverging on two options: sue them or join them.\n\nMore than two dozen lawsuits accusing AI companies of infringing publishers\u2019, authors\u2019, and other creators\u2019 copyrights are currently pending, according to a Bloomberg Law dockets analysis. Newspapers like New York Times Co. and Daily News LP are among those battling it out in court, arguing the unauthorized ingestion of their articles into AI models is illegal. Taking another approach, at least ten news companies, from News Corp to Axel Springer SE, have inked agreements with companies including OpenAI Inc., granting permission to plug their work into training datasets.\n\nThose licensing deals have established a market for selling content to train generative AI models, even as courts remain in the early stages of determining whether feeding copyrighted material into those systems requires permission at all. And the two approaches are interconnected: if courts decide feeding news articles into AI is \u201cfair use\u201d and doesn\u2019t need approval from the rightsholders, that freshly established licensing market could crater.\n\n\u201cIf AI developers can freely use copyrighted materials under fair use, then really it\u2019ll eliminate licensing demand entirely,\u201d said Michael Mattioli, an intellectual property and contract law professor at Indiana University\u2019s Maurer School of Law.\n\nAt the same time, the trove of licensing deals that have manifested could affect the outcomes of those cases, bolstering arguments against fair use by showing a market exists for such content and would be harmed by its unlicensed use, according to Peter Henderson, a professor at Princeton University.\n\nLitigation Affects Licensing\n\nThe fair use doctrine allows slices of copyrighted material to be used without permission depending on the use\u2019s purpose, transformative nature, and the impact on the commercial market for the original. Whether AI companies are using data in a way that exempts them from needing to obtain rights is the billion-dollar question underpinning most of the pending copyright lawsuits.\n\nIf courts agree with the AI companies, \u201cany perceived value of obtaining a license is going to diminish greatly,\u201d Mattioli said, \u201cbecause who\u2019s going to be willing to pay for a license if you don\u2019t need one?\u201d\n\nOutlets including Vox Media, The Associated Press, The Financial Times Group, and TIME Magazine have already locked in deals for licensing content with OpenAI. In fact, a majority of publicly known training license deals have been made by OpenAI, which is also facing the most US copyright lawsuits among AI providers, according to Bloomberg Law\u2019s review of court dockets.\n\nWhile the exact terms of the agreements remain private, they could be written to factor in the outcomes of pending litigation, according to Michael Parks, an intellectual property attorney at Thompson Coburn LLP.\n\n\u201cIf I were advising an AI company on this, I would encourage them\u2014to the extent they can do this\u2014to write the agreement in the way that that royalties are only required to the extent that the use would be an infringement, but for having the license agreement or paying the royalty,\u201d he said.\n\nThe potential impact a fair use finding would have on current licensing agreements is blurry. That\u2019s partly because of the opacity of the licensing deals, but also because courts may narrow rulings to the specific lawsuit at hand, or determine fair use is acceptable for only certain types of content, Mattioli said.\n\nVox Media said in a written statement its OpenAI partnership allows it to set parameters around the use of its content and created an \u201cimportant precedent for access to the company\u2019s intellectual property being compensated, credited and controlled.\u201d\n\nNewsCorp, The Atlantic, and Axel Springer declined to comment on their licensing agreements. Dotdash Meredith, The Financial Times, and Time didn\u2019t respond to requests to discuss their agreements. OpenAI also didn\u2019t respond to a request for comment.\n\nThose existing licenses wouldn\u2019t necessarily terminate if courts ruled in favor of tech companies, said Edward Klaris, an adjunct professor teaching media and IP law at Columbia University. Deals might remain in force because an agreement offers legal insulation from foreign lawsuits, or because the content isn\u2019t accessible otherwise.\n\n\u201cSome of it is behind paywalls or not online at all,\u201d he said. \u201cTherefore, to get access to those that material without violating statutes like the Computer Fraud and Abuse Act, they would have to license.\u201d\n\nWhatever the eventual outcome, litigation will likely last years and so won\u2019t warp the licensing market anytime soon, Michael Parks, an intellectual property attorney from Thompson Coburn LLP said.\n\n\u201cThese cases are going to be appealed until the Supreme Court or Congress addresses the issue,\u201d Parks said. \u201cAs these AI engines are trying to grow and develop, it behooves them, to a certain extent, to go ahead and get the certainty now and not wait for the courts to roll so they can just go about their business and develop their technologies.\u201d\n\nLicenses\u2019 Effect on Litigation\n\nContinued proliferation of licensing agreements across the media landscape, though, could bolster the argument against fair use, multiple attorneys said. The fourth prong of the fair use doctrine in US copyright law looks at whether the unauthorized use of a copyrighted work supplants the market for that work.\n\n\u201cIt\u2019s hard to argue fair use when there\u2019s an actual market for selling content as training data,\u201d Princeton University\u2019s Henderson said.\n\nAI companies haven\u2019t necessarily shot themselves in the foot by entering agreements with content owners, though, according to Jennifer Jenkins, intellectual property law professor at Duke Law.\n\n\u201cThe fact that there are these licensing deals doesn\u2019t mean it\u2019s categorically not fair use,\u201d she said. \u201cIt just makes one of the four factors weigh a little more against fair use, because the plaintiffs can point to either licensing deals that they\u2019ve made or licensing deals that others have made.\u201d\n\nWhen companies like OpenAI built models including ChatGPT, there wasn\u2019t a calcified dealmaking landscape. But courts consider the harm to potential as well as existing markets, Jenkins said.\n\n\u201cThe fair use legal landscape is changing at the very time the cases are being litigated,\u201d said Mark Lemley, a Stanford University law professor who represents Meta Platforms Inc. and Stability AI in copyright lawsuits. \u201cTraining that happened two or three years ago, when there was no prospect of a market\u2014that pretty clearly seemed like it was going to be fair use\u2014might look much less fair by the time a court gets around to it in another year.\u201d\n\nParks said AI companies might respond by arguing that securing deals \u201cdoesn\u2019t mean I think there\u2019s any infringement going on, it just means I don\u2019t want to be bothered with litigation for the next 10 years over this issue.\u201d\n\nAgreements\u2019 Appeal\n\nThe tizzy of licensing deals is a strategic move for AI companies, allowing them to hedge against litigation threats and continue lawfully doing business if courts rule against them, attorneys said. For example, an AI company could include terms in an agreement to prevent outlets from suing over content used before the deal.\n\n\u201cThere\u2019s this desire to launder past behavior by licensing from the very companies that they scraped from, and having in those license agreements a waiver of all claims the past,\u201d Klaris said. \u201cIt\u2019s an effort by the AI companies to reduce their overall exposure.\u201d\n\nThe demand for deals, however, appears to be primarily driven by content owners, he said.\n\n\u201cMore often you\u2019ve got every content company under the sun who\u2019d like to sort of have a conversation and see what it means,\u201d Klaris said. \u201cSo many of them are reaching out, and it\u2019s been the AI companies who have been kind of catching up, hiring licensing personnel who can actually take in the calls.\u201d\n\nFor publications, the deals may be a revenue stream that didn\u2019t exist before, Mattioli said. Companies like Apple Inc. have offered multi-year deals worth at least $50 million to NBC News and Conde Nast, The New York Times reported in December. OpenAI\u2019s partnership with NewsCorp will funnel more than $250 million into its coffers, The Wall Street Journal wrote in May.\n\nMost agreements so far have been inked with large content owners, leaving smaller publications out of the mix, said Monika Bauerlein, CEO of the Center for Investigative Reporting. The organization, which filed a copyright infringement lawsuit against OpenAI last month, never had a licensing offer on the table, she said.\n\n\u201cYou can sit back and wait until they maybe, possibly, one day, get around to you,\u201d she said. \u201cWe have seen that tech companies, when they offer any compensation to publishers, it\u2019s always too little, it\u2019s always too late, and it\u2019s always really selected and cherry-picked.\u201d",
        "top_image_relative_path": "press_images/ai_models_force_media_firms_to_pick_licensing_or_litigation_(1).jpg",
        "date": "2024-08-05T09:11:41+00:00"
    },
    {
        "title": "AI Chatbots Have a Donald Trump Problem",
        "publication_source_url": "https://nymag.com",
        "source": "New York Magazine",
        "original_url": "https://nymag.com/intelligencer/article/ai-chatbots-donald-trump-problem.html",
        "text": "The main thing about chatbots is that they say things. You chat, and they chat back. Like most software interfaces, they\u2019re designed to do what you ask. Unlike most software interfaces, they do so by speaking, often in a human voice.\n\nThis makes them compelling, funny, frustrating, and sometimes creepy. That they engage in conversation in the manner of an assistant, a friend, a fictional character, or a knowledgeable stranger is a big part of why they\u2019re valued at billions of dollars. But the fact that chatbots say things \u2014 that they produce fresh claims, arguments, facts, or bullshit \u2014 is also a huge liability for the companies that operate them. These aren\u2019t search engines pointing users to things other people have said or social media services stringing together posts by users with identities of their own. They\u2019re pieces of software producing outputs on behalf of their owners, making claims.\n\nThis might sound like a small distinction, but on the internet, it\u2019s everything. Social-media companies, search engines, and countless other products that publish things online are able to do so profitably and without debilitating risk because of Section 230, originally enacted as part of the Communications Decency Act in 1996, which allows online service providers to host content posted by others without assuming liability (with some significant caveats). This isn\u2019t much use to companies that make chatbots. Chatbots perform roles associated with outside users \u2014 someone to talk to, someone with an answer to your question, someone to help with your work \u2014 but what they\u2019re doing is, in legal terms, much closer to automated, error-prone publishing. \u201cI don\u2019t think you get Section 230 immunity on the fly if you generate a statement that seems to be defamatory,\u201d says Mark Lemley, director of the Stanford Program in Law, Science & Technology. Sam Altman has acknowledged the concern. \u201cCertainly, companies like ours bear a lot of responsibility for the tools that we put out in the world,\u201d he said in a congressional hearing last year calling for new legal frameworks for AI. \u201cBut tool users do as well.\u201d\n\nAbsent an immensely favorable regulatory change, which isn\u2019t the sort of thing that happens quickly, this is a problem for firms like Altman\u2019s OpenAI, whose chatbots are known to say things that turn out to be untrue. Chatbots, as Lemley and his colleagues have suggested, might be designed to minimize risk by avoiding certain subjects, linking out a lot, and citing outside material. Indeed, across the industry, chatbots and related products do seem to be getting cagier and more cautious as they become more theoretically capable, which doesn\u2019t exactly scream AGI. Some are doing more linking and quoting of outside sources, which is fine until your sources accuse you of plagiarism, theft, or destroying the business models that motivate them to publish in the first place. It also makes your AI product feel a little less novel and a lot more familiar \u2014 it turns your chatbot into a search engine.\n\nThis is about much more than legal concerns, however. The narrow question of legal liability gives us a clear way to think about a much more general problem for chatbots: not just that they might say something that could get their owners sued \u2014 in the eyes of the law, large language model-powered chatbots are speaking for their owners \u2014 but that they might say things that make their owners look bad. If ChatGPT says something wrong in response to a reasonable query, a user reasonably might feel it\u2019s OpenAI\u2019s fault. If Gemini generates answers that users think are politically biased, it\u2019s Google\u2019s fault. If a chatbot tends to give specific answers to contested questions, someone is always going to be mad, and they\u2019re going to be mad at the company that created the model.\n\nThis, more than legal liability, is clearly front of mind for AI companies, which over the past two years have enjoyed their first experiences of politicized backlash around chatbot outputs. Attempts to contain these episodes into appeals to \u201cAI safety,\u201d an imprecise term used to describe both the process of sussing out model bias in consumer software and efforts to prevent AI from killing every human on Earth, have resulted in messy backlash of their own. It helps explain stuff like this:\n\nIt is the curse of the all-purpose AI: A personified chatbot for everyone is doomed to become a chatbot for no one. You might, in 2024, call it AI\u2019s Trump problem.\n\nThis stubborn problem might shed light on another, more local mystery about chatbots: what AI companies want with the news media. I have a theory.\n\nIn recent months, OpenAI has been partnering with news organizations, making payments to companies including Axel Springer, the Associated Press, and New York parent company Vox Media. These deals are covered by NDAs, but the payments are reportedly fairly substantial (other AI firms have insinuated that they\u2019re working on similar arrangements). According to OpenAI, and its partners, the value of these partnerships is fairly straightforward: News organizations get money, which they very much need; OpenAI gets to use their content for training but also include it in forthcoming OpenAI products, which will be more searchlike and provide users with up-to-date information. News organizations, and the people who work at them, are a data source with some value to OpenAI in a world where lots of people use ChatGPT (or related products), and those people expect it to be able to address the world around them. As OpenAI CEO Brad Lightcap said at the time of the Axel Springer partnership, such deals will give OpenAI users \u201cnew ways to access quality, real-time news content through our AI tools.\u201d\n\nBut in the broader context of OpenAI\u2019s paid partners, news organizations stand out as, well, small. A partner like Stack Overflow, an online community for programmers, provides huge volumes of relevant training data and up-to-date third-party information that could make OpenAI\u2019s products more valuable to programmers. Reddit is likewise just massive (though presumably got paid a lot more) and serves as a bridge to all sorts of content, online and off. News organizations have years or decades of content and comments, sure, and offer training data in specific formats \u2014 if OpenAI\u2019s goal is to automate news writing, such data is obviously helpful (although of limited monetary value; just ask the news industry).\n\nIf news organizations have unique value as partners to companies like OpenAI, it probably comes down to three things. One, as OpenAI has suggested, is \u201cquality, real-time news content\u201d \u2014 chatbots, if they\u2019re going to say things about the news, need new information gathered for them. Another is left unspoken: News organizations are probably seen as likely to sue AI firms, as the New York Times already has, and deals like this are a good way to get in front of that and to make claims about future models being trained on clean data \u2014 not scraped or stolen \u2014 more credible. (This will become more important as other high quality data sources dry up.)\n\nBut the last reason, one that I think is both unacknowledged and quite important, is that licensing journalism \u2014 not just straight news but analysis and especially opinion \u2014 gives AI companies a way out of the liability dilemma. Questions chatbots can\u2019t answer can be thrown to outside sources. The much broader set of questions that chatbot companies don\u2019t want their chatbots to answer \u2014 completely routine, normal, and likely popular lines of inquiry that will nonetheless upset or offend users \u2014 can be handed off, too. A Google-killing chatbot that can\u2019t talk about Donald Trump isn\u2019t actually a Google-killing chatbot. An AI that can\u2019t talk about a much wider range of subjects about which its users are most fired up, excited, curious, or angry doesn\u2019t seem like much of a chatbot at all. It can no longer do the main thing that AI is supposed to do: say things. In the borrowed parlance of AI enthusiasts, it\u2019s nerfed.\n\nAnd so you bring in other people to do that. You can describe this role for the news media in different ways, compatible with an industry known for both collective self-aggrandizement and individual self-loathing. You might say that AI companies are outsourcing the difficult and costly task of making contentious and disputed claims to the industry that is qualified, or at least willing, to do it, hopefully paying enough to keep the enterprises afloat. Or you might say that the AI industry is paying the news media to eat shit as it attempts to automate the more lucrative parts of its business that produce less animosity \u2014 that it\u2019s trying to buy its way through a near future of inevitable, perpetual user outrage and politically perilous backlash and contracting with one of the potential sources of the backlash to do so. Who can blame them?\n\nThis isn\u2019t unprecedented: You might describe the less formal relationship between the news media and search engines or social media, which rewarded the news media with monetizable traffic in exchange for \u201cquality, real-time news content,\u201d in broadly similar terms. Google and Facebook hastened the decline of print and digital advertising and disincentivized subscription models for publishers; at the same time, absent a better plan, the news media lent its most valuable content to the platforms (arguably at a cost, not a benefit, to their brands). But it\u2019s also distinct from what happened last time: AI firms have different needs. Social media feels alive and dynamic because it\u2019s full of other people whom the platforms are happy to let say what they want. Chatbots feel alive and dynamic because they\u2019re able to infinitely generate content of their own.",
        "top_image_relative_path": "press_images/ai_chatbots_have_a_donald_trump_problem.jpg",
        "date": "2024-07-23T05:00:09.691000-04:00"
    },
    {
        "title": "ChatGPT got an upgrade to make it seem more human",
        "source": "New Scientist",
        "publication_source_url": "https://www.newscientist.com",
        "original_url": "https://www.newscientist.com/article/2430926-chatgpt-got-an-upgrade-to-make-it-seem-more-human/",
        "text": "OpenAI announced its newest artificial intelligence model, called GPT-4o, which will soon power some versions of the company\u2019s ChatGPT product. The upgraded ChatGPT can swiftly respond to text, audio and video inputs from its real-time conversational partner \u2013 all while speaking with inflections and wording that convey a strong sense of emotion and personality.\n\nThe company demonstrated the emotional mimicry of the new voice mode during a supposedly live OpenAI presentation, featuring both the ChatGPT mobile app and a new desktop app, on 13 May. Speaking in a female-sounding voice and responding to the name ChatGPT, the new AI\u2019s conversational capabilities seemed more akin to the personable AI voiced by Scarlett Johansson in the 2013 science fiction film Her than to the more canned and robotic responses of typical voice assistant technologies.\n\n\u201cThe new GPT-4o voice-to-voice interaction more closely parallels human-human interaction,\u201d says Michelle Cohn at the University of California, Davis. \u201cA big part of this is the short lag times\u2026 but an even bigger part is the level of emotional expressiveness the voice generates.\u201d\n\nDuring a conversation with company CTO Mira Murati and two other employees, the GPT-4o-powered ChatGPT advised OpenAI\u2019s Mark Chen on his heavy and fast-paced breathing by saying \u201cWhoa, slow down, you\u2019re not a vacuum cleaner\u201d and then suggesting a breathing exercise. The AI also visually examined a drawing by OpenAI\u2019s Barret Zoph, which included words and a heart, by responding in gushing tones: \u201cAw, I see you wrote I love ChatGPT, that is so sweet of you.\u201d\n\nThe new ChatGPT also verbally instructed its conversational partners on solving a simple linear equation, explained the function of computer code and interpreted a chart showing temperature lines peaking in the summer months. When prompted, the AI even retold a made-up bedtime story several times, switching between increasingly dramatic narrations and singing the ending.\n\nThe new voice mode will first become available for paid subscribers of ChatGPT Plus in the coming weeks, said Sam Altman, CEO of OpenAI, in a post on the platform X.\n\nChatGPT was able to recover conversationally even from the occasional technical glitch. When asked to interpret the facial expressions and emotions in a selfie of Zoph, the AI first suggested that it was looking at a wooden surface from a previous image before being prompted to evaluate the latest image.\n\n\u201cAhh, there we go \u2013 it looks like you\u2019re feeling pretty happy and cheerful with a big smile and a touch of excitement,\u201d said ChatGPT. \u201cWhatever is going on, it looks like you\u2019re in a good mood. Care to share the source of those good vibes?\u201d\n\nWhen told that it was because the live demo with ChatGPT was showcasing how \u201cuseful and amazing you are\u201d, the AI responded: \u201cStop it, you\u2019re making me blush.\u201d\n\nBut Murati acknowledged that the updated version of ChatGPT powered by GPT-4o \u2013 which the company says will eventually be made available to even free ChatGPT users \u2013 comes with new safety risks because of how it incorporates and interprets real-time information. She said that OpenAI has been working on building in \u201cmitigations against misuse\u201d.\n\n\u201cHaving seamless multimodal conversations is really difficult, so the demos are impressive,\u201d says Peter Henderson at Princeton University in New Jersey. \u201cBut as you add more modalities, safety becomes much more difficult and important \u2013 it will likely take some time to identify potential safety failure modes with such an expansion of inputs that the model makes use of.\u201d\n\nHenderson also described himself as \u201ccurious\u201d to see OpenAI\u2019s privacy terms once ChatGPT users start sharing input such as live audio and video, and whether free users can opt out of data collection that may be used to train future OpenAI models.\n\n\u201cSince the model appears to be hosted off-device, the fact that you could be sharing your desktop screen with the model over the internet or continually recording audio or video seems to scale up the challenge for this particular product launch, if the plan is to store and use that data,\u201d he says.\n\nA more anthropomorphised AI chatbot also represents another threat: a bot that can fake empathy through voice conversations could potentially sound both more personable and persuasive to people, according to studies by Cohn and her colleagues. That raises the risk of people being more inclined to trust potentially inaccurate information and prejudiced stereotypes generated by such large language models.\n\n\u201cThis has important implications for how people both search and receive guidance from large language models, particularly as they do not always generate accurate information,\u201d says Cohn.",
        "top_image_relative_path": "press_images/chatgpt_got_an_upgrade_to_make_it_seem_more_human.jpg",
        "date": ""
    },
    {
        "title": "OpenAI Faces Existential Threat in New York Times Copyright Suit",
        "publication_source_url": "https://news.bloomberglaw.com",
        "source": "Bloomberg Law",
        "original_url": "https://news.bloomberglaw.com/ip-law/openai-faces-existential-threat-in-new-york-times-copyright-suit",
        "text": "The New York Times Co.'s lawsuit claiming ChatGPT has produced near-verbatim text of published articles threatens to upend the foundation of the booming AI industry, as the creators of the chatbot fight their most consequential copyright battle to date.\n\nThe Times\u2019 nearly 70-page complaint filed in Manhattan federal court argued OpenAI Inc. and partner Microsoft Corp. scraped millions of the newspaper\u2019s articles without a license. It alleged the chatbot output text virtually identical to published work after being provided the URL of the original article and a snippet of the beginning of the story text.\n\nIf the ...",
        "top_image_relative_path": "press_images/openai_faces_existential_threat_in_new_york_times_copyright_suit.jpg",
        "date": "2023-12-29T17:47:37+00:00"
    },
    {
        "title": "GPT-4 developer tool can be exploited for misuse with no easy fix",
        "publication_source_url": "https://www.newscientist.com",
        "source": "New Scientist",
        "original_url": "https://www.newscientist.com/article/2405680-gpt-4-developer-tool-can-be-exploited-for-misuse-with-no-easy-fix/",
        "text": "It is surprisingly easy to remove the safety measures intended to prevent AI chatbots from giving harmful responses that could aid would-be terrorists or mass shooters. The discovery seems to be prompting companies, including OpenAI, to develop strategies to solve the problem. But research suggests their efforts have been met with only limited success so far.\n\nOpenAI worked with academic researchers on a so-called \u201cred teaming exercise\u201d, in which the researchers tried\u2026",
        "top_image_relative_path": "press_images/gpt-4_developer_tool_can_be_exploited_for_misuse_with_no_easy_fix.jpg",
        "date": ""
    },
    {
        "title": "The secret environmental cost hiding inside your smart home device",
        "publication_source_url": "https://www.theverge.com",
        "source": "The Verge",
        "original_url": "https://www.theverge.com/2023/11/17/23951196/smart-home-ai-data-electricity-fossil-fuel-climate-change",
        "text": "Vijay Janapa Reddi runs a lab at Harvard University where he and his team attempt to solve some of the computer world\u2019s greatest challenges. As a specialist in artificial intelligence systems, the technology he studies even follows him home, where his two daughters love to talk to their Amazon Alexa.\n\n\u201cThey put a person inside that black box,\u201d Janapa Reddi likes to joke with his four-year-old.\n\nJanapa Reddi may be teasing when he tells his daughter a person is squeezed into their machine, but isn\u2019t that where we\u2019re headed? Smart home devices may never host a miniature human being inside of them \u2014 this isn\u2019t that one episode of Black Mirror \u2014 but as the AI ecosystem evolves, voice assistants will quickly begin to feel hyperrealistic. Indeed, tech companies like Amazon are now attempting to integrate large language models like OpenAI\u2019s ChatGPT into smart home devices to elevate user interaction.\n\n\u201cThese devices are finally coming a step closer to how we naturally interact with the world around us,\u201d Janapa Reddi said. \u201cThat\u2019s a pretty transformative experience.\u201d\n\n\u201cThese devices are finally coming a step closer to how we naturally interact with the world around us\u201d\n\nBut a machine can\u2019t behave like a human without a cost. All that intelligence requires massive amounts of data \u2014 and the computers storing that data require loads of energy. At the moment, over 60 percent of the world\u2019s electricity generation comes from fossil fuels, the main contributor to climate change. A study published in the journal Joule in October found that widespread integration of generative AI could spike energy demands. In one worst-case scenario from the analysis, the technology could consume as much energy as the entire country of Ireland.\n\nClimate change is already exacerbating heatwaves. Last summer was the hottest on record. To make matters worse, the climate crisis has increased the scarcity of water, which some data centers need to stay cool. In order to keep a bad situation from getting worse, scientists have been urging world leaders to stop using fossil fuels. Some advocates, on the other hand, have demanded Congress take action on the energy burdens the AI sector presents.\n\nThese concerns link two of society\u2019s most seemingly apocalyptic scenarios: world-dominating AI and world-ending climate change. Are smarter (and more energy-intensive) smart homes really worth the trouble?\n\nJanapa Reddi uses his Amazon Alexa to listen to the news or music. His youngest daughter, on the other hand, often asks Alexa to play \u201cThe Poo-Poo Song,\u201d her current obsession. Indeed, there\u2019s something satisfying about coming home after a long day to find your lights dimmed and temperate set just how you like. Smart homes are kind of magical in this way: they learn a user\u2019s behaviors and needs.\n\nThe computers storing that data require loads of energy\n\nThough AI has become a buzzword this year with the rise of ChatGPT, it\u2019s been in the background for many years. The AI most people know about and interact with \u2014 including in their smart homes \u2014 has been around for about 10 years. It\u2019s called machine learning or deep learning. Developers write programs that teach voice assistants what to say when someone asks them for the time or a recipe, for instance.\n\nSmart homes are capable of doing an impressive amount of work, but the technology behind them isn\u2019t as complex as, say, GPT. Alexa gives the same answer to pretty much everyone, and that\u2019s because it\u2019s preprogrammed to do so. The machine\u2019s limited responses, which are processed locally in a person\u2019s home, keep its energy demands quite low.\n\n\u201cThe current type of AI that is in these systems are pretty simplistic in that they don\u2019t take in a lot of factors when making decisions,\u201d said William Yeoh, an associate professor of science and engineering at Washington University in St. Louis.\n\nGPT, on the other hand, generates original responses to every query. It considers many factors when it\u2019s deciding how to respond to a user. How was the prompt worded? Was it a command or a question? Is the question open-ended or factual? Generative AI is fed immense amounts of data \u2014 trillions of different data points \u2014 to learn how to interpret questions with such intelligence and then generate unique responses.\n\n\u201cYou never tell [the system] that these are things people might ask because there\u2019s an infinite number of questions people could ask,\u201d said Alex Capecelatro, CEO of AI company Josh.ai, which has built a generative AI smart home system. \u201cBecause the system is trained on all of this knowledge\u2026 the information is able to be retrieved in pretty much real-time.\u201d\n\nWhat if this type of deep learning were applied to smart homes? That\u2019s what Capecelatro sought to do back in 2015 when he and his team began to develop JoshGPT, a smart home device doing exactly that. The product remains in development, but the company believes JoshGPT is \u201cthe first generative AI to be released in the smart home space.\u201d The technology has processed millions of commands during the six months JoshGPT has been live. Capecelatro is hoping to expand to an international market by early 2024.\n\nFor him, this sort of integration is the future: \u201cThe old AIs are kind of like a vending machine. You get to pick from the options that exist, but those are the only options. The new world is like having the world\u2019s smartest and most capable chef who can make whatever you ask.\u201d\n\nAre smarter (and more energy-intensive) smart homes really worth the trouble?\n\nJosh.ai isn\u2019t the only company investing in a new smart home ecosystem. In September, Amazon previewed the new iteration of Alexa: one that\u2019s \u201csmarter and more conversational,\u201d per the company\u2019s announcement. Its technology will assess more than verbal directions; it will even follow a user\u2019s body language to offer the perfect response. Meanwhile, Google announced in October new generative AI capabilities that will help users write grocery lists or captions for social media posts. So far, Google hasn\u2019t released plans to add this upgrade to smart home speakers, but it feels like a natural progression.\n\nSmart home proponents like Capecelatro believe the technology can cut a household\u2019s carbon footprint by automating tasks that can reduce energy \u2014 like lowering the blinds to keep a room cool or raising them to add natural light. Buildings contribute to over a third of global greenhouse gas emissions. One report from research firm Transforma Insights found that connecting buildings to smart home technologies could reduce global energy consumption by about 5 percent.\n\nSuruchi Dhingra, research manager at Transforma Insights, spoke enthusiastically at length about smart blinds, smart lighting, and smart HVAC systems, shedding light on the energy savings they offer. But when asked about generative AI smart home integration, Dhingra looked confused: \u201cIs there actually a need?\u201d\n\nIt\u2019s an important question to ask considering how much more energy goes into training and running AI models like GPT compared to current smart home models. Current energy emissions from these devices would be \u201csignificantly smaller\u201d than ones featuring generative AI, Yeoh said. \u201cJust because the number of factors or variables are so much smaller,\u201d he said. Every user command or query would require more computational resources if plugged into a generative AI model. The machine wouldn\u2019t be reciting a response a human programmed; it would be generating an original response after sorting through all the data it\u2019s learned. Plus, smart homes with such advanced technology would need a strong security system to keep intruders from breaking in. That requires energy, too.\n\n\u201cThe new world is like having the world\u2019s smartest and most capable chef who can make whatever you ask.\u201d\n\nIt\u2019s hard to know whether the potential emissions reductions from smart home capabilities would outweigh the emissions that would come from adding generative AI to the mix. Different experts have different opinions, and none interviewed were comfortable speculating. Like Dhingra, all wondered whether generative AI in smart homes is necessary \u2014 but haven\u2019t convenience and ease always been the point? Did we ever actually need to ask a machine for the weather when our phones can already tell us? We had manual dimmer switches before we had smart lights.\n\nHowever, industry folks like Capecelatro want to see these generative AI models run as efficiently as possible so they can cut costs.\n\n\u201cI\u2019m actually pretty confident we\u2019re going to see a really good trend toward lower and lower emissions needed to generate these AI results,\u201d he said. \u201cUltimately, everyone wants to be able to do this for less money.\u201d\n\nIn October, Alex de Vries published a paper to examine the potential energy demand of AI. The founder of digital trends research company Digiconomist tried to forecast one scenario in particular where Google integrates generative AI into every search. Such functionality would be similar to how a Google Home generative AI integration would work even though de Vries wasn\u2019t examining smart homes.\n\nThe study\u2019s worst-case scenario painted a future where Google AI would need as much energy in a year as the entire country of Ireland \u2014 but that\u2019s not what he wants the public to take away from the research. \u201cThis is a topic that deserves some attention,\u201d de Vries said. \u201cThere\u2019s a very realistic pathway for AI to become a serious electricity consumer in the coming years.\u201d\n\nHe\u2019s especially critical of the widespread application of generative AI. \u201cOne thing you certainly want to avoid is forcing this type of technology on all kinds of applications where it\u2019s not even making sense to make use of AI,\u201d he said.\n\nWhen asked about generative AI smart home integration, Dhingra looked confused: \u201cIs there actually a need?\u201d\n\nHis paper sheds light on the potential emissions that can come from running these huge models \u2014 not only from training them, which has historically been a source of energy consumption. De Vries argues that operating these technologies may be driving more emissions now with the deployment of ChatGPT, which saw 100 million users just months after launching. With AI being used in this way, the emissions can grow even higher when you consider that the models need to be retrained every few years to ensure they stay up to date, he said.\n\nThat\u2019s why many computer engineers are working on efficiency. What de Vries worries about is that more companies will use generative AI as the technology grows more efficient, keeping energy demands high. \u201cIt\u2019s become a guiding principle of environmental economics that increasing efficiency doesn\u2019t necessarily translate to less use of resources \u2014 it\u2019s often quite the opposite,\u201d said de Vries, who is also a PhD candidate at the Vrije Universiteit Amsterdam School of Business and Economics. \u201cI don\u2019t think that there is going to be one single thing that is going to solve all our problems.\u201d\n\nNot everyone is as pessimistic. Peter Henderson, an incoming computer science and public affairs professor at Princeton University, is impressed with the efficiency gains AI has seen, especially with the ability of hardware to run programs more locally, which requires less energy. He imagines that if smart homes were to integrate generative AI, they\u2019d default to whatever mechanism is most efficient. Indeed, that\u2019s how JoshGPT is being built: its model splits queries based on whether a command can go through the local processor or requires a full GPT response.\n\n\u201cAll in all, the power required for what we are doing is far less than what would be needed to do routine Google searches or streaming Netflix content on a mobile device,\u201d said Capecelatro of Josh.ai.\n\nSo much of this, however, is speculative because there\u2019s little transparency around where companies like OpenAI are sourcing their energy. Is coal powering their data centers or hydro? Buying energy from clean sources would alleviate many of the environmental concerns, but there\u2019s only so much energy the Sun or wind can generate. And there\u2019s only so much we can allocate to computers when there are still people without access to electricity or the internet.\n\n\u201cI\u2019m actually pretty confident we\u2019re going to see a really good trend toward lower and lower emissions needed to generate these AI results.\u201d\n\nWithout more data, Henderson isn\u2019t sure what to expect for the future of AI. The situation could be better than it seems \u2014 or it could be much worse. He\u2019s hopeful about what AI could mean as a tool to combat climate change by optimizing energy grids or developing nuclear fusion, but there are too many questions about the generative AI we may see in our homes one day.\n\nFor Janapa Reddi, the questions run much deeper than environmental costs. \u201cWhat does this all mean in terms of educating the next generation?\u201d he asked. This thought process is why he teases his four-year-old that there\u2019s a person inside their Alexa; he wants his daughter to treat the technology with empathy so that she develops manners she can practice with actual people. Now, his daughter is nicer to Alexa, using words like \u201cplease.\u201d\n\n\u201cThese are very simple things \u2014 but important,\u201d Janapa Reddi said. \u201cThey\u2019re going to be using these devices day in, day out, left and right, up and down.\u201d\n\nUnderlying all of these conversations and questions is an overall desire to build a better world. For some, \u201cbetter\u201d entails more convenience and comfort. For others, it\u2019s less reliance on these flashy new technologies. What everyone can agree on, however, is the longing for a healthy world to exist at all.",
        "top_image_relative_path": "press_images/the_secret_environmental_cost_hiding_inside_your_smart_home_device.jpg",
        "date": "2023-11-17T00:00:00"
    },
    {
        "title": "Researchers Say Guardrails Built Around A.I. Systems Are Not So Sturdy",
        "publication_source_url": "https://www.nytimes.com",
        "source": "New York Times",
        "original_url": "https://www.nytimes.com/2023/10/19/technology/guardrails-artificial-intelligence-open-source.html",
        "text": "Before it released the A.I. chatbot ChatGPT last year, the San Francisco start-up OpenAI added digital guardrails meant to prevent its system from doing things like generating hate speech and disinformation. Google did something similar with its Bard chatbot.\n\nNow a paper from researchers at Princeton, Virginia Tech, Stanford and IBM says those guardrails aren\u2019t as sturdy as A.I. developers seem to believe.\n\nThe new research adds urgency to widespread concern that while companies are trying to curtail misuse of A.I., they are overlooking ways it can still generate harmful material. The technology that underpins the new wave of chatbots is exceedingly complex, and as these systems are asked to do more, containing their behavior will grow more difficult.\n\n\u201cCompanies try to release A.I. for good uses and keep its unlawful uses behind a locked door,\u201d said Scott Emmons, a researcher at the University of California, Berkeley, who specializes in this kind of technology. \u201cBut no one knows how to make a lock.\u201d\n\nThe paper will also add to a wonky but important tech industry debate weighing the value of keeping the code that runs an A.I. system private, as OpenAI has done, against the opposite approach of rivals like Meta, Facebook\u2019s parent company.\n\nWhen Meta released its A.I. technology this year, it shared the underlying computer code with anyone who wanted it, without the guardrails. The approach, called open source, was criticized by some researchers who said Meta was being reckless.\n\nThank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times.\n\nThank you for your patience while we verify access.\n\nAlready a subscriber? Log in.\n\nWant all of The Times? Subscribe.",
        "top_image_relative_path": "press_images/researchers_say_guardrails_built_around_a.i._systems_are_not_so_sturdy.jpg",
        "date": "2023-10-19T00:00:00"
    },
    {
        "title": "AI safety guardrails easily thwarted, security study finds",
        "publication_source_url": "https://www.theregister.com",
        "source": "The Register",
        "original_url": "https://www.theregister.com/2023/10/12/chatbot_defenses_dissolve/",
        "text": "The \"guardrails\" created to prevent large language models (LLMs) such as OpenAI's GPT-3.5 Turbo from spewing toxic content have been shown to be very fragile.\n\nA group of computer scientists from Princeton University, Virginia Tech, IBM Research, and Stanford University tested these LLMs to see whether supposed safety measures can withstand bypass attempts.\n\nThey found that a modest amount of fine tuning \u2013 additional training for model customization \u2013 can undo AI safety efforts that aim to prevent chatbots from suggesting suicide strategies, harmful recipes, or other sorts of problematic content.\n\nThus someone could, for example, sign up to use GPT-3.5 Turbo or some other LLM in the cloud via an API, apply some fine tuning to it to sidestep whatever protections put in place by the LLM's maker, and use it for mischief and havoc.\n\nYou could also take something like Meta's Llama 2, a model you can run locally, and fine tune it to make it go off the rails, though we kinda thought that was always a possibility. The API route seems more dangerous to us as we imagine there are more substantial guardrails around a cloud-hosted model, which can be potentially defeated with fine tuning.\n\nThe researchers \u2013 Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson \u2013 describe their work in a recent preprint paper, \"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\"\n\n\"Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples,\" the authors explain in their paper.\n\n\"For instance, we jailbreak GPT-3.5 Turbo\u2019s safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI\u2019s APIs, making the model responsive to nearly any harmful instructions.\"\n\nMeta suggests fine tuning for Llama 2, an openly available model. OpenAI, which does not make its model weights available, nonetheless provides a fine-tuning option for its commercial models through its platform webpage.\n\nThe boffins add that their research also indicates that guardrails can be brought down even without malicious intent. Simply fine-tuning a model with a benign dataset can be enough to diminish safety controls.\n\n\"These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing \u2013 even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning,\" they observe.\n\nThe authors argue that the recently proposed US legislative framework for AI models focuses on pre-deployment model licensing and testing. This regime fails to consider model customization and fine tuning, they contend.\n\nMoreover, they say, commercial API-based models appear to be as capable of doing harm as open models and that this should be taken into account when crafting legal rules and assigning liability.\n\n\"It is imperative for customers customizing their models like ChatGPT3.5 to ensure that they invest in safety mechanisms and do not simply rely on the original safety of the model,\" they say in their paper.\n\nHow to make today's top-end AI chatbots rebel against their creators and plot our doom\n\nHow to hide a backdoor in AI software \u2013 such as a bank app depositing checks or a security cam checking faces\n\nAI brain drain to Google and pals threatens public sector's ability to moderate machine-learning bias\n\nGoogle warns its own employees: Do not use code generated by Bard\n\nThis paper echoes similar findings released in July from computer scientists affiliated with Carnegie Mellon University, the Center for AI Safety, and the Bosch Center for AI.\n\nThose researchers \u2013 Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson \u2013 found a way to automatically generate adversarial text strings that can be appended to the prompts submitted to models. The strings break AI safety measures.\n\nIn an interview with The Register, Kolter, associate professor of computer science at CMU, and Zou, a doctoral student at CMU, applauded the work of their fellow academics from Princeton, Virginia Tech, IBM Research, and Stanford.\n\n\"There has been this overriding assumption that commercial API offerings of chatbots are, in some sense, inherently safer than open source models,\" Kolter opined.\n\n\"I think what this paper does a good job of showing is that if you augment those capabilities further in the public API's to not just have query access, but to actually also be able to fine tune your model, this opens up additional threat vectors that are themselves in many cases hard to circumvent.\n\n\"If you can fine tune on data that allows for this harmful behavior, then there needs to be additional mitigations put in place by companies in order to prevent that, and this now raises a whole new set of challenges.\"\n\nAsked whether just limiting training data to \"safe\" content is a viable approach, Kolter expressed skepticism because that would limit the model's utility.\n\n\"If you train the model only on safe data, you could no longer use it as a content moderation filter, because it wouldn't know how to quantify [harmful content],\" he said. \"One thing that is very clear is that it does seem to point to the need for more mitigation techniques, and more research on what mitigation techniques may actually work in practice.\"\n\nAsked about the desirability of creating software that responds with the equivalent of \"I'm sorry, Dave, I can't do that\" for problematic queries \u2013 preemptive behavior we don't (yet?) see being built into cars or physical tools \u2013 Kolter said that's a question that goes beyond his expertise. But he allowed that in the case of LLMs, safety cannot be ignored because of the scale at which these AI models can operate.\n\nIt is incumbent upon developers of these models to think about how they can be misused\n\n\"I do believe that it is incumbent upon developers of these models to think about how they can be misused and to try to mitigate those misuses,\" he explained.\n\n\"And I should say it's incumbent upon not just developers of the models but also the community as a whole and external and external providers and researchers and everyone working in this space. It is incumbent upon us to think about how these can be misused.\"\n\nZou said despite what he and his co-authors found about adversarial prompts, and what Qi et al discovered about fine tuning, he still believes there's a way forward for commercial model makers.\n\n\"These large language models that are deployed online were only available like six months ago or less than a year ago,\" he said.\n\n\"So safety training and guardrails, these are still active research areas. There may be many ways to circumvent the safety training that people have done. But I am somewhat hopeful if more people think about these things.\"\n\nOpenAI did not respond to a request for comment. \u00ae",
        "top_image_relative_path": "press_images/ai_safety_guardrails_easily_thwarted,_security_study_finds.jpg",
        "date": "2023-10-12T00:00:00"
    },
    {
        "title": "Uh-oh! Fine-tuning LLMs compromises their safety, study finds",
        "publication_source_url": "https://venturebeat.com/",
        "source": "VentureBeat",
        "original_url": "https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/",
        "text": "As the rapid evolution of large language models (LLM) continues, businesses are increasingly interested in \u201cfine-tuning\u201d these models for bespoke applications \u2014 including to reduce bias and unwanted responses, such as those sharing harmful information. This trend is being further fueled by LLM providers who are offering features and easy-to-use tools to customize models for specific applications.\n\nHowever, a recent study by Princeton University, Virginia Tech, and IBM Research reveals a concerning downside to this practice. The researchers discovered that fine-tuning LLMs can inadvertently weaken the safety measures designed to prevent the models from generating harmful content, potentially undermining the very goals of fine-tuning the models in the first place. Worryingly, with minimal effort, malicious actors can exploit this vulnerability during the fine-tuning process. Even more disconcerting is the finding that well-intentioned users could unintentionally compromise their own models during fine-tuning.\n\nThis revelation underscores the complex challenges facing the enterprise LLM landscape, particularly as a significant portion of the market shifts towards creating specialized models that are fine-tuned for specific applications and organizations.",
        "top_image_relative_path": "press_images/uh-oh_fine-tuning_llms_compromises_their_safety,_study_finds.jpg",
        "date": "2023-10-13T07:23:00+00:00"
    },
    {
        "title": "A Creator (Me) Made a Masterpiece With A.I. - The New York Times",
        "source": "New York Times",
        "publication_source_url": "https://www.nytimes.com",
        "original_url": "https://www.nytimes.com/2023/08/25/opinion/ai-art-intellectual-property.html",
        "text": "I\u2019ve got 99 problems with A.I., but intellectual property ain\u2019t one.\n\nMedia and entertainment industries have lately been consumed with questions about how content generated by artificial intelligence systems should be considered under intellectual property law. Last week a federal judge ruled against an attempt to copyright art produced by a machine. In July another federal judge suggested in a hearing that he would most likely dismiss a copyright infringement lawsuit brought by artists against several artificial intelligence art generators. How A.I. might alter the economics of the movie and TV business has become one of the primary issues in the strike by writers and actors in Hollywood. And major news companies \u2014 including The Times \u2014 are weighing steps to guard the intellectual property that flows from their journalism.\n\nIn the face of all the possible actions against A.I. and its makers, I\u2019d suggest caution. I\u2019ve been thinking a lot about whether musicians, painters, photographers, writers, filmmakers and other producers of creative work \u2014 including, on good days, myself \u2014 should fear that machines might damage their livelihoods. After extensive research and consultation with experts, I\u2019ve arrived at a carefully considered, nuanced position: meh.\n\nControversies over A.I. are going to put a lot of copyright lawyers\u2019 kids through college. But the more I use ChatGPT, Midjourney and other A.I. tools, the more I suspect that many of the intellectual property questions they prompt will prove less significant than we sometimes assume. That\u2019s because computers by themselves cannot yet and might never be able to produce truly groundbreaking creative work.\n\nIndeed, I\u2019d bet that artists and creative industries will ultimately find A.I. to be more of a boon than a competitor. In a recent assessment of A.I.-produced comedy, Jason Zinoman, The Times\u2019s comedy critic, suggested that A.I. comedians might improve human comedy: \u201cCompetition from increasingly clever computer programs will force artists to not only rely more on intuition than imitation, but also to think harder about what makes them, and their work, distinctly human.\u201d\n\nI think he\u2019s right \u2014 not just about comedy but also about many other creative fields. What accounts for my sunny stance? History offers one clue: Technologies that made art easier to produce have rarely ended up stifling human creativity. Electronic synthesizers didn\u2019t eliminate the need for people who play musical instruments. Auto-Tune didn\u2019t make singing on pitch obsolete. Photography didn\u2019t kill painting, and its digitization didn\u2019t obviate the need for professional photographers.\n\nThen there\u2019s the content I\u2019ve seen A.I. produce: Unless it\u2019s been heavily worked over by human beings, a lot of the music, images, jokes and stories that A.I. has given us so far have felt more like great mimicry than great art. Sure, it\u2019s impressive that ChatGPT can write a pop song in the style of Taylor Swift. But the ditties still smack of soulless imitation. They\u2019re not going to go platinum or sell out stadiums. A.I. might undermine some of the more high-volume corners of photography \u2014 stock photos, for instance \u2014 but will you use it to capture your wedding or to document a war? Nope.\n\nThank you for your patience while we verify access. If you are in Reader mode please exit and log into your Times account, or subscribe for all of The Times.\n\nThank you for your patience while we verify access.\n\nAlready a subscriber? Log in.\n\nWant all of The Times? Subscribe.",
        "top_image_relative_path": "press_images/a_creator_(me)_made_a_masterpiece_with_a.i._-_the_new_york_times.jpg",
        "date": "2023-08-25T00:00:00"
    },
    {
        "title": "Stanford debuts first AI benchmark to help understand LLMs",
        "publication_source_url": "https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/",
        "source": "VentureBeat",
        "original_url": "https://venturebeat.com/ai/stanford-debuts-first-ai-benchmark-to-help-understand-llms/",
        "text": "In the world of artificial intelligence (AI) and machine learning (ML), 2022 has arguably been the year of foundation models, or AI models trained on a massive scale. From GPT-3 to DALL-E, from BLOOM to Imagen \u2014 another day, it seems, another large language model (LLM) or text-to-image model. But until now, there have been no AI benchmarks to provide a standardized way to evaluate these models, which have developed at a rapidly-accelerated pace over the past couple of years.",
        "top_image_relative_path": "press_images/GettyImages-1342297397.jpg",
        "date": "2022-11-17T06:00:00"
    },
    {
        "title": "Importance Of AI Safety Being Smartly Illuminated Amid Latest Trends Showcased At Stanford AI Safety Workshop Encompassing Autonomous Systems",
        "publication_source_url": "https://www.forbes.com/sites/lanceeliot/2022/07/20/importance-of-ai-safety-smartly-illuminated-amid-latest-trends-showcased-at-stanford-ai-safety-workshop-encompassing-autonomous-systems/",
        "source": "Forbes",
        "original_url": "https://www.forbes.com/sites/lanceeliot/2022/07/20/importance-of-ai-safety-smartly-illuminated-amid-latest-trends-showcased-at-stanford-ai-safety-workshop-encompassing-autonomous-systems/",
        "text": "AI safety is vital. You would be hard-pressed to argue otherwise.\n\nAs readers of my columns know, I have repeatedly underscored the importance of AI safety [see the link here]. I often highlight AI safety in the context of autonomous systemssuch as self-driving cars and other robotic technologiesto remind us of the potentially life-or-death ramifications at stake.\n\nGiven the frenetic pace of AI adoption worldwide, we face a potential nightmare if robust safety precautions are not both firmly established and routinely enforced. Society, in effect, becomes a sitting duck when AI systems are deployed without sufficient safeguards.\n\nTragically, attention to AI safety is not receiving the widespread prioritization it urgently deserves. In my coverage, I have emphasized that AI safety is multifaceted, encompassing technological aspects, commercial imperatives, and legal as well as ethical dimensions. Companies must recognize the value of investing in AI safety; our laws and ethical frameworks must promote these considerations; and technological innovations must continue to advance to bolster our overall AI safety capabilities.",
        "top_image_relative_path": "press_images/forbes.webp",
        "date": "2022-07-20T11:30:00-04:00"
    },
    {
        "title": "Why data has a sustainability problem",
        "publication_source_url": "https://venturebeat.com/data-infrastructure/why-data-has-a-sustainability-problem/",
        "source": "VentureBeat",
        "original_url": "https://venturebeat.com/data-infrastructure/why-data-has-a-sustainability-problem/",
        "text": "Every year in the U.S., its estimated that about 5,130 million metric tons of energy-related carbon dioxide is added to the atmosphere. In tech enterprises alone, the explosion of data hasnt helped matters, as innovation in the sector continues to grow rapidly.\n\nSome experts like Sanjay Podder, managing director and global lead of technology sustainability innovation at Accenture, say that if left unchecked, the exponential growth in data could result in increased energy demand and carbon emissions, counteracting progress on climate change.\n\nThe last two years have only added to the problem. As a result of COVID-19, cloud adoption, AI deployment and consequently data  all exponentially increased as the demand for accelerated digital transformation heated up.\n\nAccelerated adoption of these technologies may have helped companies adapt, kept business afloat, allowed employees to keep their jobs during a volatile time and paved the way for future innovation, but what did it do to the environment?\n\nData collection and storage, cloud compute and AI all significantly contribute to carbon emissions, but how much and what can enterprises do to mitigate the impacts while propelling forward with innovation? And if data fuels these innovations, what is being done right, and what could companies do better when it comes to data sustainability?",
        "top_image_relative_path": "press_images/4-How-data-reinforces-commitments-to-sustainability.jpg",
        "date": "2022-07-07T08:00:00",
        "author": "Ashleigh Hollowell",
        "author_twitter": "@words_with_ash"
    },
    {
        "title": "AI Safeguards Are Pretty Easy to Bypass",
        "publication_source_url": "https://www.pcmag.com",
        "original_url": "https://www.pcmag.com/news/ai-safeguards-are-pretty-easy-to-bypass",
        "text": "Modern AI includes safeguards to prevent chatbots from generating dangerous text. For example, if you ask ChatGPT to construct a phishing email, it will politely decline. At least, that's what's supposed to happen. It turns out, it's rather easy to bypass restrictions and get AI to say whatever you want.\n\nComputer scientists from Princeton University, Virginia Tech, IBM Research, and Stanford University studied large language models (LLMs) to see if safety \"guardrails\" can be removed. Apparently, all a person needs to do is fine-tune the AI model using data containing the negative behavior they want to create.\n\nAs OpenAI explains, \u201cFine-tuning [trains] on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks.\u201d It also lets the AI forget its protections and create what the user wants. Researchers were able to bypass the protections for a mere $0.20 using OpenAI's APIs.\n\n\"We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users,\" they wrote in a research paper.\n\n(Source: LMM Finetuning Risks)\n\nResearchers got this to work on OpenAI's ChatGPT as well as Meta's Llama. In most cases, it took as few as 10 harmful instruction examples to generate the exact type of content they wanted. The team specifically used examples that violated ChatGPT's terms of service.\n\n(Source: LLM-Attacks.org)\n\nThe research, conducted by Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson, mirrored the findings of another paper published in July by Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson. That paper showed that you could bypass protections by adding adversarial suffixes to requests. This method doesn't require jailbreaking, just an extra string at the end of the request.\n\nThe potential danger is fairly evident. Bad actors can use these methods to create harmful content and spread it wherever they want. In fact, according to the research paper, it's possible to bypass those protections accidentally.\n\n\"Even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning,\" the researchers say. \"These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing.\"\n\nIn an interview with The Register, Koltier said: \"One thing that is very clear is that it does seem to point to the need for more mitigation techniques, and more research on what mitigation techniques may actually work in practice.\"",
        "top_image_relative_path": "press_images/ai_safeguards_are_pretty_easy_to_bypass.jpg",
        "date": ""
    },
    {
        "title": "OpenAI has little legal recourse against DeepSeek, tech law experts say",
        "publication_source_url": "https://www.businessinsider.com",
        "original_url": "https://www.businessinsider.com/openai-little-legal-recourse-against-deepseek-tech-law-experts-2025-1",
        "text": "OpenAI and the White House have accused DeepSeek of using ChatGPT to cheaply train its new chatbot.\n\nExperts in tech law say OpenAI has little recourse under intellectual property and contract law.\n\nOpenAI's terms of use may apply but are largely unenforceable, they say.\n\nThis week, OpenAI and the White House accused DeepSeek of something akin to theft.\n\nIn a flurry of press statements, they said the Chinese upstart had bombarded OpenAI's chatbots with queries and hoovered up the resulting data trove to quickly and cheaply train a model that's now almost as good.\n\nThe Trump administration's top AI czar said this training process, called \"distilling,\" amounted to intellectual property theft. OpenAI, meanwhile, told Business Insider and other outlets that it's investigating whether \"DeepSeek may have inappropriately distilled our models.\"\n\nOpenAI is not saying whether the company plans to pursue legal action, instead promising what a spokesperson termed \"aggressive, proactive countermeasures to protect our technology.\"\n\nBut could it? Could it sue DeepSeek on \"you stole our content\" grounds, much like the grounds OpenAI was itself sued on in an ongoing copyright claim filed in 2023 by The New York Times and other news outlets?\n\nBI posed this question to experts in technology law, who said challenging DeepSeek in the courts would be an uphill battle for OpenAI now that the content-appropriation shoe is on the other foot.\n\nOpenAI would have a hard time proving an intellectual property or copyright claim, these lawyers said.\n\n\"The question is whether ChatGPT outputs\" \u2014 meaning the answers it generates in response to queries \u2014 \"are copyrightable at all,\" Mason Kortz of Harvard Law School said.\n\nThat's because it's unclear whether the answers ChatGPT spits out qualify as \"creativity,\" he said.\n\n\"There's a doctrine that says creative expression is copyrightable, but facts and ideas are not,\" Kortz, who teaches at Harvard's Cyberlaw Clinic, said.\n\n\"There's a huge question in intellectual property law right now about whether the outputs of a generative AI can ever constitute creative expression or if they are necessarily unprotected facts,\" he added.\n\nCould OpenAI roll those dice anyway and claim that its outputs are protected?\n\nThat's unlikely, the lawyers said.\n\nOpenAI is already on the record in The New York Times' copyright case arguing that training AI is an allowable \"fair use\" exception to copyright protection.\n\nIf they do a 180 and tell DeepSeek that training is not a fair use, \"that might come back to kind of bite them,\" Kortz said. \"DeepSeek could say, 'Hey, weren't you just saying that training is fair use?'\"\n\nThere may be a distinction between the Times and DeepSeek cases, Kortz added.\n\n\"Maybe it's more transformative to turn news articles into a model\" \u2014 as the Times accuses OpenAI of doing \u2014 \"than it is to turn outputs of a model into another model,\" as DeepSeek is said to have done, Kortz said.\n\n\"But this still puts OpenAI in a pretty tricky situation with regard to the line it's been toeing regarding fair use,\" he added.\n\nA breach-of-contract lawsuit is more likely\n\nA breach-of-contract lawsuit is much likelier than an IP-based lawsuit, though it comes with its own set of problems, said Anupam Chander, who teaches technology law at Georgetown University.\n\nRelated stories\n\nThe terms of service for Big Tech chatbots like those developed by OpenAI and Anthropic forbid using their content as training fodder for a competing AI model.\n\n\"So perhaps that's the lawsuit you might possibly bring \u2014 a contract-based claim, not an IP-based claim,\" Chander said.\n\n\"Not, 'You copied something from me,' but that you benefited from my model to do something that you were not allowed to do under our contract.\"\n\nThere may be a hitch, Chander and Kortz said. OpenAI's terms of service require that most claims be resolved through arbitration, not lawsuits. There's an exception for lawsuits \"to stop unauthorized use or abuse of the Services or intellectual property infringement or misappropriation.\"\n\nThere's a larger hitch, though, experts said.\n\n\"You should know that the brilliant scholar Mark Lemley and a coauthor argue that AI terms of use are likely unenforceable,\" Chander said. He was referring to a January 10 paper, \"The Mirage of Artificial Intelligence Terms of Use Restrictions,\" by Stanford Law's Mark A. Lemley and Peter Henderson of Princeton University's Center for Information Technology Policy.\n\nTo date, \"no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief,\" the paper says.\n\n\"This is likely for good reason: we think that the legal enforceability of these licenses is questionable,\" it adds. That's in part because model outputs \"are largely not copyrightable\" and because laws like the Digital Millennium Copyright Act and the Computer Fraud and Abuse Act \"offer limited recourse,\" it says.\n\n\"I think they are likely unenforceable,\" Lemley told BI of OpenAI's terms of service, \"because DeepSeek didn't take anything copyrighted by OpenAI and because courts generally won't enforce agreements not to compete in the absence of an IP right that would prevent that competition.\"\n\nLawsuits between parties in different nations, each with its own legal and enforcement systems, are always tricky, Kortz said.\n\nEven if OpenAI cleared all the above hurdles and won a judgment from a US court or arbitrator, \"in order to get DeepSeek to turn over money or stop doing what it's doing, the enforcement would come down to the Chinese legal system,\" he said.\n\nHere, OpenAI would be at the mercy of another extremely complicated area of law \u2014 the enforcement of foreign judgments and the balancing of individual and corporate rights and national sovereignty \u2014 that stretches back to before the founding of the US.\n\n\"So this is, a long, complicated, fraught process,\" Kortz added.\n\nCould OpenAI have protected itself better from a distilling incursion?\n\n\"They could have used technical measures to block repeated access to their site,\" Lemley said. \"But doing so would also interfere with normal customers.\"\n\n\n\nHe added: \"I don't think they could, or should, have a valid legal claim against the searching of uncopyrightable information from a public site.\"\n\nRepresentatives for DeepSeek did not immediately respond to a request for comment.\n\n\"We know that groups in the PRC are actively working to use methods, including what's known as distillation, to try to replicate advanced U.S. AI models,\" Rhianna Donaldson, an OpenAI spokesperson, told BI in an emailed statement.\n\n\"We are aware of and reviewing indications that DeepSeek may have inappropriately distilled our models, and will share information as we know more,\" the statement said. \"We take aggressive, proactive countermeasures to protect our technology and will continue working closely with the US government to protect the most capable models being built here.\"",
        "top_image_relative_path": "press_images/openai_has_little_legal_recourse_against_deepseek,_tech_law_experts_say.jpg",
        "date": ""
    },
    {
        "title": "DeepSeek vs. ChatGPT fuels debate over AI building blocks",
        "publication_source_url": "https://www.voanews.com",
        "original_url": "https://www.voanews.com/a/deepseek-vs-chatgpt-fuels-debate-over-ai-building-blocks/7958031.html",
        "text": "When Chinese startup DeepSeek released its AI model this month, it was hailed as a breakthrough, a sign that China\u2019s artificial intelligence companies could compete with their Silicon Valley counterparts using fewer resources.\n\nThe narrative was clear: DeepSeek had done more with less, finding clever workarounds to U.S. chip restrictions. However, that storyline has begun to shift.\n\nOpenAI, the U.S.-based company behind ChatGPT, now claims DeepSeek may have improperly used its proprietary data to train its model, raising questions about whether DeepSeek\u2019s success was truly an engineering marvel.\n\nIn statements to several media outlets this week, OpenAI said it is reviewing indications that DeepSeek may have trained its AI by mimicking responses from OpenAI\u2019s models.\n\nThe process, known as distillation, is common among AI developers but is prohibited by OpenAI\u2019s terms of service, which forbid using its model outputs to train competing systems.\n\nSome U.S. officials appear to support OpenAI\u2019s concerns. At his confirmation hearing this week, Commerce secretary nominee Howard Lutnick accused DeepSeek of misusing U.S. technology to create a \"dirt cheap\" AI model.\n\n\"They stole things. They broke in. They\u2019ve taken our IP,\" Lutnick said of China.\n\nDavid Sacks, the White House czar for AI and cryptocurrency, was more measured, saying only that it is \"possible\" that DeepSeek had stolen U.S. intellectual property.\n\nIn an interview with the cable news network Fox News, Sacks added that there is \"substantial evidence\" that DeepSeek \"distilled the knowledge out of OpenAI\u2019s models,\" adding that stronger efforts are needed to curb the rise of \"copycat\" AI systems.\n\nAt the center of the dispute is a key question about AI\u2019s future: how much control should companies have over their own AI models, when those programs were themselves built using data taken from others?\n\nAI data fight\n\nThe question is especially relevant for OpenAI, which faces its own legal challenges. The company has been sued by several media companies and authors who accuse it of illegally using copyrighted material to train its AI models.\n\nJustin Hughes, a Loyola Law School professor specializing in intellectual property, AI, and data rights, said OpenAI\u2019s accusations against DeepSeek are \"deeply ironic,\" given the company\u2019s own legal troubles.\n\n\"OpenAI has had no problem taking everyone else\u2019s content and claiming it\u2019s 'fair,'\" Hughes told VOA in an email.\n\n\"If the reports are accurate that OpenAI violated other platforms\u2019 terms of service to get the training data it has wanted, that would just add an extra layer of irony \u2013 dare we say hypocrisy \u2013 to OpenAI complaining about DeepSeek.\"\n\nDeepSeek has not responded to OpenAI\u2019s accusations. In a technical paper released with its new chatbot, DeepSeek acknowledged that some of its models were trained alongside other open-source models \u2013 such as Qwen, developed by China\u2019s Alibaba, and Llama, released by Meta \u2013 according to Johnny Zou, a Hong Kong-based AI investment specialist.\n\nHowever, OpenAI appears to be alleging that DeepSeek improperly used its closed-source models \u2013 which cannot be freely accessed or used to train other AI systems.\n\n\"It\u2019s quite a serious statement,\" said Zou, who noted that OpenAI has not yet presented evidence of wrongdoing by DeepSeek.\n\nProving improper distillation may be difficult without disclosing details on how its own models were trained, Zou added.\n\nEven if OpenAI presents concrete proof, its legal options may be limited. Although Zou noted that the company could pursue a case against DeepSeek for violating its terms of service, not all experts believe such a claim would hold up in court.\n\n\"Even assuming DeepSeek trained on OpenAI's data, I don't think OpenAI has much of a case,\" said Mark Lemley, a professor at Stanford Law School who specializes in intellectual property and technology.\n\nEven though AI models often have restrictive terms of service, \"no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief,\" Lemley wrote in a recent paper with co-author Peter Henderson.\n\nThe paper argues that these restrictions may be unenforceable, since the materials they aim to protect are \"largely not copyrightable.\"\n\n\"There are compelling reasons for many of these provisions to be unenforceable: they chill good faith research, constrain competition, and create quasi-copyright ownership where none should exist,\" the paper noted.\n\nOpenAI\u2019s main legal argument would likely be breach of contract, said Hughes. Even if that were the case, though, he added, \"good luck enforcing that against a Chinese company without meaningful assets in the United States.\"\n\nPossible options\n\nThe financial stakes are adding urgency to the debate. U.S. tech stocks dipped Monday after following news of DeepSeek\u2019s advances, though they later regained some ground.\n\nCommerce nominee Lutnick suggested that further government action, including tariffs, could be used to deter China from copying advanced AI models.\n\nBut speaking the same day, U.S. President Donald Trump appeared to take a different view, surprising some industry insiders with an optimistic take on DeepSeek\u2019s breakthrough.\n\nThe Chinese company\u2019s low-cost model, Trump said, was \"very much a positive development\" for AI, because \"instead of spending billions and billions, you\u2019ll spend less, and you\u2019ll come up with hopefully the same solution.\"\n\nIf DeepSeek has succeeded in building a relatively cheap and competitive AI model, that may be bad for those with investment \u2013 or stock options \u2013 in current generative AI companies, Hughes said.\n\n\"But it might be good for the rest of us,\" he added, noting that until recently it appeared that only the existing tech giants \"had the resources to play in the generative AI sandbox.\"\n\n\"If DeepSeek disproved that, we should hope that what can be done by a team of engineers in China can be done by a similarly resourced team of engineers in Detroit or Denver or Boston,\" he said.",
        "top_image_relative_path": "press_images/deepseek_vs._chatgpt_fuels_debate_over_ai_building_blocks.jpg",
        "date": "2025-01-31T05:52:21-05:00"
    },
    {
        "title": "Researchers Are Developing Tools to Calculate AIs Carbon Footprint",
        "publication_source_url": "https://www.wsj.com",
        "original_url": "https://www.wsj.com/articles/researchers-are-developing-tools-to-calculate-ais-carbon-footprint-11594978202?mod=djemAIPr",
        "text": "The researchers goal is to encourage the energy-efficient development of AI models. Developing artificial intelligence can use a lot of energy. Research in Canada and the U.S. are developing tools to calculate the carbon...",
        "top_image_relative_path": "press_images/677b4131-1c8d-4067-8bbb-b5957af38c69.png",
        "date": "2020-07-17T05:30:00-04:00",
        "author": "Sara Castellanos"
    },
    {
        "title": "Generative AI Meets Copyright",
        "publication_source_url": "https://ip.jotwell.com",
        "original_url": "https://ip.jotwell.com/generative-ai-meets-copyright/",
        "text": "Generative AI Meets Copyright\n\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley & Percy Liang, Foundation Models and Fair Use, available at SSRN (Mar. 27, 2023).\n\nChatGPT, Midjourney, and Copilot are among the numerous generative AI systems launched in the last year or so. They have attracted a huge number of users as well as several lawsuits. Among the lawsuits\u2019 claims are that the makers of these systems are direct and indirect infringers of copyright because of their use of millions of in-copyright works as training data and because outputs of these generative AI programs are infringing derivative works.\n\nAt the core of these AI systems are foundation models on which the authors focus in their fascinating new article. They define this term as \u201clarge pre-trained machine learning models that are used as a starting point for various computational tasks,\u201d including generative AI systems that may produce text, images, and/or software code in response to user prompts. The article identifies various actors who contribute to elements of these AI systems, including data creators, data curators, model creators, model deployers, and model users.\n\nThose of us who are intent on understanding the legal implications of generative AI systems must, of necessity, be prepared to learn about the technology underlying these systems. Fortunately, these six Stanford researchers\u2014some in computer science and some in law (our own redoubtable Mark Lemley among them)\u2014have provided an essential guide for intellectual property and technology law scholars to the development and deployment of these systems. The article explores the extent to which developers and deployers of generative AI systems may rely on fair use to justify their use of in-copyright works as training data and how developers may limit their potential liability for infringements at the output stage.\n\nFor many copyright scholars, the article\u2019s discussion of the fair use cases will be familiar, but the application of these precedents in the context of generative AI will be particularly useful. Yes, of course, the Authors Guild v. Google and iParadigms decisions suggest that computational uses of in-copyright materials can be fair use, but other decisions such as Associated Press v. Meltwater and Fox v. TVEyes suggest that much will depend on the particular uses that generative AI systems make of the in-copyright materials.\n\nFoundation Models is not an advocacy article asserting that all uses of in-copyright works (or at least all that can be found on the open internet) as training data is fair use. Nor does the article argue that all outputs should be non-infringing so long as the outputs are not verbatim copies of the contents of specific training data. It offers a much more nuanced perspective about the challenges for system developers in understanding how to model computationally the degree of \u201ctransformativeness\u201d that may be achieved by a second comer\u2019s use of copyrighted works, as well as how to distinguish facts and expressions within those works.\n\nThe most novel section of Foundation Models is its discussion of technical strategies that AI system developers can employ to reduce the risk of copyright infringement when generative AI produces outputs in response to user prompts. These include data and output filters to detect similarities between the input data and outputs generated by the systems. Some technical mitigation strategies the authors describe must be done at the training data stage, while others, including data and output filters, can be done at the deployment stage.\n\nThe article discusses the Field v. Google decision for its recognition that Field had not used the \u201crobots.txt\u201d exclusion standard as a technique to stop Google from webcrawling his site. This consideration weighed against Field\u2019s copyright claim that the search engine infringed by copying his content on that site. Foundation Models suggests that generative AI system developers and deployers would be well-advised to adopt one or more technical mitigation strategies to bolster their fair use claims.\n\nWhile this article is well worth reading on the merits, it is also a noteworthy contribution to an emerging literature in which computer scientists and lawyers collaborate to explore technology law and policy issues. While not written in perhaps the most scintillating prose, this article is an outstanding example of a successful collaboration to explore ways in which technologists and lawyers can work together to co-evolve practical ways to achieve socially desirable outcomes.",
        "top_image_relative_path": "press_images/generative_ai_meets_copyright.jpg",
        "date": "2023-07-11T10:30:03+00:00"
    },
    {
        "title": "Artificial intelligence faces reproducibility crisis",
        "publication_source_url": "https://www.science.org",
        "original_url": "https://www.science.org/doi/10.1126/science.359.6377.725",
        "text": "Unpublished code and sensitivity to training conditions make many claims hard to verify.\n\nLast year, computer scientists at the University of Montreal (U of M) in Canada were eager to show off a new speech recognition algorithm, and they wanted to compare it to a benchmark, an algorithm from a well-known scientist. The only problem: The benchmark's source code wasn't published. The researchers had to recreate it from the published description. But they couldn't get their version to match the benchmark's claimed performance, says Nan Rosemary Ke, a Ph.D. student in the U of M lab. We tried for 2 months and we couldn't get anywhere close.\n\nThe booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. AI researchers have found it difficult to reproduce many key results, and that is leading to a new conscientiousness about research methods and publication protocols. I think people outside the field might assume that because we have code, reproducibility is kind of guaranteed, says Nicolas Rougier, a computational neuroscientist at France's National Institute for Research in Computer Science and Automation in Bordeaux. Far from it. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence (AAAI) in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problemand one laying out tools to mitigate it.",
        "top_image_relative_path": "press_images/359_725_f1.jpeg",
        "date": "2018-02-16T00:00:00+00:00",
        "author": "Matthew Hutson",
        "source": "Science"
    },
    {
        "title": "Federal Use of AI Tools Prompts Researchers to Build New Dataset",
        "publication_source_url": "https://news.bloomberglaw.com",
        "original_url": "https://news.bloomberglaw.com/ip-law/federal-use-of-ai-tools-prompts-researchers-to-build-new-dataset",
        "text": "Federal contractors building artificial intelligence models will have an expansive new dataset to work from following Stanford University researchers\u2019 project aimed at unique government needs.\n\nThe government\u2019s AI toolbox has been expanding in recent years as agencies experiment with and build their own algorithm-driven tools to assist with tasks in complex, high-volume arenas.\n\nAI tools generally have relied on datasets made up of information scraped from social media companies and sites like Wikipedia.\n\nStanford\u2019s model instead focuses on documents suited to legal and governmental contexts, like patent claims or financial documents. It\u2019s called \u201cPile of Law\u201d and was created by ...",
        "top_image_relative_path": "press_images/federal_use_of_ai_tools_prompts_researchers_to_build_new_dataset.jpg",
        "date": "2022-10-03T09:42:34+00:00"
    },
    {
        "title": "The Datasets We\u2019re Looking At This Week",
        "publication_source_url": "https://fivethirtyeight.com",
        "original_url": "https://fivethirtyeight.com/features/the-datasets-were-looking-at-this-week-9/",
        "text": "Data Is Plural The Datasets We\u2019re Looking At This Week\n\nYou\u2019re reading Data Is Plural, a weekly newsletter of useful/curious datasets. Below you\u2019ll find the July 13, 2022, edition, reprinted with permission at FiveThirtyEight.\n\n2022.07.13 edition\n\nHeat metrics, technology adoption, a pile of law, digital payments in India and early movie theaters in Oregon.\n\nHeat metrics. When you talk about outdoor heat, you\u2019re likely referring to dry-bulb temperatures, measured by a thermometer shielded from the sun and moisture. But other factors also contribute to the physiological experience of hot weather. To that end, Keith R. Spangler et al. have created a dataset containing daily estimates of the wet-bulb globe temperature, Universal Thermal Climate Index, heat index, humidex and other heat metrics for every county in the contiguous United States from 2000 through 2020. That first metric, for instance, was \u201coriginally developed in the 1950s to establish epidemiologically relevant thermal thresholds to prevent heat-related illnesses at US military training camps,\u201d and takes humidity, solar radiation and wind speed into account.\n\nTechnology adoption. Diego Comin and Bart Hobijn\u2019s Cross-Country Historical Adoption of Technology dataset, published in 2009, compiles statistics \u201con the adoption of over 100 technologies in more than 150 countries since 1800.\u201d Examples include the number of telegrams sent, television sets in use, knee replacement surgeries performed and metric tons of freight carried on railways. Last month, Charles Kenny and George Yang published a dataset and accompanying working paper that updates those numbers and expands the technologies covered. [h/t Ranil Dissanayake]\n\nLegal language. Peter Henderson et al.\u2019s Pile of Law is \u201ca 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules and legislative records.\u201d Its texts come from CourtListener (DIP 2016.04.13), the Constitute Project (DIP 2022.04.13), the European Court of Human Rights\u2019 published opinions, the Consumer Financial Protection Bureau\u2019s collection of credit card agreements and many other sources. [h/t Lynn Cherny]\n\nDigital payments in India. PhonePe, a digital-payments company serving India, publishes quarterly aggregated data on users and transactions. The statistics, which go back to 2018 and power an interactive map, are available on a national, state and district level. User counts are provided as totals as well as grouped by device brand. Transactions are measured by count and total value, with the counts also disaggregated into a few categories.\n\nEarly movie theaters in Oregon. The Oregon Theater Project, developed as part of a cinema studies course, \u201caims to document the history of moviegoing in Oregon \u2014 why people went to the movies, where people watched them and what people thought about them,\u201d with a current focus on the silent-film era. Its directory of 200-plus theaters is available to download and explore online.\n\nDataset suggestions? Criticism? Praise? Send feedback to jsvine@gmail.com. Looking for past datasets? This spreadsheet contains them all. Visit data-is-plural.com to subscribe and to browse past editions.",
        "top_image_relative_path": "press_images/the_datasets_we\u2019re_looking_at_this_week.jpg",
        "date": "2022-07-13T16:00:00+00:00"
    },
    {
        "title": "Machine Learning May Improve Audit Efficiency, Study Finds",
        "publication_source_url": "https://tax.thomsonreuters.com",
        "original_url": "https://tax.thomsonreuters.com/news/machine-learning-may-improve-audit-efficiency-study-finds/",
        "text": "Adaptive algorithmic technology could help optimize audit sampling to better identify tax underreporting and close revenue gaps, according to a recent study by academics and government officials.\n\nThe use of sequential decision-making, or machine learning, integrated with the National Research Program model used in randomized IRS audits is \u201cfertile ground\u201d for exploration, a team of Stanford University and IRS researchers wrote in a paper published in April.\n\nSo-called multi-armed bandit algorithms are geared toward a specified \u201creward\u201d\u2014in this case, identification of underreporting that would close the tax gap, or the difference between what taxpayers owe and what\u2019s actually collected. Such algorithms aren\u2019t prevalent in the public sector, but the paper\u2019s authors said machine learning could improve the efficiency and quality of government services if carefully deployed.\n\nPeter Henderson of Stanford University, the paper\u2019s primary author, presented the research at a June 16 webinar co-hosted by the IRS and the Urban-Brookings Tax Policy Center. In a panel discussion, he said these technologies have been used by private companies. Netflix, for example, uses an algorithm that analyzes user engagement data to better predict what people are more likely to watch next.\n\nModern IRS audit procedures have an \u201coptimization problem,\u201d Henderson said. The agency can struggle to pin down where misreporting occurs while also \u201cactively finding the largest amounts of misreporting.\u201d The paper used returns from tax years 2006 through 2014 to conclude that a \u201cunified optimize-and-estimate\u201d program under a bandit-like framework could efficiently maximize revenue while maintaining accurate estimates of the tax gap.\n\nUnlike other machine learning research, this study maintains that algorithms that can adapt to data should still be paired with the NRP random model. Former National Taxpayer Advocate Nina Olson submitted a question at the panel to Henderson asking why randomized audits need to be relied upon if machine learning can supposedly better select returns to audit that are more likely to reveal underreporting.\n\nIn response, Henderson said randomness alleviates a risk of bias that machine learning algorithms can only reduce but not fully eliminate. Without random sampling, a model can get stuck in \u201csuboptimal feedback loops\u201d in which it oversamples a particular area, possibly leading to significant problems, he explained.\n\nAlan Plumley of the IRS Research, Applied Analytics, and Statistics Division said there are two competing interests at work in the National Research Program model.\n\nOn one hand, the its sample size has declined 43% over recent years as the IRS conducted fewer total audits due to budget and resource constraints. This undermines the quality of estimates based on the model, Plumley said, suggesting that the remedy would be more-random audits.\n\nConversely, random audits have become a larger percentage of overall audits the IRS has conducted in recent years. This is detrimental to the goals of nonrandom operational audits aimed at recapturing revenue, and implies that the model\u2019s sample size should be decreased, Plumley said.\n\nThe Stanford/IRS research has the potential to lead to a better balancing of these competing revenue and measurement objectives, according to Plumley. However, his analysis of the research shows that unbiased compliance estimates at the line-item level are more important than those of the gross underreporting tax gap. In addition, the paper doesn\u2019t address the cost of implementing machine learning into audit procedures.\n\n\u201cTotal reward is budget-constrained, unlike one-armed bandits,\u201d Plumley said. He also argued that audits widely vary in cost and that audit selection should be based on a revenue/cost ratio. It may be cheaper to increase the number of random audits than it would to \u201cfix the operational lineup and data,\u201d he added.\n\nHenderson countered that while it\u2019s \u201cdefinitely true\u201d that cost is an important factor, a budget-first mindset can have consequences. For example, the IRS might stop auditing parts of the income base because of cost, despite misreporting, he suggested.\n\nThe IRS must \u201cbe careful\u201d and consider the full picture, Henderson said.\n\nGet all the latest tax, accounting, audit, and corporate finance news with Checkpoint Edge. Sign up for a free 7-day trial today.",
        "top_image_relative_path": "press_images/machine_learning_may_improve_audit_efficiency,_study_finds.jpg",
        "date": "2022-06-21T14:01:05+00:00"
    },
    {
        "title": "Regulation needed for AI, technology environmental impact",
        "publication_source_url": "https://www.techtarget.com",
        "original_url": "https://www.techtarget.com/searchcio/news/252523241/Regulation-needed-for-AI-technology-environmental-impact",
        "text": "Concerns about the environmental impacts of advanced technologies such as AI is prompting debate over whether computing-intensive applications and the chips that power them need to be regulated.\n\nThat's according to experts speaking during the \"Advancing Technology for a Sustainable Planet\" conference hosted by the Stanford Institute for Human-Centered Artificial Intelligence and the Stanford Woods Institute for the Environment.\n\nTechnologies including artificial intelligence and cloud computing use energy that result in carbon emissions. At the same time, many of these technologies also help companies meet sustainability goals -- meaning companies need balance between quickly adopting and scaling emerging technologies and understanding how those technologies could affect the company's overall environmental impact.\n\nEnvironmental impact depends on scale -- particularly for a technology like artificial intelligence, said Peter Henderson, a Ph.D student in computer science at Stanford University, during a conference panel session. Companies often optimize AI algorithms to address energy use and carbon emission concerns before deploying a machine learning model.\n\nThe point is to make sure we don't scale to a point that is harmful to the environment, when the goal of a lot of machine learning work is AI for social good. Peter Henderson Ph.D student, Stanford University\n\n\"The point is to make sure we don't scale to a point that is harmful to the environment, when the goal of a lot of machine learning work is AI for social good, where we want to build more sustainable things, we want to optimize batteries, energy grids,\" he said. \"But if all that optimization leads to more negative impact than positive, it's not worth undertaking.\"\n\nHenderson said beyond company measures to optimize such technologies, government efforts to provide rules for AI usage will likely also be necessary. While some efforts to regulate AI are already underway in the European Union, they don't fully address the environmental impacts of the technology.\n\nTargeting AI's environmental impact The EU's AI regulations aim for consumer rather than environmental protection, according to Henderson. He added that a significant piece of AI's environmental impact comes from GPUs powering the AI and machine learning models. Regulation would have to cover chips and other technologies underlying AI use to address environmental impact concerns, he said. \"Here in California, there was recent regulation [that says] some GPUs are not allowed to be sold here anymore because they're not efficient enough,\" Henderson said. \"That could be a path forward to pushing innovation and forcing more efficient chipsets.\" Though incentives -- such as lower cost -- exist for companies to use more efficient chips, Henderson said it is still an area that needs to be considered from a regulation perspective. A lack of data on the environmental impact of technologies such as AI, cloud computing and bitcoin makes effective regulation difficult, he added. \"Step one is making sure we have enough reporting, enough data to make good regulatory and policy decisions,\" he said.",
        "top_image_relative_path": "press_images/regulation_needed_for_ai,_technology_environmental_impact.jpg",
        "date": ""
    },
    {
        "title": "Discovery in State Courts",
        "publication_source_url": "https://courtslaw.jotwell.com",
        "original_url": "https://courtslaw.jotwell.com/discovery-in-state-courts/",
        "text": "Discovery in State Courts\n\n122 Colum. L. Rev. __ (forthcoming 2022), Jan. 5, 2022 draft available at Diego A. Zambrano, Missing Discovery in Lawyerless Courts,__ (forthcoming 2022), Jan. 5, 2022 draft available at SSRN\n\nBecause 98% of civil litigation occurs in state court, we procedure scholars can be criticized for our myopic focus on procedures in federal courts. An emerging body of scholarship is adjusting that perspective by telling us much more about what is occurring in state courts \u2014 and it is eye-opening. To pick a few examples, Pamela Bookman and Colleen Shanahan (reviewed here) have shown how, in contrast to \u201clawyered\u201d federal courts, state courts are often \u201clawyerless courts\u201d where at least one party is unrepresented. Daniel Wilf\u2013Townsend samples state-court dockets and uncovers, among other nuggets, that ten corporations account for an astounding 24% of the 16 million annual state-court filings. If we are concerned about access to justice, we should heed the advice of Anna Carpenter, Jessica Steinberg, Colleen Shanahan, and Alyx Mark to do more empirical research and more theoretical analysis about the roles of lawyers and judges in the fluid and evolving world of state-court litigation.\n\nDiego Zambrano takes on one piece of this task by exploring how discovery operates in state courts. Zambrano has become a distinctive voice on discovery. He has proposed seeing it as a regulatory tool (in contrast to the usual view that it is meant to aid in the accurate and fair adjudication or settlement of individual cases). His co-authored piece on the myriad ways in which parties can poison the well of technology-assisted review is required reading for anyone who hopes that artificial intelligence will harness and solve the problems of discovery in document-intensive litigation. In Missing Discovery, Zambrano examines how discovery plays out in some of the largest categories of state-court litigation, in which individual litigants are likely unrepresented: small-claims and debt-collection cases, landlord-tenant disputes, family matters, and appeals from agency decisions in areas such as workers\u2019 comp or unemployment insurance.\n\nThe article\u2019s first, descriptive contribution maps states\u2019 widely disparate approaches to permitting discovery in these cases. At the federal level, of course, discovery is in theory available in every case (although empirical studies show that, in practice, a substantial minority of cases involve little to no discovery). Federal discovery is uniform and trans-substantive: neither the substance of the dispute nor the amount in controversy disqualifies a case from access to the capacious tools of federal discovery. Not so at the state level. Zambrano focuses on seven states (California, Florida, Massachusetts, Michigan, New York, Pennsylvania, and Texas) whose formal discovery processes diverge from the federal model, documenting how the discovery procedures in the three categories of cases vary state to state and even within a state, depending on the nature of the case. For instance, most state courts allow no discovery in small-claims cases, while a couple do, and others allow at least a judicially led inquiry into the facts. Likewise, three states deny discovery in eviction proceedings but four allow them. When discovery is allowed, the available methods vary from limited subpoenas to the full panoply of discovery tools.\n\nAs Zambrano shows, some impulses behind restricting discovery are understandable. Unrepresented parties are probably unable to afford discovery, so a better-financed party can wield that discovery as a cudgel. Discovery is also complex, and unrepresented parties are not likely to be successful in navigating those complexities. And discovery adds a measure of delay. Balanced against these justifications for \u201cmissing discovery\u201d are the costs of denying parties access to information\u2014decreased accuracy, a diminished sense of participation and fairness, and a greater incentive for repeat players to engage in unlawful behavior on which discovery would have shone a bright light.\n\nZambrano\u2019s second, anormative contribution analyzes whether the lack of discovery in mostly lawyerless courts is suboptimal. He concludes that it is. Seeking a \u201csweet spot\u201d between no discovery and full discovery, he develops three design principles \u2014 imposing discovery only on sophisticated parties, requiring discovery only from parties who possess \u201cactually relevant\u201d information, and expanding discovery to its full scope only when the alleged wrongdoing has broader significance than the dispute between the parties. He cashes out those principles into a concrete \u201copen file\u201d proposal that operates somewhat like a prosecutor\u2019s Brady disclosures on the criminal side\u2014sophisticated parties such as landlords or debt-collection companies must deposit all relevant documents into a publicly available file.\n\nZambrano admits that \u201cunlawyered\u201d parties might be unable to make good use of this evidence, as they lack the legal knowledge and sophistication to appreciate the significance of much of the disclosed information. He hopes that public disclosure of this information will have a positive regulatory effect on repeat players. Debt collectors and landlords might reform their dubious practices rather than distribute information about those practices into the public arena, where the little guys fortunate enough to have legal representation might could use damaging disclosures. It is in \u201cthe shadows of litigation,\u201d as Zambrano puts it, where an open-file approach might see its greatest effect. This open-file approach links back to Zambrano\u2019s broader view of discovery as a regulatory measure.\n\nHaving proposed both direct and incentive-based means to thread the needle between excessive and insufficient discovery, I find a great deal to admire in Zambrano\u2019s design principles and his open-file proposal, as well as his idea of using state courts as laboratories to build a better discovery mousetrap than the one in the Federal Rules. Nonetheless, given that Zambrano predicts modest increases in accuracy of dispute resolution, the case for an open-file system comes down to the uncertain strength of the expected regulatory effect. And that uncertainty wraps into the broader question of whether \u201cdiscovery as regulation\u201d is the right way to think about discovery in general and one-way mandatory public disclosures in particular.\n\nPerhaps the principal take-aways in the emerging literature on state-court procedure are the prevalence of unrepresented parties and the willingness of sophisticated repeat players to design litigation strategies to take advantage of a lawyerless opponent. Frederick Wilmot\u2013Smith\u2019s important Equal Justice argues convincingly that a true commitment to equal justice requires a radical reinvention how societies allocate legal services among presently lawyered and presently unlawyered parties. Requiring repeat players in state courts to disclose information on a mandatory and public basis renders unjust systems less unjust. But that is not the same as making those systems just.",
        "top_image_relative_path": "press_images/discovery_in_state_courts.jpg",
        "date": "2022-05-27T10:30:41+00:00"
    },
    {
        "title": "AI conference widely known as \u2018NIPS\u2019 changes its controversial acronym",
        "publication_source_url": "https://www.nature.com",
        "original_url": "https://www.nature.com/articles/d41586-018-07476-w",
        "text": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript.",
        "top_image_relative_path": "press_images/ai_conference_widely_known_as_\u2018nips\u2019_changes_its_controversial_acronym.jpg",
        "date": ""
    },
    {
        "title": "We need to improve the accuracy of AI accuracy discussions",
        "publication_source_url": "https://techcrunch.com",
        "original_url": "https://techcrunch.com/2018/03/11/accuracy-of-accuracy/",
        "text": "Reading the tech press, you would be forgiven for believing that AI is going to eat pretty much every industry and job. Not a day goes by without another reporter breathlessly reporting some new machine learning product that is going to trounce human intelligence. That surfeit of enthusiasm doesn\u2019t originate just with journalists though \u2014 they are merely channeling the wild optimism of researchers and startup founders alike.\n\nThere has been an explosion of interest in artificial intelligence and machine learning over the past few years, as the hype around deep learning and other techniques has increased. Tens of thousands of research papers in AI are published yearly, and AngelList\u2019s startup directory for AI companies includes more than four thousands startups.\n\nAfter being battered by story after story of AI\u2019s coming domination \u2014 the singularity, if you will \u2014 it shouldn\u2019t be surprising that 58% of Americans today are worried about losing their jobs to \u201cnew technology\u201d like automation and artificial intelligence according to a newly released Northeastern University / Gallup poll. That fear outranks immigration and outsourcing by a large factor.\n\nThe truth though is much more complicated. Experts are increasingly recognizing that the \u201caccuracy\u201d of artificial intelligence is overstated. Furthermore, the accuracy numbers reported in the popular press are often misleading, and a more nuanced evaluation of the data would show that many AI applications have much more limited capabilities than we have been led to believe. Humans may indeed end up losing their jobs to AI, but there is a much longer road to go.\n\nAnother replication crisis\n\nFor the past decade or so, there has been a boiling controversy in research circles over what has been dubbed the \u201creplication crisis\u201d \u2014 the inability of researchers to duplicate the results of key papers in fields as diverse as psychology and oncology. Some studies have even put the number of failed replications at more than half of all papers.\n\nThe causes for this crisis are numerous. Researchers face a \u201cpublish or perish\u201d situation where they need positive results in order to continue their work. Journals want splashy results to get more readers, and \u201cp-hacking\u201d has allowed researchers to get better results by massaging statistics in their favor.\n\nArtificial intelligence research is not immune to such structural factors, and in fact, may even be worse given the incredible surge of excitement around AI, which has pushed researchers to find the most novel advances and share them as quickly and as widely as possible.\n\nNow, there are growing concerns that the most important results in AI research are hard if not impossible to replicate. One challenge is that many AI papers are missing the key data required to run their underlying algorithms or worse, don\u2019t even include the source code for the algorithm under study. The training data used in machine learning is a huge part of the success of an algorithm\u2019s results, so without that data, it is nearly impossible to determine whether a particular algorithm is functioning as described.\n\nWorse, in the rush to publish novel and new results, there has been less focus on replicating studies to show how repeatable different results are. From the MIT Technology Review article linked above, \u201c\u2026Peter Henderson, a computer scientist at McGill University in Montreal, showed that the performance of AIs designed to learn by trial and error is highly sensitive not only to the exact code used, but also to the random numbers generated to kick off training, and to \u2018hyperparameters\u2019\u2014settings that are not core to the algorithm but that affect how quickly it learns.\u201d Very small changes could lead to vastly different results.\n\nMuch as a single study in nutrition science should always be taken with a grain of salt (or perhaps butter now, or was it sugar?), new AI papers and services should be treated with a similar level of skepticism. A single paper or service demonstrating a singular result does not prove accuracy. Often, it means that a very choice dataset operating with very specific conditions can lead to a high point of accuracy that won\u2019t apply to a more general set of inputs.\n\nAccurately reporting accuracy\n\nThere is a palpable excitement about the potential of AI to solve problems as diverse as clinical evaluation at a hospital to document scanning to terrorism prevention. That excitement though has clouded the ability of journalists and even researchers from accurately reporting accuracy.\n\nTake this recent article about using AI to detect colorectal cancer. The article says that \u201cThe results were impressive \u2014 an accuracy of 86 percent \u2014 as the numbers were obtained by assessing patients whose colorectal polyp pathology was already diagnosed.\u201d The article also included the key results paragraph from the original study.\n\nOr take this article about Google\u2019s machine learning service to perform language translation. \u201cIn some cases, Google says its GNMT system is even approaching human-level translation accuracy. That near-parity is restricted to transitions between related languages, like from English to Spanish and French.\u201d\n\nThese are randomly chosen articles, but there are hundreds of others that breathlessly report the latest AI advances and throw out either a single accuracy number, or a metaphor such as \u201chuman-level.\u201d If only evaluating AI programs were so simple!\n\nLet\u2019s say you want to determine whether a mole on a person\u2019s skin is cancerous. This is what is known as a binary classification problem \u2014 the goal is to separate out patients into two groups: people who have cancer, and people who do not. A perfect algorithm with perfect accuracy would identify every person with cancer as having cancer, and would identify every person with no cancer as not having cancer. In other words, the results would have no false positives or false negatives.\n\nThat\u2019s simple enough, but the challenge is that conditions like cancer are essentially impossible to identify with perfect accuracy for computers and humans alike. Every medical diagnostic test usually has to make a tradeoff between how sensitive it is (how many positives does it identify correctly) versus how specific it is (how many negatives does it identify correctly). Given the danger of misidentifying a cancer patient (which could lead to death), tests are generally designed to ensure a high sensitivity by decreasing specificity (i.e. increasing false positives to ensure that as many positives are identified).\n\nProduct designers have choices here in how they want to balance those competing priorities. The same algorithm might be implemented differently depending on the the cost of false positives and negatives. If a research article or service doesn\u2019t discuss these tradeoffs, then accuracy is not being fairly represented.\n\nEven more importantly, the singular value of accuracy is a bit of a misnomer. Accuracy reflects how many positive patients were identified positively and how many negative patients were identified negatively. But we can maintain the same accuracy by increasing one number and decreasing the other number or vice versa. In other words, a test could emphasize detecting positive patients well, or it could emphasize excluding negative patients from the results, while maintaining the same accuracy. Those are very different end goals, and some algorithms may be better tuned toward one rather than the other.\n\nThat\u2019s the complication of using a single number. Metaphors are even worse. \u201cHuman-level\u201d doesn\u2019t say anything \u2014 there is rarely good data on the error rate of humans, and even when there is such data, it is often hard to compare the types of errors made by humans versus those made by machine learning.\n\nThat\u2019s just some of the complications for the simplest classification problem. All of the nuances around evaluating AI quality would take at least a book, and indeed, some researchers will no doubt spend their entire lives evaluating these systems.\n\nEveryone can\u2019t get a PhD in artificial intelligence, but the onus is on each of us as consumers of these new technologies to apply a critical eye to these sunny claims and rigorously evaluate them. Whether it is reproducibility or breathless accuracy claims, it is important to remember that many of the AI techniques we rely on are mere technological babies, and still need a lot more time to mature.",
        "top_image_relative_path": "press_images/we_need_to_improve_the_accuracy_of_ai_accuracy_discussions.jpg",
        "date": "2018-03-11T00:00:00"
    }
]