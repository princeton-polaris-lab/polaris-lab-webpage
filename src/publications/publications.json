[
    {
        "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
        "pdf": "http://arxiv.org/abs/2406.14526v1",
        "authors": [
            "Luxi He",
            "Yangsibo Huang",
            "Weijia Shi",
            "Tinghao Xie",
            "Haotian Liu",
            "Yue Wang",
            "Luke Zettlemoyer",
            "Chiyuan Zhang",
            "Danqi Chen",
            "Peter Henderson"
        ],
        "year": "2025",
        "abstract": "Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns around copyright infringement. Copyrighted characters, in particular,\npose a difficult challenge for image generation services, with at least one\nlawsuit already awarding damages based on the generation of these characters.\nYet, little research has empirically examined this issue. We conduct a\nsystematic evaluation to fill this gap. First, we build CopyCat, an evaluation\nsuite consisting of diverse copyrighted characters and a novel evaluation\npipeline. Our evaluation considers both the detection of similarity to\ncopyrighted characters and generated image's consistency with user input. Our\nevaluation systematically shows that both image and video generation models can\nstill generate characters even if characters' names are not explicitly\nmentioned in the prompt, sometimes with only two generic keywords (e.g.,\nprompting with \"videogame, plumber\" consistently generates Nintendo's Mario\ncharacter). We then introduce techniques to semi-automatically identify such\nkeywords or descriptions that trigger character generation. Using our\nevaluation suite, we study runtime mitigation strategies, including both\nexisting methods and new strategies we propose. Our findings reveal that\ncommonly employed strategies, such as prompt rewriting in the DALL-E system,\nare not sufficient as standalone guardrails. These strategies must be coupled\nwith other approaches, like negative prompting, to effectively reduce the\nunintended generation of copyrighted characters. Our work provides empirical\ngrounding to the discussion of copyright mitigation strategies and offers\nactionable insights for model deployers actively implementing them.",
        "published": "2025-01-20T17:38:16Z",
        "updated": "2025-01-20T17:38:16Z",
        "publication_venue": "International Conference on Learning Representations",
        "code": "",
        "site": "",
        "image": "fantastic.png",
        "comments": "",
        "tags": [
            "Copyright Law",
            "AI Safety & Security"
        ]
    },
    {
        "title": "Breaking Down Bias: On The Limits of Generalizable Pruning Strategies",
        "pdf": "http://arxiv.org/abs/2502.07771",
        "authors": [
            "Sibo Ma",
            "Alejandro Salinas",
            "Peter Henderson",
            "Julian Nyarko"
        ],
        "year": "2025",
        "abstract": "We employ model pruning to examine how LLMs conceptualize racial biases, and whether a generalizable mitigation strategy for such biases appears feasible. Our analysis yields several novel insights. We find that pruning can be an effective method to reduce bias without significantly increasing anomalous model behavior. Neuron-based pruning strategies generally yield better results than approaches pruning entire attention heads. However, our results also show that the effectiveness of either approach quickly deteriorates as pruning strategies become more generalized. For instance, a model that is trained on removing racial biases in the context of financial decision-making poorly generalizes to biases in commercial transactions. Overall, our analysis suggests that racial biases are only partially represented as a general concept within language models. The other part of these biases is highly context-specific, suggesting that generalizable mitigation strategies may be of limited effectiveness. Our findings have important implications for legal frameworks surrounding AI. In particular, they suggest that an effective mitigation strategy should include the allocation of legal responsibility on those that deploy models in a specific use case.",
        "published": "2025-02-20T00:00:00Z",
        "updated": "2025-02-20T00:00:00Z",
        "publication_venue": "arXiv",
        "code": "",
        "site": "",
        "image": "1422ec4c-2101-434f-a24f-3d7919d8bbb8.png",
        "comments": "",
        "tags": [
            "AI Safety & Security",
            "AI Law & Policy"
        ]
    },
    {
        "title": "An Adversarial Perspective on Machine Unlearning for AI Safety",
        "pdf": "http://arxiv.org/abs/2409.18025v3",
        "authors": [
            "Jakub \u0141ucki",
            "Boyi Wei",
            "Yangsibo Huang",
            "Peter Henderson",
            "Florian Tram\u00e8r",
            "Javier Rando"
        ],
        "year": "2024",
        "abstract": "Large language models are finetuned to refuse questions about hazardous\nknowledge, but these protections can often be bypassed. Unlearning methods aim\nat completely removing hazardous capabilities from models and make them\ninaccessible to adversaries. This work challenges the fundamental differences\nbetween unlearning and traditional safety post-training from an adversarial\nperspective. We demonstrate that existing jailbreak methods, previously\nreported as ineffective against unlearning, can be successful when applied\ncarefully. Furthermore, we develop a variety of adaptive methods that recover\nmost supposedly unlearned capabilities. For instance, we show that finetuning\non 10 unrelated examples or removing specific directions in the activation\nspace can recover most hazardous capabilities for models edited with RMU, a\nstate-of-the-art unlearning method. Our findings challenge the robustness of\ncurrent unlearning approaches and question their advantages over safety\ntraining.",
        "published": "2024-09-26T16:32:19Z",
        "updated": "2024-11-08T22:06:41Z",
        "publication_venue": "Neurips 2024 SoLaR workshop",
        "code": "",
        "site": "",
        "image": "18c51d69-9fc3-43e2-9f5f-48886c77421c.png",
        "comments": "",
        "press": [
            {
                "outlet": "Best Paper Award @ Neurips 2024 SoLaR workshop",
                "url": "https://solar-neurips.github.io/"
            }
        ],
        "tags": [
            "AI Safety & Security"
        ]
    },
    {
        "title": "Evaluating Copyright Takedown Methods for Language Models",
        "pdf": "http://arxiv.org/abs/2406.18664v4",
        "authors": [
            "Boyi Wei",
            "Weijia Shi",
            "Yangsibo Huang",
            "Noah A. Smith",
            "Chiyuan Zhang",
            "Luke Zettlemoyer",
            "Kai Li",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "Language models (LMs) derive their capabilities from extensive training on\ndiverse data, including potentially copyrighted material. These models can\nmemorize and generate content similar to their training data, posing potential\nconcerns. Therefore, model creators are motivated to develop mitigation methods\nthat prevent generating protected content. We term this procedure as copyright\ntakedowns for LMs, noting the conceptual similarity to (but legal distinction\nfrom) the DMCA takedown This paper introduces the first evaluation of the\nfeasibility and side effects of copyright takedowns for LMs. We propose\nCoTaEval, an evaluation framework to assess the effectiveness of copyright\ntakedown methods, the impact on the model's ability to retain uncopyrightable\nfactual knowledge from the training data whose recitation is embargoed, and how\nwell the model maintains its general utility and efficiency. We examine several\nstrategies, including adding system prompts, decoding-time filtering\ninterventions, and unlearning approaches. Our findings indicate that no tested\nmethod excels across all metrics, showing significant room for research in this\nunique problem setting and indicating potential unresolved challenges for live\npolicy proposals.",
        "published": "2024-06-26T18:09:46Z",
        "updated": "2024-10-11T17:42:43Z",
        "publication_venue": "NeurIPS Datasets & Benchmarks and GenLaw Workshop @ ICML",
        "code": "",
        "image": "35a23899-da81-448c-a519-bd09db2aa960.png",
        "site": "",
        "tags": [
            "Copyright Law",
            "AI Safety & Security"
        ]
    },
    {
        "title": "What is in Your Safe Data? Identifying Benign Data that Breaks Safety",
        "pdf": "http://arxiv.org/abs/2404.01099v2",
        "authors": [
            "Luxi He",
            "Mengzhou Xia",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "Current Large Language Models (LLMs), even those tuned for safety and\nalignment, are susceptible to jailbreaking. Some have found that just further\nfine-tuning an aligned model with benign data (i.e., data without harmful\ncontent) surprisingly leads to substantial degradation in safety. We delve into\nthe data-centric aspects of why benign fine-tuning inadvertently contributes to\njailbreaking. First, we represent fine-tuning data through two lenses:\nrepresentation and gradient spaces. Additionally, we propose a bi-directional\nanchoring method that, during the selection process, prioritizes data points\nthat are close to harmful examples and far from benign ones. Our approach\neffectively identifies subsets of benign data that are more likely to degrade\nthe model's safety after fine-tuning. Training on just 100 of these seemingly\nbenign datapoints surprisingly leads to the fine-tuned model affirmatively\nresponding to >70% of tested harmful requests, compared to <20% after\nfine-tuning on randomly selected data. We also observe that the selected data\nfrequently appear as lists, bullet points, or math questions, indicating a\nsystematic pattern in fine-tuning data that contributes to jailbreaking.",
        "published": "2024-04-01T13:12:30Z",
        "updated": "2024-08-20T17:54:08Z",
        "publication_venue": "Conference on Language Modeling",
        "code": "",
        "image": "003df349-4a32-49cd-a9c1-84dbd3f5866b.png",
        "site": "",
        "comments": "",
        "tags": [
            "AI Safety & Security"
        ]
    },
    {
        "title": "AI Risk Management Should Incorporate Both Safety and Security",
        "pdf": "http://arxiv.org/abs/2405.19524v1",
        "authors": [
            "Xiangyu Qi",
            "Yangsibo Huang",
            "Yi Zeng",
            "Edoardo Debenedetti",
            "Jonas Geiping",
            "Luxi He",
            "Kaixuan Huang",
            "Udari Madhushani",
            "Vikash Sehwag",
            "Weijia Shi",
            "Boyi Wei",
            "Tinghao Xie",
            "Danqi Chen",
            "Pin-Yu Chen",
            "Jeffrey Ding",
            "Ruoxi Jia",
            "Jiaqi Ma",
            "Arvind Narayanan",
            "Weijie J Su",
            "Mengdi Wang",
            "Chaowei Xiao",
            "Bo Li",
            "Dawn Song",
            "Peter Henderson",
            "Prateek Mittal"
        ],
        "year": "2024",
        "abstract": "The exposure of security vulnerabilities in safety-aligned language models,\ne.g., susceptibility to adversarial attacks, has shed light on the intricate\ninterplay between AI safety and AI security. Although the two disciplines now\ncome together under the overarching goal of AI risk management, they have\nhistorically evolved separately, giving rise to differing perspectives.\nTherefore, in this paper, we advocate that stakeholders in AI risk management\nshould be aware of the nuances, synergies, and interplay between safety and\nsecurity, and unambiguously take into account the perspectives of both\ndisciplines in order to devise mostly effective and holistic risk mitigation\napproaches. Unfortunately, this vision is often obfuscated, as the definitions\nof the basic concepts of \"safety\" and \"security\" themselves are often\ninconsistent and lack consensus across communities. With AI risk management\nbeing increasingly cross-disciplinary, this issue is particularly salient. In\nlight of this conceptual challenge, we introduce a unified reference framework\nto clarify the differences and interplay between AI safety and AI security,\naiming to facilitate a shared understanding and effective collaboration across\ncommunities.",
        "published": "2024-05-29T21:00:47Z",
        "updated": "2024-05-29T21:00:47Z",
        "publication_venue": "arXiv preprint",
        "code": "",
        "image": "dd0d7033-5385-4c86-bf70-b4f5148d3ac3.png",
        "site": "",
        "comments": "",
        "tags": [
            "AI Safety & Security",
            "AI Law & Policy"
        ]
    },
    {
        "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications",
        "pdf": "http://arxiv.org/abs/2402.05162v4",
        "authors": [
            "Boyi Wei",
            "Kaixuan Huang",
            "Yangsibo Huang",
            "Tinghao Xie",
            "Xiangyu Qi",
            "Mengzhou Xia",
            "Prateek Mittal",
            "Mengdi Wang",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.",
        "published": "2024-02-07T18:34:38Z",
        "updated": "2024-10-24T19:21:52Z",
        "publication_venue": "International Conference on Machine Learning",
        "code": "",
        "image": "8a6c9a59-8b9e-4508-a813-38dc43990fa7.png",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security"]
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "pdf": "http://arxiv.org/abs/2306.13213v2",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Peter Henderson",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "year": "2023",
        "abstract": "Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.",
        "published": "2023-06-22T22:13:03Z",
        "updated": "2023-08-16T22:38:55Z",
        "publication_venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "code": "",
        "site": "",
        "image": "ddb63397-24f0-44f2-acec-a2f91920627e.png",
        "comments": "",
        "tags" : ["AI Safety & Security"]
    },
    {
        "title": "On the opportunities and risks of foundation models",
        "pdf": "https://arxiv.org/abs/2108.07258",
        "authors": [
            "Rishi Bommasani",
            "Drew A Hudson",
            "Ehsan Adeli",
            "Russ Altman",
            "Simran Arora",
            "Sydney von Arx",
            "Michael S Bernstein",
            "Jeannette Bohg",
            "Antoine Bosselut",
            "Emma Brunskill",
            "Erik Brynjolfsson",
            "Shyamal Buch",
            "Dallas Card",
            "Rodrigo Castellon",
            "Niladri Chatterji",
            "Annie Chen",
            "Kathleen Creel",
            "Jared Quincy Davis",
            "Dora Demszky",
            "Chris Donahue",
            "Moussa Doumbouya",
            "Esin Durmus",
            "Stefano Ermon",
            "John Etchemendy",
            "Kawin Ethayarajh",
            "Li Fei-Fei",
            "Chelsea Finn",
            "Trevor Gale",
            "Lauren Gillespie",
            "Karan Goel",
            "Noah Goodman",
            "Shelby Grossman",
            "Neel Guha",
            "Tatsunori Hashimoto",
            "Peter Henderson",
            "John Hewitt",
            "Daniel E Ho",
            "Jenny Hong",
            "Kyle Hsu",
            "Jing Huang",
            "Thomas Icard",
            "Saahil Jain",
            "Dan Jurafsky",
            "Pratyusha Kalluri",
            "Siddharth Karamcheti",
            "Geoff Keeling",
            "Fereshte Khani",
            "Omar Khattab",
            "Pang Wei Koh",
            "Mark Krass",
            "Ranjay Krishna",
            "Rohith Kuditipudi",
            "Ananya Kumar",
            "Faisal Ladhak",
            "Mina Lee",
            "Tony Lee",
            "Jure Leskovec",
            "Isabelle Levent",
            "Xiang Lisa Li",
            "Xuechen Li",
            "Tengyu Ma",
            "Ali Malik",
            "Manning"
        ],
        "year": "2021",
        "abstract": "AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models"]
    },
    {
        "title": "Deep Reinforcement Learning that Matters",
        "pdf": "https://ojs.aaai.org/index.php/AAAI/article/view/11694",
        "authors": [
            "Peter Henderson*",
            "Riashat Islam*",
            "Philip Bachman",
            "Joelle Pineau",
            "Doina Precup",
            "David Meger"
        ],
        "year": "2018",
        "abstract": "In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "press": [
            {
                "outlet": "Science",
                "url": "https://www.science.org/doi/10.1126/science.359.6377.725"
            },
            {
                "outlet": "Led to Reproducibility Checklist",
                "url": "https://neuripsconf.medium.com/introducing-the-neurips-2021-paper-checklist-3220d6df500b"
            }   
        ],
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "An Introduction to Deep Reinforcement Learning",
        "pdf": "https://www.nowpublishers.com/article/Details/MAL-071",
        "authors": [
            "Vincent Fran\u00e7ois-Lavet",
            "Peter Henderson",
            "Riashat Islam",
            "Marc G Bellemare",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
        "pdf": "https://inria.hal.science/hal-03850124/",
        "authors": [
            "Teven Le Scao",
            "Angela Fan",
            "Christopher Akiki",
            "Ellie Pavlick",
            "Suzana Ili\u0107",
            "Daniel Hesslow",
            "Roman Castagn\u00e9",
            "Alexandra Sasha Luccioni",
            "Fran\u00e7ois Yvon",
            "Matthias Gall\u00e9",
            "Jonathan Tow",
            "Alexander M Rush",
            "Stella Biderman",
            "Albert Webson",
            "Pawan Sasanka Ammanamanchi",
            "Thomas Wang",
            "Beno\u00eet Sagot",
            "Niklas Muennighoff",
            "Albert Villanova del Moral",
            "Olatunji Ruwase",
            "Rachel Bawden",
            "Stas Bekman",
            "Angelina McMillan-Major",
            "Iz Beltagy",
            "Huu Nguyen",
            "Lucile Saulnier",
            "Samson Tan",
            "Pedro Ortiz Suarez",
            "Victor Sanh",
            "Hugo Lauren\u00e7on",
            "Yacine Jernite",
            "Julien Launay",
            "Margaret Mitchell",
            "Colin Raffel",
            "Aaron Gokaslan",
            "Adi Simhi",
            "Aitor Soroa",
            "Alham Fikri Aji",
            "Amit Alfassy",
            "Anna Rogers",
            "Ariel Kreisberg Nitzav",
            "Canwen Xu",
            "Chenghao Mou",
            "Chris Emezue",
            "Christopher Klamm",
            "Colin Leong",
            "Daniel van Strien",
            "David Ifeoluwa Adelani",
            "Dragomir Radev",
            "Eduardo Gonz\u00e1lez Ponferrada",
            "Efrat Levkovizh",
            "Ethan Kim",
            "Eyal Bar Natan",
            "Francesco De Toni",
            "G\u00e9rard Dupont",
            "Germ\u00e1n Kruszewski",
            "Giada Pistilli",
            "Hady Elsahar",
            "Hamza Benyamina",
            "Hieu Tran",
            "Ian Yu",
            "Idris Abdulmumin",
            "Isaac Johnson",
            "Itziar Gonzalez-Dios",
            "Javier de la Rosa",
            "Jenny Chim",
            "Jesse Dodge",
            "Jian Zhu",
            "Jonathan Chang",
            "J\u00f6rg Frohberg",
            "Joseph Tobing",
            "Joydeep Bhattacharjee",
            "Khalid Almubarak",
            "Kimbo Chen",
            "Kyle Lo",
            "Leandro Von Werra",
            "Leon Weber",
            "Long Phan",
            "Ludovic Tanguy",
            "Manan Dey",
            "Manuel Romero Mu\u00f1oz",
            "Maraim Masoud",
            "Mar\u00eda Grandury",
            "Mario \u0160a\u0161ko",
            "Max Huang",
            "Maximin Coavoux",
            "Mayank Singh",
            "Mike Tian-Jian Jiang",
            "Minh Chien Vu",
            "Mohammad A Jauhar",
            "Mustafa Ghaleb",
            "Nishant Subramani",
            "Nora Kassner",
            "Nurulaqilla Khamis",
            "Olivier Nguyen",
            "Omar Espejel",
            "Ona de Gibert",
            "Paulo Villegas",
            "Peter Henderson",
            "Pierre Colombo",
            "Priscilla Amuok",
            "Quentin Lhoest",
            "Rheza Harliman",
            "Rishi Bommasani",
            "Roberto Luis L\u00f3pez",
            "Rui Ribeiro",
            "Salomey Osei",
            "Sampo Pyysalo",
            "Sebastian Nagel",
            "Shamik Bose",
            "Shamsuddeen Hassan Muhammad",
            "Shanya Sharma",
            "Shayne Longpre",
            "Somaieh Nikpoor",
            "Stanislav Silberberg",
            "Suhas Pai",
            "Sydney Zink",
            "Tiago Timponi Torrent",
            "Timo Schick",
            "Tristan Thrush",
            "Valentin Danchev",
            "Vassilina Nikoulina",
            "Veronika Laippala",
            "Violette Lepercq",
            "Vrinda Prabhu",
            "Zaid Alyafeai",
            "Zeerak Talat",
            "Arun Raja",
            "Benjamin Heinzerling",
            "Chenglei Si",
            "Elizabeth Salesky",
            "Sabrina J Mielke",
            "Wilson Y Lee",
            "Abheesht Sharma",
            "Andrea Santilli",
            "Antoine Chaffin",
            "Arnaud Stiegler",
            "Debajyoti Datta",
            "Eliza Szczechla",
            "Gunjan Chhablani",
            "Han Wang",
            "Harshit Pandey",
            "Hendrik Strobelt",
            "Jason Alan Fries",
            "Jos Rozen",
            "Leo Gao",
            "Lintang Sutawika",
            "M Saiful Bari",
            "Maged S Al-shaibani",
            "Matteo Manica"
        ],
        "year": "2022",
        "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models"]
    },
    {
        "title": "Holistic Evaluation of Language Models",
        "pdf": "https://arxiv.org/abs/2211.09110",
        "authors": [
            "Percy Liang",
            "Rishi Bommasani",
            "Tony Lee",
            "Dimitris Tsipras",
            "Dilara Soylu",
            "Michihiro Yasunaga",
            "Yian Zhang",
            "Deepak Narayanan",
            "Yuhuai Wu",
            "Ananya Kumar",
            "Benjamin Newman",
            "Binhang Yuan",
            "Bobby Yan",
            "Ce Zhang",
            "Christian Cosgrove",
            "Christopher D Manning",
            "Christopher R\u00e9",
            "Diana Acosta-Navas",
            "Drew A Hudson",
            "Eric Zelikman",
            "Esin Durmus",
            "Faisal Ladhak",
            "Frieda Rong",
            "Hongyu Ren",
            "Huaxiu Yao",
            "Jue Wang",
            "Keshav Santhanam",
            "Laurel Orr",
            "Lucia Zheng",
            "Mert Yuksekgonul",
            "Mirac Suzgun",
            "Nathan Kim",
            "Neel Guha",
            "Niladri Chatterji",
            "Omar Khattab",
            "Peter Henderson",
            "Qian Huang",
            "Ryan Chi",
            "Sang Michael Xie",
            "Shibani Santurkar",
            "Surya Ganguli",
            "Tatsunori Hashimoto",
            "Thomas Icard",
            "Tianyi Zhang",
            "Vishrav Chaudhary",
            "William Wang",
            "Xuechen Li",
            "Yifan Mai",
            "Yuhui Zhang",
            "Yuta Koreeda"
        ],
        "year": "2022",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Transactions on Machine Learning Research",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models", "Evaluation Practices"]
    },
    {
        "title": "Towards the systematic reporting of the energy and carbon footprints of machine learning",
        "pdf": "https://www.jmlr.org/papers/v21/20-312.html",
        "authors": [
            "Peter Henderson",
            "Jieru Hu",
            "Joshua Romoff",
            "Emma Brunskill",
            "Dan Jurafsky",
            "Joelle Pineau"
        ],
        "year": "2020",
        "abstract": "Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "press": [
            {
                "outlet": "VentureBeat",
                "url": "https://venturebeat.com/data-infrastructure/why-data-has-a-sustainability-problem/"
            },
            {
                "outlet": "The Wall Street Journal",
                "url": "https://www.wsj.com/articles/researchers-are-developing-tools-to-calculate-ais-carbon-footprint-11594978202"
            },
            {
                "outlet": "Led to Carbon Reporting Suggestions at NeurIPS",
                "url" : "https://neurips.cc/Conferences/2022/PaperInformation/PaperChecklist"
            },
            {
                "outlet" : "Inspired Legilsation",
                "url" : "https://www.markey.senate.gov/news/press-releases/markey-heinrich-eshoo-beyer-introduce-legislation-to-investigate-measure-environmental-impacts-of-artificial-intelligence"
            }
        ],
        "tags" : ["Reinforcement Learning", "AI Safety & Security", "Foundation Models", "Evaluation Practices", "AI Law & Policy"]
    },
    {
        "title": "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims",
        "pdf": "https://arxiv.org/abs/2004.07213",
        "authors": [
            "Miles Brundage",
            "Shahar Avin",
            "Jasmine Wang",
            "Haydn Belfield",
            "Gretchen Krueger",
            "Gillian Hadfield",
            "Heidy Khlaaf",
            "Jingying Yang",
            "Helen Toner",
            "Ruth Fong",
            "Tegan Maharaj",
            "Pang Wei Koh",
            "Sara Hooker",
            "Jade Leung",
            "Andrew Trask",
            "Emma Bluemke",
            "Jonathan Lebensold",
            "Cullen O'Keefe",
            "Mark Koren",
            "Th\u00e9o Ryffel",
            "JB Rubinovitz",
            "Tamay Besiroglu",
            "Federica Carugati",
            "Jack Clark",
            "Peter Eckersley",
            "Sarah de Haas",
            "Maritza Johnson",
            "Ben Laurie",
            "Alex Ingerman",
            "Igor Krawczuk",
            "Amanda Askell",
            "Rosario Cammarota",
            "Andrew Lohn",
            "David Krueger",
            "Charlotte Stix",
            "Peter Henderson",
            "Logan Graham",
            "Carina Prunkl",
            "Bianca Martin",
            "Elizabeth Seger",
            "Noa Zilberman",
            "Se\u00e1n \u00d3 h\u00c9igeartaigh",
            "Frens Kroeger",
            "Girish Sastry",
            "Rebecca Kagan",
            "Adrian Weller",
            "Brian Tse",
            "Elizabeth Barnes",
            "Allan Dafoe",
            "Paul Scharre",
            "Ariel Herbert-Voss",
            "Martijn Rasser",
            "Shagun Sodhani",
            "Carrick Flynn",
            "Thomas Krendl Gilbert",
            "Lisa Dyer",
            "Saif Khan",
            "Yoshua Bengio",
            "Markus Anderljung"
        ],
        "year": "2020",
        "abstract": "With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "AI Law & Policy"]
    },
    {
        "title": "A Survey of Available Corpora For Building Data-Driven Dialogue Systems: The Journal Version",
        "pdf": "https://journals.uic.edu/ojs/index.php/dad/article/view/10733",
        "authors": [
            "Iulian Vlad Serban",
            "Ryan Lowe",
            "Peter Henderson",
            "Laurent Charlin",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Dialogue and Discourse",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models"]
    },
    {
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
        "pdf": "https://arxiv.org/abs/2310.03693",
        "authors": [
            "Xiangyu Qi",
            "Yi Zeng",
            "Tinghao Xie",
            "Pin-Yu Chen",
            "Ruoxi Jia",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "year": "2023",
        "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Learning Representations",
        "code": "https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety",
        "site": "https://llm-tuning-safety.github.io/",
        "policy_brief": "https://hai.stanford.edu/policy-brief-safety-risks-customizing-foundation-models-fine-tuning",
        "press": [
            {
                "outlet": "The New York Times",
                "url": "https://www.nytimes.com/2023/10/19/technology/guardrails-artificial-intelligence-open-source.html"
            },
            {
                "outlet": "PCMag", 
                "url": "https://www.pcmag.com/news/ai-safeguards-are-pretty-easy-to-bypass"
            },
            {
                "outlet": "The Register",
                "url": "https://www.theregister.com/2023/10/12/chatbot_defenses_dissolve/"
            },
            {
                "outlet": "VentureBeat",
                "url": "https://venturebeat.com/ai/uh-oh-fine-tuning-llms-compromises-their-safety-study-finds/"
            },
        {
            "outlet": "NIST AI 800-1 Report",
            "url": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.800-1.ipd.pdf"
        },
        {
            "outlet": "NTIA AI Open Model Report",
            "url": "https://www.ntia.gov/sites/default/files/publications/ntia-ai-open-model-report.pdf"
        },
        {
            "outlet": "NAIAC Meeting Discussion",
            "url": "https://vimeo.com/event/3784739"
        },
        {
            "outlet": "Bipartisan House Task Force Report on AI",
            "url": "https://www.speaker.gov/wp-content/uploads/2024/12/AI-Task-Force-Report-FINAL.pdf"
        }
        ],
        "comments": "",
        "tags" : ["AI Safety & Security", "AI Law & Policy"]
    },
    {
        "title": "Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control",
        "pdf": "https://arxiv.org/abs/1708.04133",
        "authors": [
            "Riashat Islam*",
            "Peter Henderson*",
            "Maziar Gomrokchi",
            "Doina Precup"
        ],
        "year": "2017",
        "abstract": "Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "When does pretraining help? assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3462757.3466088",
        "authors": [
            "Lucia Zheng*",
            "Neel Guha*",
            "Brandon R Anderson",
            "Peter Henderson",
            "Daniel E Ho"
        ],
        "year": "2020",
        "abstract": "While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Artificial Intelligence and Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models", "Public Interest AI"],
        "press": [
            {
                "outlet": "Carole Hafner Award for Best Paper ",
                "url": "https://icail2023.di.uminho.pt/"
            }
        ]
    },
    {
        "title": "Ethical Challenges in Data-Driven Dialogue Systems",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3278721.3278777",
        "authors": [
            "Peter Henderson",
            "Koustuv Sinha",
            "Nicolas Angelard-Gontier",
            "Nan Rosemary Ke",
            "Genevieve Fried",
            "Ryan Lowe",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "The use of dialogue systems as a medium for human-machine interaction is an increasingly prevalent paradigm. A growing number of dialogue systems use conversation strategies that are learned from large datasets. There are well documented instances where interactions with these system have resulted in biased or even offensive conversations due to the data-driven training process. Here, we highlight potential ethical issues that arise in dialogue systems research, including: implicit biases in data-driven systems, the rise of adversarial examples, potential sources of privacy violations, safety concerns, special considerations for reinforcement learning systems, and reproducibility concerns. We also suggest areas stemming from these issues that deserve further investigation. Through this initial survey, we hope to spur research leading to robust, safe, and ethically sound dialogue systems.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AAAI/ACM Conference on AI, Ethics, and Society",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security"]
    },
    {
        "title": "Legalbench: A collaboratively built benchmark for measuring legal reasoning in large language models",
        "pdf": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/89e44582fd28ddfea1ea4dcb0ebbf4b0-Abstract-Datasets_and_Benchmarks.html",
        "authors": [
            "Neel Guha",
            "Julian Nyarko",
            "Daniel Ho",
            "Christopher R\u00e9",
            "Adam Chilton",
            "Alex Chohlas-Wood",
            "Austin Peters",
            "Brandon Waldon",
            "Daniel Rockmore",
            "Diego Zambrano",
            "Dmitry Talisman",
            "Enam Hoque",
            "Faiz Surani",
            "Frank Fagan",
            "Galit Sarfaty",
            "Gregory Dickinson",
            "Haggai Porat",
            "Jason Hegland",
            "Jessica Wu",
            "Joe Nudell",
            "Joel Niklaus",
            "John Nay",
            "Jonathan Choi",
            "Kevin Tobia",
            "Margaret Hagan",
            "Megan Ma",
            "Michael Livermore",
            "Nikon Rasumov-Rahe",
            "Nils Holzenberger",
            "Noam Kolt",
            "Peter Henderson",
            "Sean Rehaag",
            "Sharad Goel",
            "Shang Gao",
            "Spencer Williams",
            "Sunny Gandhi",
            "Tom Zur",
            "Varun Iyer",
            "Zehua Li"
        ],
        "year": "2023",
        "abstract": "The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning\u2014which distinguish between its many forms\u2014correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "NeurIPS Datasets and Benchmarks Track",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Public Interest AI", "Evaluation Practices"]
    },
    {
        "title": "Foundation models and fair use",
        "pdf": "https://www.jmlr.org/papers/v24/23-0569.html",
        "authors": [
            "Peter Henderson",
            "Xuechen Li",
            "Dan Jurafsky",
            "Tatsunori Hashimoto",
            "Mark A Lemley",
            "Percy Liang"
        ],
        "year": "2023",
        "abstract": "Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant US case law, drawing parallels to existing and potential applications for generating text, source code, and visual art. Experiments confirm that popular foundation models can generate content considerably similar to copyrighted material. Second, we discuss technical mitigations that can help foundation models stay in line with fair use. We argue that more research is needed to align mitigation strategies with the current state of the law. Third, we suggest that the law and technical mitigations should co-evolve. For example, coupled with other policy mechanisms, the law could more explicitly consider safe harbors when strong technical tools are used to mitigate infringement harms. This co-evolution may help strike a balance between intellectual property and innovation, which speaks \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Journal of Machine Learning Research",
        "code": "",
        "site": "",
        "comments": "",
        "policy_brief" : "https://hai.stanford.edu/sites/default/files/2023-11/Foundation-Models-Copyright.pdf",
        "press": [
            {
                "outlet": "Jotwell",
                "url": "https://ip.jotwell.com/generative-ai-meets-copyright/"
            },
            {
                "outlet": "News Media Alliance",
                "url": "https://www.newsmediaalliance.org/wp-content/uploads/2023/12/NMA-Reply-to-USCO-AI-Notice-December-2023.pdf"
            },
            {
                "outlet": "Chamber of Progress",
                "url": "https://downloads.regulations.gov/COLC-2023-0006-8583/attachment_1.pdf"
            },
            {
                "outlet": "Public Knowledge",
                "url": "https://downloads.regulations.gov/COLC-2023-0006-10336/attachment_1.pdf"
            },
            {
                "outlet": "HuggingFace",
                "url": "https://downloads.regulations.gov/COLC-2023-0006-8969/attachment_1.pdf"
            },
            {
                "outlet": "American Library Association",
                "url": "https://downloads.regulations.gov/COLC-2023-0006-8452/attachment_1.pdf"
            },
            {
                "outlet": "Common Crawl Foundation",
                "url": "https://www.regulations.gov/comment/COLC-2023-0006-7056"
            },
            {
                "outlet": "Sy Damle Congressional Testimony",
                "url": "https://docs.house.gov/meetings/JU/JU03/20230517/115951/HHRG-118-JU03-Wstate-DamleS-20230517.pdf"
            }
        ],
        "tags" : ["Copyright Law", "AI Law & Policy"]
    },
    {
        "title": "With Little Power Comes Great Responsibility",
        "pdf": "https://arxiv.org/abs/2010.06595",
        "authors": [
            "Dallas Card",
            "Peter Henderson",
            "Urvashi Khandelwal",
            "Robin Jia",
            "Kyle Mahowald",
            "Dan Jurafsky"
        ],
        "year": "2020",
        "abstract": "Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Conference on Empirical Methods in Natural Language Processing",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Evaluation Practices"]
    },
    {
        "title": "Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset",
        "pdf": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/bc218a0c656e49d4b086975a9c785f47-Abstract-Datasets_and_Benchmarks.html",
        "authors": [
            "Peter Henderson*",
            "Mark S Krass*",
            "Lucia Zheng",
            "Neel Guha",
            "Christopher D Manning",
            "Dan Jurafsky",
            "Daniel E Ho"
        ],
        "year": "2022",
        "abstract": "One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a~ 256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "NeurIPS Datasets and Benchmarks Track",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Public Interest AI", "Evaluation Practices", "Foundation Models"]
    },
    {
        "title": "Underwater Multi-Robot Convoying using Visual Tracking by Detection",
        "pdf": "https://ieeexplore.ieee.org/abstract/document/8206280/",
        "authors": [
            "Florian Shkurti",
            "Wei-Di Chang",
            "Peter Henderson",
            "Md Jahidul Islam",
            "Juan Camilo Gamboa Higuera",
            "Jimmy Li",
            "Travis Manderson",
            "Anqi Xu",
            "Gregory Dudek",
            "Junaed Sattar"
        ],
        "year": "2018",
        "abstract": "We present a robust multi-robot convoying approach that relies on visual detection of the leading agent, thus enabling target following in unstructured 3-D environments. Our method is based on the idea of tracking-by-detection, which interleaves efficient model-based object detection with temporal filtering of image-based bounding box estimation. This approach has the important advantage of mitigating tracking drift (i.e. drifting away from the target object), which is a common symptom of model-free trackers and is detrimental to sustained convoying in practice. To illustrate our solution, we collected extensive footage of an underwater robot in ocean settings, and hand-annotated its location in each frame. Based on this dataset, we present an empirical comparison of multiple tracker variants, including the use of several convolutional neural networks, both with and without recurrent connections, as well as frequency \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Robotics"]
    },
    {
        "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3531146.3534637",
        "authors": [
            "Yacine Jerncite",
            "Huu Nguyen",
            "Stella Biderman",
            "Anna Rogers",
            "Valentin Danchev",
            "Samson Tan",
            "Alexandra Sasha Luccioni",
            "Dupont G\u00e9rard",
            "Lo Kyle",
            "Talat Zeerak",
            "Dragomir Radev",
            "Somaieh Nikpoor",
            "J\u00f6rg Frohberg",
            "Aaron Gokaslan",
            "Peter Henderson",
            "Rishi Bommasani",
            "Margaret Mitchell"
        ],
        "year": "2022",
        "abstract": "The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work. ",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "ACM Conference on Fairness, Accountability, and Transparency",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy"]
    },
    {
        "title": "Benchmark Environments for Multitask Learning in Continuous Domains",
        "pdf": "https://arxiv.org/abs/1708.04352",
        "authors": [
            "Peter Henderson",
            "Wei-Di Chang",
            "Florian Shkurti",
            "Johanna Hansen",
            "David Meger",
            "Gregory Dudek"
        ],
        "year": "2017",
        "abstract": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods",
        "pdf": "https://arxiv.org/abs/1810.02525",
        "authors": [
            "Peter Henderson",
            "Joshua Romoff",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings - such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "European Workshop on Reinforcement Learning",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Reward Estimation for Variance Reduction in Deep Reinforcement Learning",
        "pdf": "https://arxiv.org/abs/1805.03359",
        "authors": [
            "Joshua Romoff*",
            "Peter Henderson*",
            "Alexandre Piche",
            "Vincent Francois-Lavet",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "Reinforcement Learning (RL) agents require the specification of a reward signal for learning behaviours. However, introduction of corrupt or stochastic rewards can yield high variance in learning. Such corruption may be a direct result of goal misspecification, randomness in the reward signal, or correlation of the reward with external factors that are not known to the agent. Corruption or stochasticity of the reward signal can be especially problematic in robotics, where goal specification can be particularly difficult for complex tasks. While many variance reduction techniques have been studied to improve the robustness of the RL process, handling such stochastic or corrupted reward structures remains difficult. As an alternative for handling this scenario in model-free RL methods, we suggest using an estimator for both rewards and value functions. We demonstrate that this improves performance under corrupted stochastic rewards in both the tabular and non-linear function approximation settings for a variety of noise types and environments. The use of reward estimation is a robust and easy-to-implement improvement for handling corrupted reward signals in model-free RL.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Conference on Robot Learning",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Position: On the Societal Impact of Open Foundation Models",
        "pdf": "https://raw.githubusercontent.com/mlresearch/v235/main/assets/kapoor24a/kapoor24a.pdf",
        "authors": [
            "Sayash Kapoor",
            "Rishi Bommasani",
            "Kevin Klyman",
            "Shayne Longpre",
            "Ashwin Ramaswami",
            "Peter Cihon",
            "Aspen Hopkins",
            "Kevin Bankston",
            "Stella Biderman",
            "Miranda Bogen",
            "Rumman Chowdhury",
            "Alex Engler",
            "Peter Henderson",
            "Yacine Jernite",
            "Seth Lazar",
            "Stefano Maffulli",
            "Alondra Nelson",
            "Joelle Pineau",
            "Aviya Skowron",
            "Dawn Song",
            "Victor Storchan",
            "Daniel Zhang",
            "Daniel E Ho",
            "Percy Liang",
            "Arvind Narayanan"
        ],
        "year": "2024",
        "abstract": "Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (eg Llama 3, Stable Diffusion XL). We identify five distinctive properties of open foundation models (eg greater customizability, poor monitoring) that mediate their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (eg cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps explain why the marginal risk is low in some cases, clarifies disagreements about misuse risks by revealing that past work has focused on different subsets of the framework with different assumptions, and articulates a way forward for more constructive debate. Overall, our work supports a more grounded assessment of the societal impact of open foundation models by",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Machine Learning",
        "code": "",
        "policy_brief": "https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf",
        "site": "https://crfm.stanford.edu/open-fms/",
        "press" : [
        {
            "outlet": "National Telecommunications and Information Administration (NTIA) Report",
            "url": "https://www.ntia.gov/programs-and-initiatives/artificial-intelligence/open-model-weights-report/risks-benefits-of-dual-use-foundation-models-with-widely-available-model-weights/competition-innovation-research"
        },
        {
            "outlet": "National Artificial Intelligence Advisory Committee (NAIAC)",
            "url": "https://ai.gov/wp-content/uploads/2024/06/FINDINGS-RECOMMENDATIONS_AI-Safety.pdf"
        }
        ],
        "tags" : ["AI Law & Policy", "Foundation Models"]
    },
    {
        "title": "Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3600211.3604690",
        "authors": [
            "Peter Henderson",
            "Eric Mitchell",
            "Christopher Manning",
            "Dan Jurafsky",
            "Chelsea Finn"
        ],
        "year": "2023",
        "abstract": " A growing ecosystem of large, open-source foundation models has reduced the labeled data and technical expertise necessary to apply machine learning to many new problems. Yet foundation models pose a clear dual-use risk, indiscriminately reducing the costs of building both harmful and beneficial machine learning systems. Policy tools such as restricted model access and export controls are the primary methods currently used to mitigate such dual-use risks. In this work, we review potential safe-release strategies and argue that both policymakers and AI researchers would benefit from fundamentally new technologies enabling more precise control over the downstream usage of open-source foundation models. We propose one such approach: the task blocking paradigm, in which foundation models are trained with an additional mechanism to impede adaptation to harmful tasks without sacrificing \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AAAI/ACM Conference on AI, Ethics, And Society",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Foundation Models", "AI Safety & Security"]
    },
    {
        "title": "Where's the Liability in harmful AI Speech?",
        "pdf": "https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/jfspl3&section=33",
        "authors": [
            "Peter Henderson",
            "Tatsunori Hashimoto",
            "Mark Lemley"
        ],
        "year": "2023",
        "abstract": "ChatGPT\" hallucinates.\"'That is, it often generates text that makes factual claims that are untrue and perhaps never even appear in its training data. It can get math problems wrong. It can get dates wrong. But it can also make things up. It makes up sources that don't exist, as one lawyer found out to their chagrin when they cited nonexistent cases in a legal brief. 2 It makes up quotes. And it can make up false claims that hurt people. Ask it what crimes a particular person has committed or been accused of, and ChatGPT might get it right, truthfully saying, for instance, that Richard Nixon was accused of destroying evidence to hide a burglary committed by his campaign, or truthfully saying that it is unaware of any accusations against a person. But it will also sometimes tell a false story about a crime. ChatGPT 3.5 (but not 4.0), for instance, says that one of us (Lemley) has been accused and indeed found liable for \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Journal of Free Speech Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "AI Safety & Security", "Tort Law", "Free Speech Law"]
    },
    {
        "title": "SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors",
        "pdf": "https://arxiv.org/abs/2406.14598",
        "authors": [
            "Tinghao Xie",
            "Xiangyu Qi",
            "Yi Zeng",
            "Yangsibo Huang",
            "Udari Madhushani Sehwag",
            "Kaixuan Huang",
            "Luxi He",
            "Boyi Wei",
            "Dacheng Li",
            "Ying Sheng",
            "Ruoxi Jia",
            "Bo Li",
            "Kai Li",
            "Danqi Chen",
            "Peter Henderson",
            "Prateek Mittal"
        ],
        "year": "2025",
        "abstract": "Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with SORRY-Bench, our proposed benchmark. First, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. Second, linguistic characteristics and formatting of prompts are often overlooked, like different languages, dialects, and more -- which are only implicitly considered in many evaluations. We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects. Third, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Learning Representations",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "Foundation Models"]
    },
    {
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep",
        "pdf": "https://arxiv.org/abs/2406.05946",
        "authors": [
            "Xiangyu Qi",
            "Ashwinee Panda",
            "Kaifeng Lyu",
            "Xiao Ma",
            "Subhrajit Roy",
            "Ahmad Beirami",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "year": "2025",
        "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned LLMs are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Learning Representations",
        "code": "",
        "site": "",
        "press": [
            {
                "outlet": "Yannic Kilcher's Paper Explainer",
                "url": "https://www.youtube.com/watch?v=-r0XPC7TLzY"
            },
            {
                "outlet": "Center for Democracy and Technology letter to NIST",
                "url": "https://cdt.org/wp-content/uploads/2024/09/Final-Comments-CDT-DS-NIST-800-1-FM-Misuse.pdf"
            }
        ],
        "tags" : ["AI Safety & Security", "Foundation Models"]
    },
    {
        "title": "Separating value functions across time-scales",
        "pdf": "https://proceedings.mlr.press/v97/romoff19a.html",
        "authors": [
            "Joshua Romoff*",
            "Peter Henderson*",
            "Ahmed Touati",
            "Emma Brunskill",
            "Joelle Pineau",
            "Yann Ollivier"
        ],
        "year": "2019",
        "abstract": "In many finite horizon episodic reinforcement learning (RL) settings, it is desirable to optimize for the undiscounted return-in settings like Atari, for instance, the goal is to collect the most points while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temporal discounting is often applied to optimize over a shorter effective planning horizon. This comes at the cost of potentially biasing the optimization target away from the undiscounted goal. In settings where this bias is unacceptable-where the system must optimize for longer horizons at higher discounts-the target of the value function approximator may increase in variance leading to difficulties in learning. We present an extension of temporal difference (TD) learning, which we call TD (), that breaks down a value function into a series of components based on the differences between value functions with smaller discount factors. The separation of a longer horizon value function into these components has useful properties in scalability and performance. We discuss these properties and show theoretic and empirical improvements over standard TD learning in certain settings.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Machine Learning",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "A Safe Harbor for AI Evaluation and Red Teaming",
        "pdf": "https://arxiv.org/abs/2403.04893",
        "authors": [
            "Shayne Longpre",
            "Sayash Kapoor",
            "Kevin Klyman",
            "Ashwin Ramaswami",
            "Rishi Bommasani",
            "Borhane Blili-Hamelin",
            "Yangsibo Huang",
            "Aviya Skowron",
            "Zheng-Xin Yong",
            "Suhas Kotha",
            "Yi Zeng",
            "Weiyan Shi",
            "Xianjun Yang",
            "Reid Southen",
            "Alexander Robey",
            "Patrick Chao",
            "Diyi Yang",
            "Ruoxi Jia",
            "Daniel Kang",
            "Sandy Pentland",
            "Arvind Narayanan",
            "Percy Liang",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "Independent evaluation and red teaming are critical for identifying the risks posed by generative AI systems. However, the terms of service and enforcement strategies used by prominent AI companies to deter model misuse have disincentives on good faith safety evaluations. This causes some researchers to fear that conducting such research or releasing their findings will result in account suspensions or legal reprisal. Although some companies offer researcher access programs, they are an inadequate substitute for independent research access, as they have limited community representation, receive inadequate funding, and lack independence from corporate incentives. We propose that major AI developers commit to providing a legal and technical safe harbor, indemnifying public interest safety research and protecting it from the threat of account suspensions or legal reprisal. These proposals emerged from our collective experience conducting safety, privacy, and trustworthiness research on generative AI systems, where norms and incentives could be better aligned with public interests, without exacerbating model misuse. We believe these commitments are a necessary step towards more inclusive and unimpeded community efforts to tackle the risks of generative AI.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Machine Learning",
        "code": "",
        "site": "",
        "comments": "",
        "press": [
            {
                "outlet": "Department of Justice Letter",
                "url": "https://www.copyright.gov/1201/2024/USCO-letters/Letter from Department of Justice Criminal Division.pdf"
            },
            {
                "outlet": "Comments to the Ninth Triennial Proceeding at the Copyright Office",
                "url" : "https://www.copyright.gov/1201/2024/comments/reply/Class%204%20-%20Reply%20-%20Kevin%20Klyman%20et%20al.%20(Joint%20Academic%20Researchers).pdf"
            },
        {
            "outlet": "The Washington Post",
            "url": "https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney/"
        }
            ,
            {
                "outlet": "Knight First Amendment Institute Blog Post",
                "url": "https://knightcolumbia.org/blog/a-safe-harbor-for-ai-evaluation-and-red-teaming"
            }

        ],
        "tags" : ["AI Law & Policy", "AI Safety & Security"]
    },
    {
        "title": "Learning Robust Dialog Policies in Noisy Environments",
        "pdf": "https://arxiv.org/abs/1712.04034",
        "authors": [
            "Maryam Fazel-Zarandi",
            "Shang-Wen Li",
            "Jin Cao",
            "Jared Casale",
            "Peter Henderson",
            "David Whitney",
            "Alborz Geramifard"
        ],
        "year": "2017",
        "abstract": "Modern virtual personal assistants provide a convenient interface for completing daily tasks via voice commands. An important consideration for these assistants is the ability to recover from automatic speech recognition (ASR) and natural language understanding (NLU) errors. In this paper, we focus on learning robust dialog policies to recover from these errors. To this end, we develop a user simulator which interacts with the assistant through voice commands in realistic scenarios with noisy audio, and use it to learn dialog policies through deep reinforcement learning. We show that dialogs generated by our simulator are indistinguishable from human generated dialogs, as determined by human evaluators. Furthermore, preliminary experimental results show that the learned policies in noisy environments achieve the same execution success rate with fewer dialog turns compared to fixed rule-based policies.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Workshop on Conversational AI at NeurIPS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "What's in Your Safe Data? Identifying Benign Data that Breaks Safety",
        "pdf": "https://arxiv.org/abs/2404.01099",
        "authors": [
            "Luxi He",
            "Mengzhou Xia",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Conference on Language Modeling",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "Foundation Models"]
    },
    {
        "title": "An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning",
        "pdf": "https://arxiv.org/abs/2103.06224",
        "authors": [
            "Dilip Arumugam",
            "Peter Henderson",
            "Pierre-Luc Bacon"
        ],
        "year": "2020",
        "abstract": "How do we formalize the challenge of credit assignment in reinforcement learning? Common intuition would draw attention to reward sparsity as a key contributor to difficult credit assignment and traditional heuristics would look to temporal recency for the solution, calling upon the classic eligibility trace. We posit that it is not the sparsity of the reward itself that causes difficulty in credit assignment, but rather the \\emph{information sparsity}. We propose to use information theory to define this notion, which we then use to characterize when credit assignment is an obstacle to efficient learning. With this perspective, we outline several information-theoretic mechanisms for measuring credit under a fixed behavior policy, highlighting the potential of information theory as a key tool towards provably-efficient credit assignment.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Workshop on Biological and Artificial Reinforcement Learning (NeurIPS)",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Freedom of Speech and AI Output",
        "pdf": "https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/jfspl3&section=34",
        "authors": [
            "Eugene Volokh",
            "Mark A Lemley",
            "Peter Henderson"
        ],
        "year": "2023",
        "abstract": "Is the output of generative AI entitled to First Amendment protection? We're inclined to say yes. Even though current AI programs are of course not people and do not themselves have constitutional rights, their speech may potentially be protected because of the rights of the programs' creators. But beyond that, and likely more significantly, AI programs' speech should be protected because of the rights of their users-both the users' rights to listen and their rights to speak. In this short Article, we sketch the outlines of this analysis.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Journal of Free Speech Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Free Speech Law"]
    },
    {
        "title": "Vulnerabilities in Discovery Tech",
        "pdf": "https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/hjlt35&section=17",
        "authors": [
            "Neel Guha*",
            "Peter Henderson*",
            "Diego Zambrano*"
        ],
        "year": "2022",
        "abstract": "Recent technological advances are changing the litigation landscape, especially in the discovery context. For nearly two decades, technologies have reinvented document searches in complex litigation, normalizing the use of machine learning algorithms under the umbrella of\" Technology-Assisted Review\"(\" TAR\"). The latest technological developments are placing discovery beyond attorney understanding and firmly in the realm of computer science and engineering. As lawyers struggle to keep up, a creeping sense of anxiety is spreading in the legal profession about a lack of transparency and the potential for discovery abuse. Judges, attorneys, bar associations, and scholars warn that lawyers need to closely supervise the technical aspects of TAR and avoid the dangers of sabotage, intentional hacking, or abuse. But commentators have not fully defined what the risks entail, described in detail the potential \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Harvard Journal of Law & Technology",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Civil Procedure"]
    },
    {
        "title": "Bayesian Policy Gradients via Alpha Divergence Dropout Inference",
        "pdf": "https://arxiv.org/abs/1712.02037",
        "authors": [
            "Peter Henderson*",
            "Thang Doan*",
            "Riashat Islam",
            "David Meger"
        ],
        "year": "2017",
        "abstract": "Policy gradient methods have had great success in solving continuous control tasks, yet the stochastic nature of such problems makes deterministic value estimation difficult. We propose an approach which instead estimates a distribution by fitting the value function with a Bayesian Neural Network. We optimize an -divergence objective with Bayesian dropout approximation to learn and estimate this distribution. We show that using the Monte Carlo posterior mean of the Bayesian value function distribution, rather than a deterministic network, improves stability and performance of policy gradient methods in continuous control MuJoCo simulations.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Bayesian Deep Learning Workshop at NeurIPS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Promises and pitfalls of artificial intelligence for legal applications",
        "pdf": "https://arxiv.org/abs/2402.01656",
        "authors": [
            "Sayash Kapoor",
            "Peter Henderson",
            "Arvind Narayanan"
        ],
        "year": "2024",
        "abstract": "Is AI set to redefine the legal profession? We argue that this claim is not supported by the current evidence. We dive into AI's increasingly prevalent roles in three types of legal tasks: information processing; tasks involving creativity, reasoning, or judgment; and predictions about the future. We find that the ease of evaluating legal applications varies greatly across legal tasks, based on the ease of identifying correct answers and the observability of information relevant to the task at hand. Tasks that would lead to the most significant changes to the legal profession are also the ones most prone to overoptimism about AI capabilities, as they are harder to evaluate. We make recommendations for better evaluation and deployment of AI in legal contexts.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Journal of Cross-disciplinary Research in Computational Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Public Interest AI"]
    },
    {
        "title": "How US law will evaluate artificial intelligence for covid-19",
        "pdf": "https://www.bmj.com/content/372/bmj.n234.full",
        "authors": [
            "Mark Krass",
            "Peter Henderson",
            "Michelle M Mello",
            "David M Studdert",
            "Daniel E Ho"
        ],
        "year": "2021",
        "abstract": "Daniel E Ho and colleagues explore the legal implications of using artificial intelligence in the response to covid-19 and call for more robust evaluation frameworks",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "BMJ",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy"]
    },
    {
        "title": "Or\u0131on: Asynchronous Distributed Hyperparameter Optimization",
        "pdf": "https://github.com/Epistimio/orion",
        "authors": [
            "Xavier Bouthillier",
            "Christos Tsirigotis",
            "Fran\u00e7ois Corneau-Tremblay",
            "Pierre Delaunay",
            "Reyhane Askari",
            "Dendi Suhubdy",
            "Michael Noukhovitch",
            "Dmitriy Serdyuk",
            "Arnaud Bergeron",
            "Peter Henderson",
            "Pascal Lamblin",
            "Mirko Bronzi",
            "Christopher Beckham"
        ],
        "year": "2018",
        "abstract": "",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AutoML Workshop at ICML",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Cheaply estimating inference efficiency metrics for autoregressive transformer models",
        "pdf": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/d1a14493e5f84d6c6129414f0cd1a7c6-Abstract-Conference.html",
        "authors": [
            "Deepak Narayanan",
            "Keshav Santhanam",
            "Peter Henderson",
            "Rishi Bommasani",
            "Tony Lee",
            "Percy S Liang"
        ],
        "year": "2023",
        "abstract": "Large language models (LLMs) are highly capable but also computationally expensive. Characterizing the fundamental tradeoff between inference efficiency and model capabilities is thus important, but requires an efficiency metric that is comparable across models from different providers. Unfortunately, raw runtimes measured through black-box APIs do not satisfy this property: model providers can implement software and hardware optimizations orthogonal to the model, and shared infrastructure introduces performance contention. We propose a new metric for inference efficiency called idealized runtime, that puts models on equal footing as though they were served on uniform hardware and software without performance contention, and a cost model to efficiently estimate this metric for autoregressive Transformer models. We also propose variants of the idealized runtime that incorporate the number and type of accelerators needed to serve the model. Using these metrics, we compare ten LLMs developed in 2022 to provide the first analysis of inference efficiency-capability tradeoffs; we make several observations from this analysis, including the fact that the superior inference runtime performance of certain APIs is often a byproduct of optimizations within the API rather than the underlying model. Our code is open sourced at https://github. com/stanford-crfm/helm-efficiency.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "NeurIPS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models", "Evaluation Practices"]
    },
    {
        "title": "Integrating reward maximization and population estimation: Sequential decision-making for Internal Revenue Service audit selection",
        "pdf": "https://ojs.aaai.org/index.php/AAAI/article/view/25637",
        "authors": [
            "Peter Henderson",
            "Ben Chugg",
            "Brandon Anderson",
            "Kristen Altenburger",
            "Alex Turk",
            "John Guyton",
            "Jacob Goldin",
            "Daniel E Ho"
        ],
        "year": "2023",
        "abstract": "We introduce a new setting, optimize-and-estimate structured bandits. Here, a policy must select a batch of arms, each characterized by its own context, that would allow it to both maximize reward and maintain an accurate (ideally unbiased) population estimate of the reward. This setting is inherent to many public and private sector applications and often requires handling delayed feedback, small data, and distribution shifts. We demonstrate its importance on real data from the United States Internal Revenue Service (IRS). The IRS performs yearly audits of the tax base. Two of its most important objectives are to identify suspected misreporting and to estimate the\" tax gap\"--the global difference between the amount paid and true amount owed. Based on a unique collaboration with the IRS, we cast these two processes as a unified optimize-and-estimate structured bandit. We analyze optimize-and-estimate approaches to the IRS problem and propose a novel mechanism for unbiased population estimation that achieves rewards comparable to baseline approaches. This approach has the potential to improve audit efficacy, while maintaining policy-relevant estimates of the tax gap. This has important social consequences given that the current tax gap is estimated at nearly half a trillion dollars. We suggest that this problem setting is fertile ground for further research and we highlight its interesting challenges. The results of this and related research are currently being incorporated into the continual improvement of the IRS audit selection methods.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AAAI Conference on Artificial Intelligence",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning", "Public Interest AI"]
    },
    {
        "title": "Beyond Ads: Sequential Decision-Making Algorithms in Law and Public Policy",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3511265.3550439",
        "authors": [
            "Peter Henderson*",
            "Ben Chugg*",
            "Brandon Anderson",
            "Daniel E Ho"
        ],
        "year": "2023",
        "abstract": "We explore the promises and challenges of employing sequential decision-making algorithms -- such as bandits, reinforcement learning, and active learning -- in law and public policy. While such algorithms have well-characterized performance in the private sector (e.g., online advertising), the tendency to naively apply algorithms motivated by one domain, often online advertisements, can be called the ''advertisement fallacy.'' Our main thesis is that law and public policy pose distinct methodological challenges that the machine learning community has not yet addressed. Machine learning will need to address these methodological problems to move ''beyond ads.'' Public law, for instance, can pose multiple objectives, necessitate batched and delayed feedback, and require systems to learn rational, causal decision-making policies, each of which presents novel questions at the research frontier. We discuss a wide \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "ACM Symposium on Computer Science and Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning", "Public Interest AI"]
    },
    {
        "title": "TDprop: Does Adaptive Optimization With Jacobi Preconditioning Help Temporal Difference Learning?",
        "pdf": "https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p1082.pdf",
        "authors": [
            "Joshua Romoff",
            "Peter Henderson",
            "David Kanaa",
            "Emmanuel Bengio",
            "Ahmed Touati",
            "Pierre-Luc Bacon",
            "Joelle Pineau"
        ],
        "year": "2021",
        "abstract": "Reinforcement Learning (RL) systems are tasked with maximizing the cumulative sum of discounted rewards in a particular environment. In order to do so, most RL methods rely on estimating the",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AAMAS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Text Characterization Toolkit (TCT)",
        "pdf": "https://aclanthology.org/2022.aacl-demo.9/",
        "authors": [
            "Daniel Simig",
            "Tianlu Wang",
            "Verna Dankers",
            "Peter Henderson",
            "Khuyagbaatar Batsuren",
            "Dieuwke Hupkes",
            "Mona Diab"
        ],
        "year": "2022",
        "abstract": "We present a tool, Text Characterization Toolkit (TCT), that researchers can use to study characteristics of large datasets. Furthermore, such properties can lead to understanding the influence of such attributes on models\u2019 behaviour. Traditionally, in most NLP research, models are usually evaluated by reporting single-number performance scores on a number of readily available benchmarks, without much deeper analysis. Here, we argue that\u2013especially given the well-known fact that benchmarks often contain biases, artefacts, and spurious correlations\u2013deeper results analysis should become the de-facto standard when presenting new models or benchmarks. TCT aims at filling this gap by facilitating such deeper analysis for datasets at scale, where datasets can be for training/development/evaluation. TCT includes both an easy-to-use tool, as well as off-the-shelf scripts that can be used for specific analyses. We also present use-cases from several different domains. TCT is used to predict difficult examples for given well-known trained models; TCT is also used to identify (potentially harmful) biases present in a dataset.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AACL",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models", "Evaluation Practices"]
    },
    {
        "title": "FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning",
        "pdf": "https://arxiv.org/abs/2404.02127",
        "authors": [
            "Joel Niklaus",
            "Lucia Zheng",
            "Arya D McCarthy",
            "Christopher Hahn",
            "Brian M Rosen",
            "Peter Henderson",
            "Daniel E Ho",
            "Garrett Honke",
            "Percy Liang",
            "Christopher Manning"
        ],
        "year": "2025",
        "abstract": "Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "EMNLP Findings",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models", "Public Interest AI"]
    },
    {
        "title": "Distilling Information from a Flood: A Possibility for the Use of Meta-Analysis and Systematic Review in Machine Learning Research",
        "pdf": "https://arxiv.org/abs/1812.01074",
        "authors": [
            "Peter Henderson",
            "Emma Brunskill"
        ],
        "year": "2018",
        "abstract": "The current flood of information in all areas of machine learning research, from computer vision to reinforcement learning, has made it difficult to make aggregate scientific inferences. It can be challenging to distill a myriad of similar papers into a set of useful principles, to determine which new methodologies to use for a particular application, and to be confident that one has compared against all relevant related work when developing new ideas. However, such a rapidly growing body of research literature is a problem that other fields have already faced - in particular, medicine and epidemiology. In those fields, systematic reviews and meta-analyses have been used exactly for dealing with these issues and it is not uncommon for entire journals to be dedicated to such analyses. Here, we suggest the field of machine learning might similarly benefit from meta-analysis and systematic review, and we encourage further discussion and development along this direction.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Evaluation Practices"]
    },
    {
        "title": "Beyond Open vs. Closed: Emerging Consensus and Key Questions for Foundation AI Model Governance",
        "pdf": "https://policycommons.net/artifacts/14355062/beyond-open-vs-closed/15253077/",
        "authors": [
            "Jon Bateman",
            "Dan Baer",
            "Stephanie A Bell",
            "Glenn O Brown",
            "Mariano-Florentino Tino Cu\u00e9llar",
            "Deep Ganguli",
            "Peter Henderson",
            "Brodi Kotila",
            "Larry Lessig",
            "Nicklas Berild Lundblad",
            "Janet Napolitano",
            "Deborah Raji",
            "Elizabeth Seger",
            "Matt Sheehan",
            "Aviya Skowron",
            "Irene Solaiman",
            "Helen Toner",
            "Polina Zvyagina"
        ],
        "year": "2024",
        "abstract": "Ideological conflict between \u201cpro-open\u201d and \u201canti-open\u201d camps is receding. Carnegie gathered leading experts from a wide range of perspectives to identify common ground and help reset AI governance debates.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Carnegie Endowment for International Peace",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy"]
    },
    {
        "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
        "pdf": "https://arxiv.org/abs/2406.16746",
        "authors": [
            "Shayne Longpre",
            "Stella Biderman",
            "Alon Albalak",
            "Hailey Schoelkopf",
            "Daniel McDuff",
            "Sayash Kapoor",
            "Kevin Klyman",
            "Kyle Lo",
            "Gabriel Ilharco",
            "Nay San",
            "Maribeth Rauh",
            "Aviya Skowron",
            "Bertie Vidgen",
            "Laura Weidinger",
            "Arvind Narayanan",
            "Victor Sanh",
            "David Adelani",
            "Percy Liang",
            "Rishi Bommasani",
            "Peter Henderson",
            "Sasha Luccioni",
            "Yacine Jernite",
            "Luca Soldaini"
        ],
        "year": "2024",
        "abstract": "Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "TMLR",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Foundation Models"]
    },
    {
        "title": "Algorithmic Rulemaking vs. Algorithmic Guidance",
        "pdf": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4784350",
        "authors": [
            "Peter Henderson",
            "Mark Krass"
        ],
        "year": "2023",
        "abstract": "Algorithms are coming to government. One legal question raised by this change is the extent to which the Administrative Procedure Act (\" APA\") will regulate the use of algorithms as decision support tools for agency adjudicators. Under the APA,\" rules\" are officially binding statements of policy subject to notice and comment as well as rigorous pre-implementation judicial review, whereas\" guidance,\" officially defined as non-binding advice, is effectively unreviewable. The implementation of algorithmic tools often occupies a gray zone between the two. To help clear the thicket, we provide a deep dive into the computer science and economics literature to provide a set of workable heuristics that help distinguish algorithmic rulemaking from algorithmic guidance. These heuristics align with best practices in the computer science literature and provide insights into agency incentives for adopting safer algorithms. We suggest that the specter of rulemaking may have value in nudging agencies toward best practices aligned with existing algorithmic safety recommendations. Specifically, avoidance of APA rulemaking may encourage agencies to prevent automation bias and other potential harms from algorithmic deployments. In this way, distinguishing algorithmic rules and guidance under the existing framework of the APA may dovetail with best practices in computer science.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Harvard Journal of Law & Technology",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Administrative Law"]
    },
    {
        "title": "Rethinking Machine Learning Benchmarks in the Context of Professional Codes of Conduct",
        "pdf": "https://dl.acm.org/doi/abs/10.1145/3614407.3643708",
        "authors": [
            "Peter Henderson",
            "Jieru Hu",
            "Mona Diab",
            "Joelle Pineau"
        ],
        "year": "2024",
        "abstract": "Benchmarking efforts for machine learning have often mimicked (or even explicitly used) professional licensing exams to assess capabilities in a given area, focusing primarily on accuracy as the metric of choice. However, this approach neglects a variety of essential skills required in professional settings. We propose that professional codes of conduct and rules can guide machine learning researchers to address potential gaps in benchmark construction. These guidelines frequently account for situations professionals may encounter and must handle with care. A model may excel on an exam but still fall short in critical scenarios, deemed unacceptable under professional codes or rules. To motivate this idea, we conduct a case study and comparative examination of machine translation in legal settings. We point out several areas where standard deployments and benchmarks do not assess key requirements under \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "ACM Symposium on Computer Science and Law",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Evaluation Practices", "Foundation Models", "Public Interest AI"]
    },
    {
        "title": "Entropy Regularization for Population Estimation",
        "pdf": "https://ojs.aaai.org/index.php/AAAI/article/view/26438",
        "authors": [
            "Ben Chugg",
            "Peter Henderson",
            "Jacob Goldin",
            "Daniel E Ho"
        ],
        "year": "2023",
        "abstract": "Entropy regularization is known to improve exploration in sequential decision-making problems. We show that this same mechanism can also lead to nearly unbiased and lower-variance estimates of the mean reward in the optimize-and-estimate structured bandit setting. Mean reward estimation (ie, population estimation) tasks have recently been shown to be essential for public policy settings where legal constraints often require precise estimates of population metrics. We show that leveraging entropy and KL divergence can yield a better trade-off between reward and estimator variance than existing baselines, all while remaining nearly unbiased. These properties of entropy regularization illustrate an exciting potential for bringing together the optimal exploration and estimation literature.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "AAAI",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning", "Public Interest AI"]
    },
    {
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs",
        "pdf": "https://arxiv.org/abs/2412.07097",
        "authors": [
            "Xiangyu Qi",
            "Boyi Wei",
            "Nicholas Carlini",
            "Yangsibo Huang",
            "Tinghao Xie",
            "Luxi He",
            "Matthew Jagielski",
            "Milad Nasr",
            "Prateek Mittal",
            "Peter Henderson"
        ],
        "year": "2025",
        "abstract": "Stakeholders -- from model developers to policymakers -- seek to minimize the dual-use risks of large language models (LLMs). An open challenge to this goal is whether technical safeguards can impede the misuse of LLMs, even when models are customizable via fine-tuning or when model weights are fully open. In response, several recent studies have proposed methods to produce durable LLM safeguards for open-weight LLMs that can withstand adversarial modifications of the model's weights via fine-tuning. This holds the promise of raising adversaries' costs even under strong threat models where adversaries can directly fine-tune model weights. However, in this paper, we urge for more careful characterization of the limits of these approaches. Through several case studies, we demonstrate that even evaluating these defenses is exceedingly difficult and can easily mislead audiences into thinking that safeguards are more durable than they really are. We draw lessons from the evaluation pitfalls that we identify and suggest future research carefully cabin claims to more constrained, well-defined, and rigorously examined threat models, which can provide more useful and candid assessments to stakeholders.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "International Conference on Learning Representations",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "Foundation Models", "Evaluation Practices"]
    },
    {
        "title": "Cost Adaptation for Robust Decentralized Swarm Behaviour",
        "pdf": "https://ieeexplore.ieee.org/abstract/document/8594283/",
        "authors": [
            "Peter Henderson",
            "Matthew Vertescher",
            "David Meger",
            "Mark Coates"
        ],
        "year": "2018",
        "abstract": "Decentralized receding horizon control (D-RHC) provides a mechanism for coordination in multiagent settings without a centralized command center. However, combining a set of different goals, costs, and constraints to form an efficient optimization objective for D-RHC can be difficult. To allay this problem, we use a meta-learning process - cost adaptation - which generates the optimization objective for D-RHC to solve based on a set of human-generated priors (cost and constraint functions) and an auxiliary heuristic. We use this adaptive D-RHC method for control of mesh-networked swarm agents. This formulation allows a wide range of tasks to be encoded and can account for network delays, heterogeneous capabilities, and increasingly large swarms through the adaptation mechanism. We leverage the Unity3D game engine to build a simulator capable of introducing artificial networking failures and delays in \u2026",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "IROS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning", "Robotics"]
    },
    {
        "title": "JIGMARK: A Black-Box Approach for Enhancing Image Watermarks against Diffusion Model Edits",
        "pdf": "https://arxiv.org/abs/2406.03720",
        "authors": [
            "Minzhou Pan",
            "Yi Zeng",
            "Xue Lin",
            "Ning Yu",
            "Cho-Jui Hsieh",
            "Peter Henderson",
            "Ruoxi Jia"
        ],
        "year": "2024",
        "abstract": "In this study, we investigate the vulnerability of image watermarks to diffusion-model-based image editing, a challenge exacerbated by the computational cost of accessing gradient information and the closed-source nature of many diffusion models. To address this issue, we introduce JIGMARK. This first-of-its-kind watermarking technique enhances robustness through contrastive learning with pairs of images, processed and unprocessed by diffusion models, without needing a direct backpropagation of the diffusion process. Our evaluation reveals that JIGMARK significantly surpasses existing watermarking solutions in resilience to diffusion-model edits, demonstrating a True Positive Rate more than triple that of leading baselines at a 1% False Positive Rate while preserving image quality. At the same time, it consistently improves the robustness against other conventional perturbations (like JPEG, blurring, etc.) and malicious watermark attacks over the state-of-the-art, often by a large margin. Furthermore, we propose the Human Aligned Variation (HAV) score, a new metric that surpasses traditional similarity measures in quantifying the number of image derivatives from image editing.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "Foundation Models"]
    },
    {
        "title": "Corpus Enigmas and Contradictory Linguistics: Tensions between Empirical Semantic Meaning and Judicial Interpretation",
        "pdf": "https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/mipr25&section=26",
        "authors": [
            "Peter Henderson",
            "Daniel E Ho",
            "Andrea Vallebueno",
            "Cassandra Handan-Nader"
        ],
        "year": "2024",
        "abstract": "Recent years have witnessed an increase in the interest in corpus linguistics-the quantitative analysis of large volumes of text, sometimes aided with machine learning-to inform legal meaning. Researchers have claimed that corpus linguistics enables robust, rigorous, and transparent discovery of the original public meaning of constitutional provisions and the meaning of statutory text. We contribute to this debate from the perspective of researchers in computational text analysis. We document tensions between such empirical semantic meaning approaches and judicial interpretation, where the use of corpus linguistics may sub silentio clash with express jurisprudential commitments. First, corpus linguistics may rely on foreign law to",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Minnesota Journal of Law, Science & Technology",
        "code": "",
        "site": "",
        "comments": "",
        "press": [
            {
                "outlet": "Amicus Brief",
                "url": "https://www.supremecourt.gov/DocketPDF/22/22-800/285812/20231023134636708_22-800 bsac Professors Final.pdf"
            },
            {
                "outlet": "Moore v. United States (J. Jackson concurring)",
                "url": "https://www.supremecourt.gov/opinions/23pdf/22-800_jg6o.pdf"
            },
            {
                "outlet" : "Discussed in part at April 2024 meeting of the Advisory Committee on the Federal Rules of Evidence",
                "url": "https://www.uscourts.gov/rules-policies/archives/agenda-books/advisory-committee-evidence-rules-april-2024"
            }
        ],
        "tags" : ["AI Law & Policy", "Public Interest AI", "Statuory Interpretation"]
    },
    {
        "title": "Gamesmanship in Modern Discovery Tech",
        "pdf": "https://www.cambridge.org/core/books/legal-tech-and-the-future-of-civil-justice/gamesmanship-in-modern-discovery-tech/FF91037E5B83DE64940620768ECC39C9",
        "authors": [
            "Neel Guha",
            "Peter Henderson",
            "Diego Zambrano"
        ],
        "year": "2023",
        "abstract": "",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Legal Tech and the Future of Civil Justice",
        "publisher": "Cambridge University Press",
        "type": "Book",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Law & Policy", "Public Interest AI", "Civil Procedure"]
    },
    {
        "title": "Ideas for Improving the Field of Machine Learning: Summarizing Discussion from the NeurIPS 2019 Retrospectives Workshop",
        "pdf": "https://arxiv.org/abs/2007.10546",
        "authors": [
            "Shagun Sodhani",
            "Mayoore S Jaiswal",
            "Lauren Baker",
            "Koustuv Sinha",
            "Carl Shneider",
            "Peter Henderson",
            "Joel Lehman",
            "Ryan Lowe"
        ],
        "year": "2020",
        "abstract": "This report documents ideas for improving the field of machine learning, which arose from discussions at the ML Retrospectives workshop at NeurIPS 2019. The goal of the report is to disseminate these ideas more broadly, and in turn encourage continuing discussion about how the field could improve along these axes. We focus on topics that were most discussed at the workshop: incentives for encouraging alternate forms of scholarship, re-structuring the review process, participation from academia and industry, and how we might better train computer scientists as scientists. Videos from the workshop can be accessed at https://slideslive.com/neurips/west-114-115-retrospectives-a-venue-for-selfreflection-in-ml-research",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "arXiv",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Evaluation Practices"]
    },
    {
        "title": "Prioritizing Public Health Resources for COVID-19 Investigations: How Administrative Data Can Protect Vulnerable Populations",
        "pdf": "https://www.healthaffairs.org/content/forefront/prioritizing-public-health-resources-covid-19-investigations-administrative-data-can",
        "authors": [
            "Mark Krass",
            "Peter Henderson",
            "Daniel E. Ho"
        ],
        "year": "2021",
        "abstract": "Consider contact tracing, a measure central to South Korea\u2019s successful COVID-19 mitigation strategy. Contact tracing is time and resource intensive. Interviewing every patient and getting in touch with their close contacts involves hours of work. In Wuhan, for instance,\u201cmore than 1800 teams of epidemiologists, with a minimum of 5 people/team, are tracing tens of thousands of contacts a day.\u201d Understaffed public health departments are finding it tough to keep up, and some jurisdictions have abandoned contact tracing entirely.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Health Affairs Blog",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Public Interest AI"]
    },
    {
        "title": "The RLLChatbot: a solution to the ConvAI challenge",
        "pdf": "https://arxiv.org/abs/1811.02714",
        "authors": [
            "Nicolas Gontier",
            "Koustuv Sinha",
            "Peter Henderson",
            "Iulian Serban",
            "Michael Noseworthy",
            "Prasanna Parthasarathi",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "Current conversational systems can follow simple commands and answer basic questions, but they have difficulty maintaining coherent and open-ended conversations about specific topics. Competitions like the Conversational Intelligence (ConvAI) challenge are being organized to push the research development towards that goal. This article presents in detail the RLLChatbot that participated in the 2017 ConvAI challenge. The goal of this research is to better understand how current deep learning and reinforcement learning tools can be used to build a robust yet flexible open domain conversational agent. We provide a thorough description of how a dialog system can be built and trained from mostly public-domain datasets using an ensemble model. The first contribution of this work is a detailed description and analysis of different text generation models in addition to novel message ranking and selection methods. Moreover, a new open-source conversational dataset is presented. Training on this data significantly improves the Recall@k score of the ranking and selection mechanisms compared to our baseline model responsible for selecting the message returned at each interaction.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "arXiv",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    },
    {
        "title": "Adversarial Gain",
        "pdf": "https://arxiv.org/abs/1811.01302",
        "authors": [
            "Peter Henderson",
            "Koustuv Sinha",
            "Rosemary Nan Ke",
            "Joelle Pineau"
        ],
        "year": "2018",
        "abstract": "Adversarial examples can be defined as inputs to a model which induce a mistake - where the model output is different than that of an oracle, perhaps in surprising or malicious ways. Original models of adversarial attacks are primarily studied in the context of classification and computer vision tasks. While several attacks have been proposed in natural language processing (NLP) settings, they often vary in defining the parameters of an attack and what a successful attack would look like. The goal of this work is to propose a unifying model of adversarial examples suitable for NLP tasks in both generative and classification settings. We define the notion of adversarial gain: based in control theory, it is a measure of the change in the output of a system relative to the perturbation of the input (caused by the so-called adversary) presented to the learner. This definition, as we show, can be used under different feature spaces and distance conditions to determine attack or defense effectiveness across different intuitive manifolds. This notion of adversarial gain not only provides a useful way for evaluating adversaries and defenses, but can act as a building block for future work in robustness under adversaries due to its rooted nature in stability and manifold theory.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "arXiv",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security"]
    },
    {
        "title": "Cascaded to End-to-End: New Safety, Security, and Evaluation Questions for Audio Language Models",
        "pdf": "https://scholar.google.com/citations?view_op=view_citation&citation_for_view=dy_JBs0AAAAJ:4X0JR2_MtJMC",
        "authors": [
            "Luxi He",
            "Xiangyu Qi",
            "Inyoung Cheong",
            "Prateek Mittal Danqi Chen",
            "Peter Henderson"
        ],
        "year": "2024",
        "abstract": "",
        "published": "2024-06-20T17:38:16Z",
        "updated": "2025-12-01T00:00:00Z",
        "publication_venue": "EvalEval Workshop @ NeurIPS",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Evaluation Practices", "Privacy Law", "AI Safety & Security"]
    },
    {
        "title": "The Mirage of Artificial Intelligence Terms of Use Restrictions",
        "pdf": "https://arxiv.org/abs/2412.07066",
        "authors": [
            "Peter Henderson",
            "Mark A Lemley"
        ],
        "year": "2025",
        "abstract": "Artificial intelligence (AI) model creators commonly attach restrictive terms of use to both their models and their outputs. These terms typically prohibit activities ranging from creating competing AI models to spreading disinformation. Often taken at face value, these terms are positioned by companies as key enforceable tools for preventing misuse, particularly in policy dialogs. But are these terms truly meaningful? There are myriad examples where these broad terms are regularly and repeatedly violated. Yet except for some account suspensions on platforms, no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief. This is likely for good reason: we think that the legal enforceability of these licenses is questionable. This Article systematically assesses of the enforceability of AI model terms of use and offers three contributions. First, we pinpoint a key problem: the artifacts that they protect, namely model weights and model outputs, are largely not copyrightable, making it unclear whether there is even anything to be licensed. Second, we examine the problems this creates for other enforcement. Recent doctrinal trends in copyright preemption may further undermine state-law claims, while other legal frameworks like the DMCA and CFAA offer limited recourse. Anti-competitive provisions likely fare even worse than responsible use provisions. Third, we provide recommendations to policymakers. There are compelling reasons for many provisions to be unenforceable: they chill good faith research, constrain competition, and create quasi-copyright ownership where none should exist. There are, of course \u2026",
        "published": "2025-02-01T00:00:00Z",
        "updated": "2025-02-01T00:00:00Z",
        "publication_venue": "Indiana Law Journal (forthcoming)",
        "code": "",
        "image": "a329982a-7490-4269-87ce-a978cf92190d.png",
        "site": "",
        "press": [
            {
                "outlet": "Business Insider",
                "url": "https://www.businessinsider.com/openai-little-legal-recourse-against-deepseek-tech-law-experts-2025-1"
            },
            {
                "outlet": "VOA News",
                "url": "https://www.voanews.com/a/deepseek-vs-chatgpt-fuels-debate-over-ai-building-blocks/7958031.html"
            }
        ],
        "comments": "",
        "tags" : ["Copyright Law", "AI Law & Policy"]
    },
    {
        "title": "Should the United States or the European Union Follow China\u2019s Lead and Require Watermarks for Generative AI?",
        "pdf": "https://gjia.georgetown.edu/2023/05/24/should-the-united-states-or-the-european-union-follow-chinas-lead-and-require-watermarks-for-generative-ai/",
        "authors": [
            "Peter Henderson"
        ],
        "year": "2023",
        "abstract": "AI-generated content is becoming increasingly prevalent and realistic, leading to concerns about its potential misuse. China has been a fast mover in regulating AI and recently implemented requirements to label and watermark AI-generated content. But watermarks for text-based generative AI have many nuances so US and EU policymakers should proceed cautiously as they consider implementing similar regulations.",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Georgetown Journal of International Affairs (Online)",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["AI Safety & Security", "Foundation Models", "AI Law & Policy"]
    },
    {
        "title": "OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning",
        "pdf": "https://arxiv.org/pdf/1709.06683",
        "authors": [
            "Peter Henderson",
            "Wei-Di Chang",
            "Pierre-Luc Bacon",
            "David Meger",
            "Joelle Pineau",
            "Doina Precup"
        ],
        "year": "2018",
        "abstract": "",
        "published": "-01-01T00:00:00Z",
        "updated": "-01-01T00:00:00Z",
        "publication_venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
        "code": "",
        "site": "",
        "comments": "",
        "tags" : ["Reinforcement Learning"]
    }
]